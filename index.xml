<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
>

    <channel>
        <title>Quant Out of Water</title>
        <atom:link href="%7balternate%20%7bRSS%20application/rss&#43;xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20https://rjtk.github.io/index.xml%7d" rel="self" type="application/rss+xml" />
        <link>https://rjtk.github.io/</link>
        <managingEditor>qoow</managingEditor>
        <description>Quant Out of Water</description>
        <lastBuildDate>Sun, 19 Feb 2023 00:00:00 -0600</lastBuildDate>
        <language>en-us</language>
        <generator>Hugo -- gohugo.io</generator><item>
            <title>Group Theory, Symmetry, and a Brain Teaser</title>
            <link>https://rjtk.github.io/posts/group-theory-symmetry-and-a-brain-teaser/</link>
            <pubDate>Sun, 19 Feb 2023 00:00:00 -0600</pubDate>
            <guid>https://rjtk.github.io/posts/group-theory-symmetry-and-a-brain-teaser/</guid><description>&lt;p&gt;I review some elementary discrete math concepts, use them to describe and solve a neat brain-teaser, and generalize the solution into a form of robust breadth first search.&lt;/p&gt;
&lt;h2 class=&#34;group &#34; id=&#34;introduction&#34;
    &gt;Introduction&lt;a href=&#34;#introduction&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;Recently, a friend introduced me to the following brain teaser:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Problem Statement&lt;/strong&gt;: Suppose you are kidnapped, blindfolded, and placed into a room.  Your kidnappers hand you a plate with four coins on it, arranged in a&lt;/kbd&gt; \(2 \times 2\) &lt;kbd&gt;square.  They will let you leave if you can flip all of the coins so that they are face up, in a &lt;em&gt;finite&lt;/em&gt; amount of time (they ain&amp;rsquo;t got all day!).  Since you can&amp;rsquo;t see, you can ask the interrogator if you&amp;rsquo;ve successfully placed all the coins heads up (if you&amp;rsquo;d like to increase the immersion, you can imagine you are actually blind, otherwise you could just take off the blindfold).  The interrogator won&amp;rsquo;t lie to you, but will try to mess with you: any time the answer is &amp;ldquo;no&amp;rdquo;, they will rotate the plate before giving it back to you.  The amount by which it is rotated is arbitrary &amp;ndash; it could be random, follow a fixed pattern, or be adversarial.&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;The first things coming to mind with this puzzle is that you could just iterate through every possible sequence of flips, asking the interrogator whether they&amp;rsquo;re all heads up after each flip.  This possibility is immediately shut down by their adversarial rotations &amp;ndash; the interrogator can easily foil this plan.&lt;/p&gt;
&lt;p&gt;Another possibility is through &lt;em&gt;random&lt;/em&gt; flips.  Randomization is an effective tool because the interrogator cannot determine your next move by finding a pattern in your behaviour (if you randomize effectively, even you don&amp;rsquo;t know your next move!).  If you simply pick up all the coins at random, flip them randomly, and then ask if you&amp;rsquo;ve solved the puzzle, it is guaranteed (by the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Borell-Cantelli Lemma&lt;/a
&gt;
) that you will eventually stumble upon the solution.  However, this is where the caveat about &lt;em&gt;finite time&lt;/em&gt; comes in.  In probability theory, it is sometimes a point of confusion what is actually meant by a random variable being finite, so let&amp;rsquo;s spell it out.  If \(N(\omega)\) is the number of flips taken, given a state of the universe \(\omega\), the Borell-Cantelli Lemma tells us that \(N(\omega) &amp;lt; \infty\) almost surely, so \(N\) is &lt;em&gt;bounded&lt;/em&gt;, but it is not &lt;em&gt;uniformly bounded&lt;/em&gt; (over all possible events \(\omega\)).  What we want is that there is some real number \(N_{\text{max}} &amp;lt; \infty\) such that \(N(\omega) \le N_{\text{max}}\) almost-surely, and this cannot be done by the random flipping strategy.&lt;/p&gt;
&lt;p&gt;That all being said, the solution I arrived upon does not include any randomization.  Before explaining the solution, I&amp;rsquo;m going to take a detour through the ideas that came to my mind, and some of the formalisms that can be used to understand the puzzle, its solution, and potential generalization.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;a-mathematical-preamble&#34;
    &gt;A Mathematical Preamble&lt;a href=&#34;#a-mathematical-preamble&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;The purpose of this section is to review some mathematical formalism and terminology used in reasoning about the coins-on-a-plate puzzle.  I have a limited background in discrete math or abstract algebra, so even though these concepts are rather elementary, it has been good for me to write things down.  Indeed, reviewing elementary concepts facilitates &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Chunking_%28psychology%29&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chunking&lt;/a
&gt;
 (an essential part of &lt;a
    class=&#34;link&#34;
    href=&#34;https://amzn.to/41gvuFQ&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning&lt;/a
&gt;
).  If all one is interested in is the solution to the puzzle, or if they are already well familiar with these concepts, they can skip to &lt;a
    class=&#34;link&#34;
    href=&#34;#solving-the-puzzle&#34;&gt;Solving the Puzzle&lt;/a
&gt;
.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;equivalence-classes&#34;
    &gt;Equivalence Classes&lt;a href=&#34;#equivalence-classes&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;In mathematics, an &lt;em&gt;equivalence relation&lt;/em&gt; \(\sim\) on a set \(X\) is a binary relation satisfying the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Reflexivity&lt;/em&gt;: \(\forall x \in X\ x \sim x\)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Symmetry&lt;/em&gt;: \(\forall x, y \in X\ x \sim y \iff y \sim x\)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transitivity&lt;/em&gt;: $∀ x, y, z ∈ X\ x ∼ y, y ∼ z \implies x ∼ z.$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most famous equivalence relation is &amp;ldquo;ordinary&amp;rdquo; equality: &amp;lsquo;\(=\)&amp;rsquo;, but there are various more exotic equivalence relations: equivalence up to rotations in geometry, equivalence almost surely in probability, equivalence except on measure-zero sets in the theory of \(L_p\) spaces, &lt;em&gt;etc&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When dealing with a finite number of objects (finiteness is not necessary in some/many of the cases I talk about, but I keep the assumption to avoid accidentally writing something that is technically wrong for infinite sets), equivalence classes easily arise from partitions of the set.  That is, if \(X\) is a finite set, and \(X = \bigcup_{i = 1}^M X_i\) for subsets \(X_i \subseteq X\) which are all disjoint \(i \ne j \implies X_i \cap X_j = \emptyset\) (&lt;em&gt;i.e.&lt;/em&gt;, the \(X_i\) are a partition \(X\)) we can define an equivalence relation on \(X\) by \[x \sim y \iff (\exists i: x \in X_i \wedge y \in X_i),\] where \(\wedge\) indicates &amp;ldquo;and&amp;rdquo;.  Let us quickly check that this is a bona-fide equivalence relation: Let \(x \in X\).  There is an \(X_i\) in the partition such that \(x \in X_i\) and hence &amp;ldquo;also&amp;rdquo; \(x \in X_i\) verifying it is reflexive.  That it is symmetric arises since \((x \in X_i \wedge y \in X_i) \iff (y \in X_i \wedge x \in X_i)\).  Finally, it is transitive since the \(X_i\) are a partition without any overlap: if \(x, y\) are in the same set \(X_i\) and \(y, z\) are both in another set \(X_j\), it must be that \(X_i = X_j\) (and hence \(x, z \in X_i\)) since \(y\) must be in exactly one of the \(X_i\).  Conversely, the reader can check that any set partition also induces an equivalence relation.  Thus, set partitions and equivalence relations are equivalent.&lt;/p&gt;
&lt;p&gt;Given an equivalence relation \(\sim\) on a set \(X\), the corresponding partition (&lt;em&gt;i.e.&lt;/em&gt;, the partition &lt;em&gt;induced&lt;/em&gt; by \(\sim\)) is commonly denoted as \(X / \sim\) as the &lt;em&gt;quotient set&lt;/em&gt;.  We then say that elements of a set in the partition (that is, elements \(x \in X_i\) for some \(X_i \in X / \sim\), which is a subset \(X_i \subseteq X\)) are &lt;em&gt;equivalent up to&lt;/em&gt; \(\sim\).&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;functions-permutations-and-groups&#34;
    &gt;Functions, Permutations, and Groups&lt;a href=&#34;#functions-permutations-and-groups&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Any &lt;em&gt;finite&lt;/em&gt; set with \(N\) elements can be represented simply as the set of integers from \(1\) to \(N\) as in \(X = \{1, 2, \ldots, N\}\) (a set which is commonly denoted \([N]\)).  It does not matter what the elements of the set actually are, you can simply refer to the first, second, &lt;em&gt;etc&lt;/em&gt;., in an arbitrary order.&lt;/p&gt;
&lt;p&gt;Now, what are the functions on finite sets?  Well, a function \(f: X \rightarrow X\) is just a mapping from some element of \(x \in X\) (without loss, an integer between \(1\) and \(N\)) to another element \(f(x) \in X\).  If the function is a one-to-one function (&lt;em&gt;i.e.&lt;/em&gt;, it is &lt;em&gt;injective&lt;/em&gt;) whose domain is all of \(X\), then it must also (why?) be an onto function (&lt;em&gt;i.e.&lt;/em&gt;, a &lt;em&gt;surjection&lt;/em&gt;) and thus it is a &lt;em&gt;bijection&lt;/em&gt; and in this context such functions are called &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Permutation&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;permutations&lt;/em&gt;&lt;/a
&gt;
.  These are nothing but re-orderings of the integers, usually denoted by \(\sigma\) (or other Greek letter, instead of the more generic \(f\)) as in &amp;ldquo;\(\sigma\) re-orders the sequence \((1, 2, \ldots, N)\) into \((\sigma(1), \sigma(2), \ldots, \sigma(N))\)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Since permutations are bijections, they have an inverse.  Moreover, the composition operation amongst permutations is &lt;em&gt;associative&lt;/em&gt; (why?): for permutations \(\sigma, \pi, \tau\) on \(X\), it is the case that \((\sigma \circ \pi) \circ \tau = \sigma \circ (\pi \circ \tau)\), where, &lt;em&gt;e.g.&lt;/em&gt;, \((\sigma \circ \pi)(x) = \sigma(\pi(x))\).  This means that (with the trivial &amp;ldquo;doing nothing&amp;rdquo; permutation \(Id(x) = x\) serving as the identity) the set of all permutations of \(X\), call it \[G_X = \{\sigma: X \rightarrow X\ | \ \sigma \text{ is a bijection}\},\] along with the composition operation constitutes a (finite) &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Group_%28mathematics%29&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Group&lt;/em&gt;&lt;/a
&gt;
 \((G_X, \circ)\).  This particular group \(G_X\) of all bijections is particularly special, and is referred to as the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Symmetric_group&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Symmetric Group&lt;/a
&gt;
 \(S_N\) on \(N\) elements &amp;ndash; it is worthwhile noting that the size of this group grows rather quickly: there are \(N!\) bijections in \(S_N\).&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;cycles-and-decomposition&#34;
    &gt;Cycles and Decomposition&lt;a href=&#34;#cycles-and-decomposition&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;Given a permutation \(\sigma \in S_N\), an $r$-&lt;em&gt;cycle&lt;/em&gt; is a sequence of \(r \ge 1\) numbers \(i_1, \ldots, i_r\) such that \(i_{k + 1} = \sigma(i_k)\) and \(i_r = i_1\).  Given an $r$-cycle of \(\sigma\), we can collect each \(i_1, \ldots, i_r\) into a set \(C \subseteq [N]\) called an &lt;em&gt;orbit&lt;/em&gt; and restrict \(\sigma\) to this cycle so that \(\sigma_C\) is equal to \(\sigma\) in \(C\), and the identity outside of \(C\) as in \(\forall x \in C:\ \sigma_C(x) = \sigma(x)\) and \(\forall x \in C^{\mathsf{c}}:\ \sigma_C(x) = x\).  Remarkably, permutations can be &lt;em&gt;decomposed&lt;/em&gt; into a composition of their cycles:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Cyclic Decomposition): Let $\sigma \in S_N$.  Then, there exists a partition $C_1, \ldots, C_m$ of $[N]$ such that $\sigma = \sigma_{C_1} \circ \cdots \circ \sigma_{C_m}.$  Moreover, each $C_i$ is an orbit of a cycle of $\sigma$, and for any cycle of $\sigma$ there is a corresponding orbit $C_i$.&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;proof (sketch): Since \(\sigma\) acts on a finite set, there exists at least one cycle, call its orbit \(C_1\).  We have the partition \([N] = C_1^{\mathsf{c}} \cup C_1\) and the decomposition \(\sigma = \sigma_{C_1^{\mathsf{c}}} \circ \sigma_{C_1}\) since \(\sigma_{C_1}(x) = x\) on \(C_1^{\mathsf{c}}\).  Now, \(C_1\) contains at least one element, and \(\sigma_{C_1^{\mathsf{C}}}\) is another permutation when restricted to \(C_1^{\mathsf{c}}\), so the theorem follows by induction.&lt;/em&gt; \(\square\)&lt;/p&gt;
&lt;p&gt;From the partitioning of \([N]\) by the orbits of a permutation, we see that any permutation also induces a collection of equivalence classes (exactly the orbits of the permutation)!  Within each of these equivalence classes, we can say that the elements are identical &lt;em&gt;up to&lt;/em&gt; the operation of \(\sigma\).  As well, we say that any set \(C\) such that \(x \in C \implies \sigma(x) \in C\) is &lt;em&gt;invariant&lt;/em&gt; to \(\sigma\) &amp;ndash; clearly, the partitions induced by \(\sigma\) are all invariant sets.  These formalisms can be used to understand the structure of the coins-on-a-plate puzzle.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;solving-the-puzzle&#34;
    &gt;Solving the Puzzle&lt;a href=&#34;#solving-the-puzzle&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;The main idea for solving the puzzle revolves around identifying particular partitions of the set of all states of the game, thus identifying equivalence relations.  I then define some natural operations that one can apply to the coins on the plate and reason about how those operations map elements of one equivalence class into another.  It turns out that we can apply these operations in a particular order such that we are guaranteed to eventually reach the solution.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;representation-and-rotations&#34;
    &gt;Representation and Rotations&lt;a href=&#34;#representation-and-rotations&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Abstractly, we can represent the state of each coin as either being heads \(H\) or tails \(T\).  Since the coins are arranged in a square, we can think of the state of the whole puzzle as being the sequence of states of each individual coin following a left-to-right and top-to-bottom ordering.  Notice that we&amp;rsquo;ve already encountered an equivalence class: we naturally think of aligning the coins to look as though they are on a &amp;ldquo;right-angle&amp;rdquo; when facing us &amp;ndash; following this convention, a whole set of angles of the plate spanning \(90^\circ\) (\(45^\circ\) in either direction) all represent &lt;em&gt;equivalent&lt;/em&gt; configurations.  If the interrogator rotates the plate by, say, \(37^\circ\) before handing it back to us, we should be able to feel around the coins to rotate this back to \(0^\circ\).  Thus, when we speak of an arrangement of coins on the plate, we are already really speaking of a whole equivalence class, all of which are equivalent up to rotation back to the nearest right-angle alignment of the coins.  The upshot here is that listing the coins in this order results in a finite representation of the set of all possible states (of which there are \(2^4 = 16\)) as elements of the set \[S = \Bigl\{{HH \atop HH}, {TH \atop HH}, {HT \atop HH}, \ldots, {TT \atop TH}, {TT \atop TT}\Bigr\},\] which is naturally isomorphic (&lt;em&gt;i.e.&lt;/em&gt;, equivalent) to the set \([16] = \{1, \ldots, 16\}\).&lt;/p&gt;
&lt;p&gt;Any such rotation operation can now naturally be thought of as a permutation \(\texttt{rotate}^n\), where \(n\) indicates the number of \(90^\circ\) angles the plate is clockwise rotated.  For example, \(\texttt{rotate} \Bigl({HT \atop HH}\Bigr) = {HH \atop HT}\) and \(\texttt{rotate}^2 \Bigl({HT \atop HH}\Bigr) = {HH \atop TH}\), &lt;em&gt;etc.&lt;/em&gt;  The power notation with \(n\) is naturally suggestive since \(n\) rotations by \(90^\circ\) is equivalent to applying the single \(\texttt{rotate}\) permutation \(n\) times.  Since we don&amp;rsquo;t know how many times the interrogator will apply \(\texttt{rotate}\) before giving us the plate back, we need to work with equivalence classes of states which are equivalent up to the action of this permutation.&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;: It is an exercise for the reader to enumerate all of the equivalence classes induced by $\texttt{rotate}$.  I should note at this point though that I stumbled upon the appropriate sets and operations for solving the puzzle through intuition, and only went backwards afterwards to add some formalism.  In my experience, this is typical of mathematical problem solving and research &amp;ndash; you begin by understanding and solving particular problems and special cases ad-hoc through intuition, and then go back to formalize the ideas, which opens up new ideas and generalizations, which you begin by solving ad-hoc, and then later formalize, which leads to more questions&amp;hellip;&lt;/kbd&gt;&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;solved-states&#34;
    &gt;Solved States&lt;a href=&#34;#solved-states&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;The next key observation involves another permutation and collection of equivalence classes.  Consider a \(\texttt{flip\_all}\) operation &amp;ndash; you can apply this operator to the coins on the plate by flipping every coin onto its other face.  Since this \(\texttt{flip\_all}\) operation is just a permutation, it decomposes the space \(S\) into equivalence classes (whose elements are all equivalent up to the operation of \(\texttt{flip\_all}\)).  Examples include \(\Bigl\{ {HT \atop HT}, {TH \atop TH} \Bigr\}, \Bigl\{ {HT \atop TH}, {TH \atop HT} \Bigr\}\), &lt;em&gt;etc&lt;/em&gt;, which are all orbits containing just two elements.  Another important example is \[S_\star = \Bigl\{{HH \atop HH}, {TT \atop TT}\Bigr\},\] which is the equivalence class (of \(\texttt{flip\_all}\)) containing the solved state \({HH \atop HH}\).  The importance of this class arises as follows.&lt;/p&gt;
&lt;p&gt;Consider the following query, which I&amp;rsquo;ll refer to as \(\texttt{solved?}(s)\): &lt;kbd&gt;Ask the interrogator if the puzzle in state&lt;/kbd&gt; \(s\) &lt;kbd&gt;is solved.  If it is, you&amp;rsquo;re done.  If not, the plate is returned to you after application of some number of $\texttt{rotate}$ permutations.&lt;/kbd&gt;  Now, consider the composite query (\(\texttt{solved?}(s)\) is not formally a permutation, so I use the word &amp;ldquo;query&amp;rdquo;): \(\texttt{solved?} \circ \texttt{flip\_all}\), which in words describes: &lt;kbd&gt;first flip all the coins onto their other side and then ask the interrogator if the puzzle is solved.&lt;/kbd&gt;  Therefore, since \(\texttt{rotate}\) does not change the number of heads or tails in the configuration (&lt;em&gt;i.e.&lt;/em&gt;, both elements of \(S_\star\) are &lt;em&gt;fixed-points&lt;/em&gt; of \(\texttt{rotate}\)) if you reach a state \(s \in S_\star\) then you can first query \(\texttt{solved?}\) and if the plate is returned to you (which means you must have asked about the configuration \({TT \atop TT}\)) then you know that the state \(s\) has not changed and you can apply the composite query \(\texttt{solved?} \circ \texttt{flip\_all}\) which is now guaranteed to result in successfully solving the puzzle.&lt;/p&gt;
&lt;p&gt;Thus, if you do this every time you want to know if the puzzle is solved, then you have &lt;em&gt;effectively&lt;/em&gt; solved the puzzle anytime all of the coins are all face up, or all face down &amp;ndash; if they are face down, then they will be face up the second time you ask.  Thus, the two states in \(S_\star\) &amp;ldquo;&lt;kbd&gt;coins all face up&lt;/kbd&gt;&amp;rdquo; and &amp;ldquo;&lt;kbd&gt;coins all face down&lt;/kbd&gt;&amp;rdquo; form an equivalence class of &amp;ldquo;solved states&amp;rdquo; according to the composite query described above &amp;ndash; I&amp;rsquo;ll call this composite query \(\texttt{solved?}^\star\).  We can now frame our problem as reaching any state in \(S_\star\).&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;three-more-equivalence-classes&#34;
    &gt;Three More Equivalence Classes&lt;a href=&#34;#three-more-equivalence-classes&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Consider the following three sets: \[S_1 = \Bigl\{{HH \atop HT}, {HH \atop TH}, {HT \atop HH}, {TH \atop HH}, {TT \atop TH}, {TT \atop HT}, {TH \atop TT}, {HT \atop TT}\Bigr\},\] which has either a single head or a single tail; \[S_2^{(d)} = \Bigl\{{HT \atop TH}, {TH \atop HT}\Bigr\},\] with two heads and two tails arranged diagonally from one and other; and \[S_2^{(s)} = \Bigl\{{HH \atop TT}, {HT \atop HT}, {TT \atop HH}, {TH \atop TH}\Bigr\},\] consisting again of two heads and two tails, but now sitting side-by-side.  Critically, the sets we have so far defined form a &lt;em&gt;partition&lt;/em&gt; of the space: \[S = S_\star \cup S_1 \cup S_2^{(s)} \cup S_2^{(d)},\] and thus more equivalence relations given by membership in these sets.  Moreover, each of these sets is invariant to both permutations \(\texttt{rotate}\) and \(\texttt{flip\_all}\).  Indeed, \(S_2^{(d)}\) is exactly one of the equivalence classes induced by both, \(S_2^{(s)}\) is an equivalence class induced by \(\texttt{rotate}\) and is the union of two equivalence classes of \(\texttt{flip\_all}\), and \(S_1\) is the union of two equivalence classes of \(\texttt{rotate}\) along with the corresponding ones of \(\texttt{flip\_all}\).  Since the elements of the sets are all equivalent to one and other (in the sense of being members of a partition, and in the sense of rotational symmetry) we can treat all members of the class simultaneously.  These sets, along with three more operations, will allow us to formalize a solution to the puzzle.&lt;/p&gt;
&lt;p&gt;Due to the invariance of these sets to the \(\texttt{rotate}\) permutation, the action of the rotation introduced by the interrogator when you query \(\texttt{solved?}\) does not have any affect on the membership of \(s\) in one of these sets.  This is, of course, &lt;em&gt;not&lt;/em&gt; true of any arbitrary subset of \(S\), and hence the choice of sets above is in some sense &amp;ldquo;cunning&amp;rdquo; (and now we see, well motivated by the consideration of equivalence classes!).  Indeed, rotations applied to \(\tilde{S} = \{{HH \atop TT}, {TT \atop HH}\}\) could produce something in \(\tilde{S}\), but it could also produce something in \(\{{HT \atop HT}, {TH \atop TH}\}\)!&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;more-permutations-on-s&#34;
    &gt;More Permutations on \(S\)&lt;a href=&#34;#more-permutations-on-s&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;The additional permutations we need to solve the puzzle are: \(\texttt{flip\_one}\), which flips over an arbitrary single coin; \(\texttt{flip\_side}\), which flips over two side-by-side coins; and \(\texttt{flip\_diag}\), which flips over arbitrary cross-diagonal coins.  These functions are not uniquely defined (&lt;em&gt;e.g.&lt;/em&gt;, which coin do you flip over for \(\texttt{flip\_one}\)) but the choice of which particular permutation you use does not matter, so I&amp;rsquo;m glossing over this.&lt;/p&gt;
&lt;p&gt;We can reason through the effects of how these map between our equivalence classes as follows (I skip the consideration of how \(S_\star\) is affected since we will always query \(\texttt{solved?}^\star\) between each operation, so if we hit \(S_\star\) we are done):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
\texttt{flip\_one}: S_1 \rightarrow S_2^{(d)} \cup S_2^{(s)} \cup S_\star\\
\texttt{flip\_one}: S_2^{(d)} \cup S_2^{(s)} \rightarrow S_1,
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;since if \(s \in S_1\) and we flip over a single coin, then we must now have either two heads and two tails, or we happened to flip over the &amp;ldquo;right&amp;rdquo; coin and reach a solved state.  Alternatively, if we begin with two $H$s and two $T$s, then flipping one coin over necessarily puts us back in \(S_1\) with either three $H$s and one \(T\), or three $T$s and one \(H\).  For the side-by-side flip operator:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
\texttt{flip\_side}: S_1 \rightarrow S_1\\
\texttt{flip\_side}: S_2^{(d)} \rightarrow S_2^{(s)}\\
\texttt{flip\_side}: S_2^{(s)} \rightarrow S_\star \cup S_2^{(d)},
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;since if there is one head (or one tail), application of \(\texttt{flip\_side}\) must necessarily result in three heads or tails; if there are heads diagonal from one and other (&lt;em&gt;i.e.&lt;/em&gt;, \(s \in S_2^{(d)}\)) then \(\texttt{flip\_side}\) must necessarily flip one head and one tail, and thus result in side-by-side heads and tails; and if \(s \in S_2^{(s)}\) then we either get lucky and flip the &amp;ldquo;right&amp;rdquo; coins into a solved state, or we flip the &amp;ldquo;wrong&amp;rdquo; coins into a diagonal state \(S_2^{(d)}\).  Finally, for the diagonal flipping operator:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
\texttt{flip\_diag}: S_1 \rightarrow S_1\\
\texttt{flip\_diag}: S_2^{(d)} \rightarrow S_\star\\
\texttt{flip\_diag}: S_2^{(s)} \rightarrow S_2^{(s)},
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;by similar reasoning.  This is the &lt;strong&gt;&lt;strong&gt;key to the puzzle&lt;/strong&gt;&lt;/strong&gt;.  If we are faced with some \(s \in S_2^{(d)}\), then application of \(\texttt{flip\_diag}\) is guaranteed to place us into the solved state \(S_\star\).  The only difficulty now is that we cannot &lt;em&gt;know&lt;/em&gt; whether or not \(s \in S_2^{(d)}\)!  Regardless, this feels like progress.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;transition-diagramming&#34;
    &gt;Transition Diagramming&lt;a href=&#34;#transition-diagramming&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;The simple enumeration of the action of the operators \(\texttt{flip\_one}, \texttt{flip\_side}, \texttt{flip\_diag}\) on the sets \(S_1, S_2^{(s)}, S_2^{(d)}\) described in the previous section can be difficult to visualize.  Our intuition is that we should try to find some appropriate order in which to apply these operations which will guarantee that we eventually reach the state \(S_\star\), but how?  We can make progress towards this task by drawing a diagram of all the possible transitions:&lt;/p&gt;


&lt;p&gt;One thing we can notice from this diagram is that some operations map from a set and back into itself, &lt;em&gt;i.e.&lt;/em&gt;, some of the sets described above are also invariant to \(\texttt{flip\_diag}\) or \(\texttt{flip\_side}\).  For example, \(\texttt{flip\_diag}\) maps from \(S_2^{(s)}\) back into itself, as well as from \(S_1\) and back into itself.  Recall that there is also an &lt;em&gt;implicit&lt;/em&gt; self-loop corresponding to the \(\texttt{rotate}\) permutation for each set, &lt;em&gt;i.e.&lt;/em&gt;, we can safely ask if the puzzle is solved (following the logic described in &lt;a
    class=&#34;link&#34;
    href=&#34;#solved-states&#34;&gt;Solved States&lt;/a
&gt;
) without the interrogator being able to effect the state we are in.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;a-solution&#34;
    &gt;A Solution&lt;a href=&#34;#a-solution&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Using the diagram above, we can deduce a solution to the puzzle.  Start in any arbitrary state \(s \in S\): first, make the \(\texttt{solved?}^\star\) query, if \(s \in S_\star\) then we are done (that was easy!), otherwise, we are in a state \[s \in S \setminus S_\star = S_1 \cup S_2^{(d)} \cup S_2^{(s)}.\]  Apply the operator \(\texttt{flip\_diag}\) and then query \(\texttt{solved?}^\star\) again.  If we happened to be in \(S_2^{(d)}\), then the puzzle is solved, otherwise (notice the self-loops) we must be in \(s \in S_2^{(s)} \cup S_1\).  Now, apply \(\texttt{flip\_side}\), followed by \(\texttt{solved?}^\star\), \(\texttt{flip\_diag}\), and \(\texttt{solved?}^\star\) again.  If we were in \(S_2^{(s)}\), then \(\texttt{flip\_side}\) transitioned us into \(S_2^{(d)}\), and \(\texttt{flip\_diag}\) brought us into \(S_\star\).  If we were in \(S_1\), then we are still in \(S_1\), since this set is invariant to both \(\texttt{flip\_diag}\) and \(\texttt{flip\_side}\).  To solve the puzzle from here, we apply \(\texttt{flip\_one}\) and query \(\texttt{solved?}^\star\).  If that query failed, then we must be in \(S_2^{(d)} \cup S_2^{(s)}\) in which case we can apply \(\texttt{flip\_diag}\) and query \(\texttt{solved}?^\star\), which will finish the puzzle if we were in \(S_2^{(d)}\).  Otherwise, we are in \(S_2^{(s)}\) and applying \(\texttt{flip\_side}\) followed by \(\texttt{solved?}^\star\) and then again \(\texttt{flip\_diag}\) which is finally guaranteed to place us into \(S_\star\), wherein a final \(\texttt{solved?}^\star\) query is the end.&lt;/p&gt;
&lt;p&gt;We have solved the puzzle &amp;ndash; regardless of our starting position, and regardless of how random or adversarial are the rotations to the plate of coins done by the interrogator (they can even know our exact plan!), the above policy will eventually move us into the solved state \(S_\star\).  To summarize the policy, we apply the following order of operations, querying \(\texttt{solved?}^\star\) between each operator: \(\texttt{flip\_diag}, \texttt{flip\_side}, \texttt{flip\_diag}, \texttt{flip\_one}, \texttt{flip\_diag}, \texttt{flip\_side}, \texttt{flip\_diag}\).&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;computational-generalizations&#34;
    &gt;Computational Generalizations&lt;a href=&#34;#computational-generalizations&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;Solving this particular brain-teaser is satisfying, but a solution is not a &lt;em&gt;good&lt;/em&gt; solution to me unless it is illustrative or indicative of a more general &lt;em&gt;pattern&lt;/em&gt;.  Indeed, I wrote out the formalism above exactly for the purpose of trying to reason through a more general idea.  To introduce the idea, suppose we have \(N\) states \(V = [N] \overset{\Delta}{=} \{1, 2, \ldots, N\}\) and \(K\) functions \(\mathcal{F} = \{f_k: V \rightarrow V\ |\ k \in [K]\}\) on \(V\).  Given this data, we can define a graph \(\mathcal{G} = (V, \mathcal{E})\) where the set of vertices is naturally \(V\) and the edges are tuples \(\mathcal{E} \subseteq \{(i, j, f)\ |\ i, j \in V, f \in \mathcal{F}\}\) where the edges \((i, j, f) \in \mathcal{E}\) is present in the graph if there is a function \(f \in \mathcal{F}\) which maps from node \(i\) to node \(j\).  Each node will have \(K\) edges emanating from it &amp;ndash; one for each function.  Let&amp;rsquo;s suppose as well that there is a fixed &lt;em&gt;target&lt;/em&gt; state \(s^\star \in S\) and that this state is invariant to every function application \(f_k(s^\star) = s^\star\), &lt;em&gt;i.e.&lt;/em&gt;, when we reach the target state, we know it.  This graph is analogous to the graph I drew for the coin problem.&lt;/p&gt;
&lt;p&gt;Now, the interesting part of this comes in when we suppose we do not know what state we are in.  The new question is: &lt;strong&gt;&lt;strong&gt;can we find a search algorithm which will find a sequence of functions leading to a goal state, but which does not require knowledge of what vertex it is currently visiting&lt;/strong&gt;&lt;/strong&gt;?  Abstractly, given a target state \(s^\star\), can we find a &lt;em&gt;finite&lt;/em&gt; (ideally minimal) sequence of functions \(f_1, f_2, \ldots, f_M \in \mathcal{F}\) which when applied from an arbitrary starting point, are guaranteed to eventually reach the target state \(s^\star\)?&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;uncertain-breadth-first-search&#34;
    &gt;Uncertain Breadth First Search&lt;a href=&#34;#uncertain-breadth-first-search&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;There may exist a better algorithm, but this problem can be solved by means of breadth first search.  However, we cannot just apply BFS directly upon the graph \(\mathcal{G}\), since we don&amp;rsquo;t know our starting point.  Instead, we&amp;rsquo;ll apply BFS on a graph over the &lt;em&gt;power set&lt;/em&gt; \(2^V\) of \(V\) (&lt;em&gt;i.e.&lt;/em&gt;, the set of all subsets of \(V\)).  We generalize the application of \(f \in \mathcal{F}\) to act on \(2^V\) by the definition \[ W\subseteq V:\ f(W) = \{f(w)\ |\ w \in W\},\] &lt;em&gt;i.e.&lt;/em&gt;, we apply \(f\) to every element of the subset.  This gives us an edge relation for a graph \(\mathcal{G}^\star = (2^V, \mathcal{E}^\star)\): we have \[(U, W) \in \mathcal{E}^\star \iff \bigl(\exists f \in \mathcal{F}:\ f(U) = W\bigr).\] The goal is to find a path through this &amp;ldquo;power-graph&amp;rdquo; leading to the state \(S_\star = \{s^\star\}\), the singleton set containing only the goal state.&lt;/p&gt;
&lt;p&gt;A completely hacked-together implementation of this idea for the coins-on-a-plate problem is given here:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;18
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;19
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;20
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;21
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;22
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;23
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;24
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;25
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;26
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;27
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;28
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;29
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;30
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;31
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;32
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;33
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;34
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;35
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;36
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;37
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;38
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;39
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;40
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;41
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;42
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;43
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;44
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;45
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;46
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;47
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;48
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;49
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;50
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;51
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;52
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;53
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;54
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;55
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;56
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;57
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;58
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;59
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; typing &lt;span style=&#34;color:#ff79c6&#34;&gt;as&lt;/span&gt; ty
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; functools &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; reduce
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; collections &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; deque
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Some nicer names for the nodes in the graph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Node &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;str&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;S1: Node &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;S1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;S2d: Node &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;S2d&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;S2s: Node &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;S2s&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;S_star: Node &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;S_star&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;TransitionFunction &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Callable[[Node], ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Set[Node]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# To easily keep track of what has been visited&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;_hash&lt;/span&gt;(s: ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Set[Node]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;hash&lt;/span&gt;(&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;join(&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;str&lt;/span&gt;(&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;hash&lt;/span&gt;(n)) &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; n &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; s))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Computes the set of next nodes obtained by applying `op` from some uncertainty set&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;_next_nodes&lt;/span&gt;(nodes: ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Set[Node], op: TransitionFunction):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; reduce(&lt;span style=&#34;color:#ff79c6&#34;&gt;lambda&lt;/span&gt; acc, new_nodes: acc &lt;span style=&#34;color:#ff79c6&#34;&gt;|&lt;/span&gt; new_nodes, (op(w) &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; nodes), &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;set&lt;/span&gt;())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# The robust implementation of BFS&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;uncertain_bfs&lt;/span&gt;(nodes: ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Set[Node], ops: ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Dict[&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;str&lt;/span&gt;, TransitionFunction]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    visited &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; {_hash(nodes)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    queue &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; deque([((&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;START&amp;#34;&lt;/span&gt;,), nodes)])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;while&lt;/span&gt; queue:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        path, nodes &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; queue&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;pop()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; name, op &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; ops&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;items():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            new_nodes &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; _next_nodes(nodes, op)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; (hsh &lt;span style=&#34;color:#ff79c6&#34;&gt;:=&lt;/span&gt; _hash(new_nodes)) &lt;span style=&#34;color:#ff79c6&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; visited:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                visited&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;add(hsh)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                queue&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;append(((&lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt;path, name), new_nodes))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; new_nodes &lt;span style=&#34;color:#ff79c6&#34;&gt;==&lt;/span&gt; {S_star}:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#f1fa8c&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;SUCCESS: &lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;{&lt;/span&gt;(&lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt;path, name)&lt;span style=&#34;color:#f1fa8c&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;FAILURE!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Helper to make transition functions with S_star being an absorbing state&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;_make_op&lt;/span&gt;(mapping: ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Dict[Node, Node]) &lt;span style=&#34;color:#ff79c6&#34;&gt;-&amp;gt;&lt;/span&gt; TransitionFunction:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;op&lt;/span&gt;(v: Node) &lt;span style=&#34;color:#ff79c6&#34;&gt;-&amp;gt;&lt;/span&gt; ty&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Set[Node]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#ff79c6&#34;&gt;==&lt;/span&gt; S_star:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; {S_star}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; mapping[v]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; op
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# The operations on the coins&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flip_diag &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; _make_op({S1: {S1}, S2d: {S_star}, S2s: {S2s}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flip_side &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; _make_op({S1: {S1}, S2d: {S2s}, S2s: {S2d, S_star}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flip_one &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; _make_op({S1: {S2d, S_star, S2s}, S2d: {S1}, S2s: {S1}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# The possible starting positions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nodes &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; {S1, S2d, S2s, S_star}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# And names for the transition functions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;transition_functions &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;flip_diag&amp;#34;&lt;/span&gt;: flip_diag, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;flip_one&amp;#34;&lt;/span&gt;: flip_one, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;flip_side&amp;#34;&lt;/span&gt;: flip_side}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Which we finally run the algorithm on&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;uncertain_bfs(nodes, transition_functions)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;\[\texttt{SUCCESS: (&amp;lsquo;START&amp;rsquo;, &amp;lsquo;flip\_diag&amp;rsquo;, &amp;lsquo;flip\_side&amp;rsquo;, &amp;lsquo;flip\_diag&amp;rsquo;, &amp;lsquo;flip\_one&amp;rsquo;, &amp;lsquo;flip\_diag&amp;rsquo;, &amp;lsquo;flip\_side&amp;rsquo;, &amp;lsquo;flip\_diag&amp;rsquo;)}\]&lt;/p&gt;
&lt;p&gt;And indeed, this code outputs the same policy I came up with ad-hoc in the previous section.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;conclusion&#34;
    &gt;Conclusion&lt;a href=&#34;#conclusion&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;I thought that the puzzle described at the beginning of this article was very neat and wanted to write down a complete solution.  We saw how the idea of symmetries were crucial for solving the puzzle, and that these symmetries can be described through the language of &lt;strong&gt;equivalence classes&lt;/strong&gt;.  Moreover, the remarkable cyclic decomposition theorem for &lt;strong&gt;permutations&lt;/strong&gt; (nothing but bijections on finite sets) tell us that we can &lt;strong&gt;partition&lt;/strong&gt; all the possible states of the coins on the plate into a collection of cycles and equivalence classes.  Some of these sets are &lt;strong&gt;invariant&lt;/strong&gt; to certain operations on the coins, and therefore create self-loops in a graph describing how the state of the puzzle evolves.  Diagramming this graph allowed us to write down an ad-hoc solution to the coins-on-a-plate puzzle.&lt;/p&gt;
&lt;p&gt;The idea of searching in a graph when you don&amp;rsquo;t know your starting point generalized to a form of &lt;strong&gt;robust breadth-first search&lt;/strong&gt;.  In this algorithm, we keep track of an entire &lt;strong&gt;uncertainty set&lt;/strong&gt; of nodes that we can plausibly be, and stop once that set contains nothing but the goal state.  I have few ideas for what such an algorithm might be practically useful for, but there it is!&lt;/p&gt;</description></item><item>
            <title>Lyapunov Stability, Linear Systems, and Semidefinite Programming</title>
            <link>https://rjtk.github.io/posts/lyapunov-stability-linear-systems-and-semidefinite-programming/</link>
            <pubDate>Sat, 17 Dec 2022 00:00:00 -0600</pubDate>
            <guid>https://rjtk.github.io/posts/lyapunov-stability-linear-systems-and-semidefinite-programming/</guid><description>&lt;p&gt;Dynamical systems are ubiquitous models occuring in science, engineering, and mathematics.  Not only are they used to model real-world dynamic phenomena like the dynamics of chemical plants, population growth, and physical engineered systems, they can also be applied to model algorithms themselves.  This post focuses on &lt;em&gt;linear&lt;/em&gt; dynamical systems, their analysis by means of semidefinite programming, and connections with control theory through the computation of quadratic functionals of their paths.&lt;/p&gt;
&lt;h2 class=&#34;group &#34; id=&#34;introduction&#34;
    &gt;Introduction&lt;a href=&#34;#introduction&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;This post introduces the Lyapunov approach to stability and convergence analysis.  While this approach and its connection to semidefinite programming may seem like &amp;ldquo;overkill&amp;rdquo; for linear systems (where you can just look at the eigenvalues of the system matrix), I believe it is an insightful starting point.  Analysis based on Lyapunov functions generalizes naturally to both nonlinear and stochastic systems, and the connection with semidefinite programming can be exploited to derive an analysis which may be, in some sense, &lt;em&gt;optimal&lt;/em&gt;.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;references&#34;
    &gt;References&lt;a href=&#34;#references&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Much of the material in this post is derived or inspired from: &lt;kbd&gt;Boyd, Stephen, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear matrix inequalities in system and control theory. Society for industrial and applied mathematics, 1994.&lt;/kbd&gt;  This is an &lt;a
    class=&#34;link&#34;
    href=&#34;https://web.stanford.edu/~boyd/lmibook/lmibook.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;easily accessible&lt;/a
&gt;
 book on &lt;em&gt;linear matrix inequalities&lt;/em&gt;.  I have also drawn some ideas and inspiration from the &lt;a
    class=&#34;link&#34;
    href=&#34;https://amzn.to/41kpMCU&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Convex Optimization book&lt;/a
&gt;
 (also &lt;a
    class=&#34;link&#34;
    href=&#34;https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;freely available&lt;/a
&gt;
) &lt;kbd&gt;Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.&lt;/kbd&gt; and have made use of &lt;a
    class=&#34;link&#34;
    href=&#34;https://www.cvxpy.org/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cvxpy&lt;/a
&gt;
 for many some of the examples: &lt;kbd&gt;Diamond, Steven, and Stephen Boyd. &amp;ldquo;CVXPY: A Python-embedded modeling language for convex optimization.&amp;rdquo; The Journal of Machine Learning Research 17, no. 1 (2016): 2909-2913.&lt;/kbd&gt;&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;linear-dynamical-systems&#34;
    &gt;Linear Dynamical Systems&lt;a href=&#34;#linear-dynamical-systems&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;Linear dynamical systems are generally characterized by a matrix \(A \in \R^{n \times n}\) which describes how the &lt;em&gt;state vector&lt;/em&gt; \(x \in \R^n\) changes through time.  For &lt;em&gt;continuous time&lt;/em&gt; dynamical systems, we work with ordinary differential equations&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\mathsf{d}}{\mathsf{d} t} x(t) = Ax(t),
\end{equation}&lt;/p&gt;
&lt;p&gt;usually written simply as \(\dot{x} = Ax\) where it is to be understood that \(x\) is a continuous function of time \(t\) and \(\dot{x}\) denotes differentiation with respect to \(t\).  In this case, \(A\) describes a &lt;em&gt;velocity field&lt;/em&gt; in state space &amp;ndash; each point \(x \in \R^n\) is associated to some velocity \(Ax\).  Analogously, for &lt;em&gt;discrete time&lt;/em&gt; dynamical systems, we have an iterative relationship&lt;/p&gt;
&lt;p&gt;\begin{equation}
x(t + 1) = Ax(t),
\end{equation}&lt;/p&gt;
&lt;p&gt;where in this case the matrix \(A\) specifies the next state of the system.&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;: I like to use $t$ for both discrete and continuous time.  The reader is trusted to recognize which is which (or if it matters at all) from the context.&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;Somewhat more generally, we could work with systems having some &lt;em&gt;constant offset&lt;/em&gt; \(b \in \R^n\) as in \(\dot{x} = Ax + b\).  However, if \(A\) is a full-rank matrix (meaning that it is invertible), then this additional offset term is not interesting.  Indeed, we can just as-well shift the coordinates of the system and instead work with \(\dot{z} = Az\) where \(z = x - A^{-1} b\).  Anything interesting that can be established for \(x\) can be done so by first doing it for \(z\).  If the matrix \(A\) is &lt;em&gt;not&lt;/em&gt; invertible, then there is a subspace which the matrix \(A\) has no effect upon, and this subspace can be analyzed separately.  It will become more-or-less obvious how to do this as we proceed, so I will make the standing assumption that \(A\) is full-rank and \(b = 0\).&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;: The case of linear systems with a general input $\dot{x}(t) = Ax(t) + Bu(t)$ on the other hand are quite interesting, and is likely to be the topic of some future post.&lt;/kbd&gt;&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;examples&#34;
    &gt;Examples&lt;a href=&#34;#examples&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Firstly, it should be understood that much of the analysis of &lt;em&gt;nonlinear&lt;/em&gt; dynamical systems, &lt;em&gt;i.e.,&lt;/em&gt; ordinary differential equations of the form \(\dot{x} = f(x)\), comes down to the linear case.  The reason lies in the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hartman-Grobman Theorem&lt;/a
&gt;
 (which we have also &lt;a
    class=&#34;link&#34;
    href=&#34;https://rjtk.github.io/posts//solving-equations-with-jacobi-iteration/&#34;&gt;already seen&lt;/a
&gt;
): the nonlinear system \(\dot{x} = f(x)\) can be approximated by the &lt;em&gt;linear&lt;/em&gt; dynamical system \(\dot{x} = Ax\) for in a neighbourhood of an &lt;em&gt;equilibrium point&lt;/em&gt; (&lt;em&gt;i.e.,&lt;/em&gt; point \(x_e\) such that \(f(x_e) = 0\)) with the matrix \(A = \mathsf{D} f(x_e)\), the derivative of \(f\).  More examples follow.&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;gradient-descent&#34;
    &gt;Gradient Descent&lt;a href=&#34;#gradient-descent&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;One of the modern drivers of interest in systems theory is for applications in machine learning, or more specifically, optimization algorithms.  One of the most famous classical algorithms for optimization is &lt;em&gt;gradient descent&lt;/em&gt;: given a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) that we want to minimize, the gradient descent algorithm proceeds through the iterates \[x(t + 1) = x(t) - \alpha \nabla f(x(t)),\] where \(\nabla f(x) = (\mathsf{D} f(x))^{\mathsf{T}}\) is the gradient and \(\alpha &amp;gt; 0\) is a step-size parameter.  In general, this is a nonlinear system.  However, there is a particularly important special case when \(\nabla f(x)\) is a linear function of \(x\): when \(f(x) = \frac{1}{2}x^{\mathsf{T}} Q x\) is a quadratic function.  In this case, \(\nabla f(x) = Qx\) and gradient descent \[x(t + 1) = (I - \alpha Q)x(t)\] is a linear system with $A = I - α Q.$&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;electronic-circuits&#34;
    &gt;Electronic Circuits&lt;a href=&#34;#electronic-circuits&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;Another interesting example comes from electrical engineering.  In the design of electronic circuits, there are devices called &lt;em&gt;capacitors&lt;/em&gt; (basically two metal plates sandwiched next to each other) which can store a small electric charge and then subsequently release it.  Similarly, &lt;em&gt;inductors&lt;/em&gt; (coils of wire) store &lt;em&gt;magnetic&lt;/em&gt; energy, and later release it.&lt;/p&gt;
&lt;p&gt;Using these devices, along with simple resistors, we can construct an &lt;em&gt;RLC circuit&lt;/em&gt; &amp;ndash; a circuit consisting of resitors, inductors, and capacitors (the &amp;lsquo;L&amp;rsquo; for inductors coming from the scientist Heinrich Lenz).  One of the simplest such circuits is to just place these three elements in a loop with each other.&lt;/p&gt;


&lt;p&gt;Using &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws#Kirchhoff%27s_voltage_law&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kirchoff&amp;rsquo;s voltage law&lt;/a
&gt;
, which says that the total voltage differences around a loop must sum to zero, we can obtain the equation \(V_R(t) + V_L(t) + V_C(t)\) where \(V_R, V_L, V_C\) are the voltages across the resistor, inductor, and capacitor at time \(t\), respectively.&lt;/p&gt;
&lt;p&gt;The needed facts for this analysis are the  the &lt;em&gt;current-voltage relationships:&lt;/em&gt; $V_R(t) = RI_R(t),$ \(V_L(t) = L\dot{I}_L(t)\), and \(V_C(t) = \frac{1}{C}\int_0^t I_C(\tau)\mathsf{d}\tau\) with \(I_R, I_L, I_C\) being the currents passing through the devices, and \(R, L, C\) denoting the &amp;ldquo;size&amp;rdquo; of the devices in units of Ohms (resistance), Henries (inductance), and Farads (capacitance).  These are nothing but functions which describe how the devices operate.  The other key observation is that, since these devices are all placed in series one after the other along the same wire: \(I_R = I_L = I_C\)!  Thus, we have obtained the integro-differential equation:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
RI(t) + L\dot{I}(t) + \frac{1}{C}\int_0^t I(\tau) \mathsf{d} \tau &amp;amp;= 0\\
\implies R\dot{I}(t) + L\ddot{I}(t) + \frac{1}{C}I(t) &amp;amp;= 0,
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;which by differentiating through the whole thing results in a second order ODE.  We now intend to play a clever trick with this equation &amp;ndash; a common source of linear systems.  We think of the state variables as being \(x(t) = \bigl(\dot{I}(t), I(t)\bigr)\), and by using the relationship between these derivatives described above we obtain:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{bmatrix}
\ddot{I}\\
\dot{I}
\end{bmatrix}=
\begin{bmatrix}
-\frac{R}{L} &amp;amp; -\frac{1}{CL}\\
1 &amp;amp; 0\\
\end{bmatrix}
\begin{bmatrix}
\dot{I}\\
I
\end{bmatrix},
\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;which is a linear dynamical system.  Simulation of this system is straightforward.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; scipy.integrate &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; solve_ivp
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;simulate&lt;/span&gt;(t, R, C, L, I0&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1.0&lt;/span&gt;, I_dot0&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;0.0&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#bd93f9&#34;&gt;1.0&lt;/span&gt;], [&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; (C &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; L), &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;R &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; L]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;ode&lt;/span&gt;(I):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; A &lt;span style=&#34;color:#ff79c6&#34;&gt;@&lt;/span&gt; I
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    res &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; solve_ivp(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fun&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;lambda&lt;/span&gt; t, y: ode(y),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        t_span&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(t[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;], t[&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y0&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;array([I0, I_dot0]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        t_eval&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;t
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; (res&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;y[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;, :], res&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;y[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;, :])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The reader unfamiliar with this trick is encouraged to study this example, as the technique is very commonly used.  The behaviour of the system can now be understood by analyzing the dynamics of this ODE given some set of initial conditions \(I(0), \dot{I}(0)\).&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;semidefinite-programming&#34;
    &gt;Semidefinite Programming&lt;a href=&#34;#semidefinite-programming&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;In convex optimization, a &lt;a
    class=&#34;link&#34;
    href=&#34;https://web.stanford.edu/~boyd/papers/pdf/semidef_prog.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;semidefinite program&lt;/a
&gt;
 is an optimization problem involving a matrix variable \(X\), a linear cost function \(J(x) = \mathsf{tr}\ C^{\mathsf{T}} X = \sum_{i, j} C_{ij} X_{ij}\), a linear equality constraint \(\mathcal{A}(X) = 0\), and a constraint \(X \succeq 0\), which means that \(X\) must be a &lt;em&gt;positive semidefinite matrix&lt;/em&gt;.  If you are not familiar with what this is, it is a matrix which is (1) &lt;em&gt;symmetric&lt;/em&gt; and (2) has &lt;em&gt;non-negative eigenvalues&lt;/em&gt;.  A generic SDP is as follows:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
\underset{X \in \R^{n \times n}}{\text{minimize}}&amp;amp;\quad \mathsf{tr}\ C^{\mathsf{T}} X\\
\text{subject to}
&amp;amp;\quad \mathcal{A}(X) = 0\\
&amp;amp;\quad X \succeq 0.
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;The linear function \(\mathcal{A}(X) = 0\) can be any linear function of \(X\) &amp;ndash; examples include \(AX = 0\), \(AXA^{\mathsf{T}} - X + Q = 0\), &lt;em&gt;etc.&lt;/em&gt;  Of course, much more general SDPs are possible, but this will be adequate for our purposes.&lt;/p&gt;
&lt;p&gt;Practically speaking, solving (relatively small: \(n &amp;lt; 100\)) SDPs is fairly straightforward.  Here is a basic &lt;a
    class=&#34;link&#34;
    href=&#34;https://www.cvxpy.org/&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CVX Program&lt;/a
&gt;
 to solve an SDP we will encounter shortly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;18
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;19
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; math &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; inf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; scipy.linalg &lt;span style=&#34;color:#ff79c6&#34;&gt;as&lt;/span&gt; la
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; cvxpy &lt;span style=&#34;color:#ff79c6&#34;&gt;as&lt;/span&gt; cvx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;solve_lyapunov_sdp&lt;/span&gt;(A, g, verbose&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;True&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Solves the feasibility problem P &amp;gt; 0, (1 - g) * P - A&amp;#39; P A &amp;gt; 0.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n, _ &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; A&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;shape
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    P &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; cvx&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Variable(shape&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(n, n), symmetric&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    I &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;eye(n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    objective &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; cvx&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Minimize(&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    constraints &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        P &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; I,  &lt;span style=&#34;color:#6272a4&#34;&gt;# P is positive semi-definite&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; g) &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; P &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; A&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;T &lt;span style=&#34;color:#ff79c6&#34;&gt;@&lt;/span&gt; P &lt;span style=&#34;color:#ff79c6&#34;&gt;@&lt;/span&gt; A &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; I
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    problem &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; cvx&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Problem(objective, constraints&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;constraints)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    problem&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;solve(solver&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;CVXOPT&amp;#34;&lt;/span&gt;, verbose&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;verbose)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; problem&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;value, P&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;value
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Given a matrix \(A\), and a constant \(\gamma\), this program will solve a semidefinite &lt;em&gt;feasibility program&lt;/em&gt; to find a matrix \(P \succeq I\) such that \((1 - \gamma) P - A^{\mathsf{T}} P A \succeq I\).  This is actually just a computational means of solving a special type of feasability problem called a &lt;em&gt;linear matrix inequality&lt;/em&gt; (LMI).  Usually, LMIs are &lt;em&gt;strict&lt;/em&gt; inequalities where it is required to find \(P \succ 0\) and \((1 - \gamma) P - A^{\mathsf{T}} P A \succ 0\), and any such \(P\) will do.  However, since optimization problems are not usually well defined when strict inequalities are involved (since the feasible region is then not closed, &lt;em&gt;c.f.,&lt;/em&gt; &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Extreme_value_theorem&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weierstrass&amp;rsquo; Theorem&lt;/a
&gt;
) it is necessary to modify the LMI to involve non-strict inequalities.  Doing this in a naive way, simply replacing \(\succ\) by \(\succeq\), may result in a trivial output \(P = 0\).  However, since the LMI is linear in \(P\), we can just scale the inequality and ask that it be \(\succeq I\) &amp;ndash; indeed, if any \(P \succ 0\) exists which satisfies the inequality, there must also exist one satisfying \(P \succeq I\).&lt;/p&gt;
&lt;p&gt;The meaning of this LMI will be explained next.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;lyapunov-stability-theory&#34;
    &gt;Lyapunov Stability Theory&lt;a href=&#34;#lyapunov-stability-theory&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;The most important question to answer when it comes to linear systems is &lt;em&gt;stability&lt;/em&gt;.  The term stability is used to invoke the idea of a system which, when slightly perturbed, returns back to its nominal position.  However, it might be somewhat of a misnomer, as what is often really meant is &lt;em&gt;convergence&lt;/em&gt;: given some starting point \(x(0)\), does the linear system satisfy \(x(t) \rightarrow 0\ \text{as}\ t \rightarrow \infty\).  There are many methods of determining this, the most natural being an analysis of the eigenvalues of the system matrix \(A\).  Analyzing the eigenvalues however, is a method which is quite particular to the case of simple linear systems.  Another method, which has a much greater deal of potential for generalization, is the method of analysis by means of &lt;em&gt;Lyapunov Stability Theory&lt;/em&gt;.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;lyapunov-stability&#34;
    &gt;Lyapunov Stability&lt;a href=&#34;#lyapunov-stability&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;One of the most important techniques for establishing system stability is &lt;em&gt;Lyapunov&amp;rsquo;s direct method&lt;/em&gt; &amp;ndash; I&amp;rsquo;ll explain the idea for discrete time systems, although much of the literature focuses on the continuous time case.&lt;/p&gt;
&lt;p&gt;The idea is to find a so-called &lt;em&gt;Lyapunov function&lt;/em&gt;  \(V: \R^n \rightarrow \R\) on the state space which captures some generalized notion of &amp;ldquo;kinetic energy&amp;rdquo; of the system.  A Lyapunov function must satisfy a few properties.  The first of which is that $V(0) = 0,$ but \(V(x) &amp;gt; 0\) everwhere else (&lt;em&gt;i.e.,&lt;/em&gt; everywhere that \(x \ne 0\)).  I will call this property &lt;em&gt;positivity&lt;/em&gt;, and it is this property that makes a Lyapunov function analogous to a generalized notion of kinetic energy: it is positive except at an equilibrium point when the system stops moving.  The next key property is that \(V\) must be constructed such that its value decreases along the path traced by the system.  That is, if it can be shown that \(V(x(t + 1)) &amp;lt; V(x(t))\) for the system \(x(t + 1) = Ax(t)\), with \(x(0) = x_0 \in \R^n\) being an arbitrary initial condition, then the system can be expected to converge towards \(0\).&lt;/p&gt;
&lt;p&gt;These two properties (positivity and decreasing along trajectories) are not the whole story though.  The additional technical property of &lt;em&gt;coerciveness&lt;/em&gt; is required: \(||x|| \rightarrow \infty \implies V(x) \rightarrow \infty\).  This appears like an esoteric technical requirement, but it is essential.  If \(V\) is not coercive, then the &lt;em&gt;level sets&lt;/em&gt; of \(V\), the sets \(L_\alpha = \{x \in \R^n\ |\ V(x) \le \alpha\}\) may be unbounded, and the trajectory \(x(t)\) can drift off \(x(t) \rightarrow \infty\) while \(V(x(t))\) is stil decreasing.&lt;/p&gt;
&lt;p&gt;In the linear case, excellent &lt;em&gt;Lyapunov function candidates&lt;/em&gt; (i.e., functions \(V\) which you hope will serve as Lyapunov functions) are quadratic forms \(V(x) = x^{\mathsf{T}} P x\), for some matrix \(P \in \R^{n \times n}\).  It is quite easy to determine when these functions are positive &amp;ndash; this is the case exactly when \(P \succ 0\) (the matrix \(P\) is positive-definite), and moreover, \(V(x)\) will be coercive when \(P \succ 0\) (the matrix \(P\) is positive definite).  Thus, it makes sense to refer to positive coercive functions \(V\) as being positive definite; we can refer to a Lyapunov function for a system as being a positive definite function which is decreasing along trajectories of the system.&lt;/p&gt;
&lt;p&gt;So, in the linear case we have the candidate function \(V(x) = x^{\mathsf{T}} P x\) and the requirement \(P &amp;gt; 0\).  All that remains is to figure out if \(V(x(t + 1)) &amp;lt; V(x(t))\) along trajectories of the system.  This can be established by verifying \(V(Ax) &amp;lt; V(x)\) (why?), that is:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
\forall x \in \R^n:\ V(Ax) - V(x) &amp;lt; 0
&amp;amp;\iff \forall x \in \R^n:\ x^{\mathsf{T}}A^{\mathsf{T}}PAx - x^{\mathsf{T}}Px &amp;lt; 0\\
&amp;amp;\iff \forall x \in \R^n:\ x^{\mathsf{T}}\bigl(A^{\mathsf{T}} P A - P\bigr)x &amp;lt; 0\\
&amp;amp;\iff A^{\mathsf{T}} P A - P \prec 0.
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;If we are able to simply find some matrix \(P \succ 0\) such that \(V(x)\) is a Lyapunov function, then we have established the convergence of our system.  The astute reader will notice that this is a linear matrix inequality and can be solved by means of the above semidefinite feasibility program (with \(\gamma = 0\)).&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;: Lyapunov stability is a fundamental tool for establishing convergence in nonlinear systems.  One of the most elegant examples is that of the gradient flow $\dot{x} = -\nabla f(x)$ for a strongly convex function $f$ (making this assumption for simplicity).  In this case, the function $f$ itself can serve as a Lyapunov function:  That is, $\frac{\mathsf{d}}{\mathsf{d}t} f(x) = \nabla f(x)^{\mathsf{T}} \dot{x} = -||\nabla f(x)||_2^2 &amp;lt; 0$ except at $x = x^\star$, the minimizer where $\nabla f(x^\star) = 0$.  Incidentally, this also tells us that the gradient flow monotonically decreases the function $f$.  Convergence proofs of &lt;em&gt;many&lt;/em&gt; algorithms come down to constructing an appropriate Lyapunov function.&lt;/kbd&gt;&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;example&#34;
    &gt;Example&lt;a href=&#34;#example&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;Consider a simple example with&lt;/p&gt;
&lt;p&gt;\begin{equation}
A = \begin{bmatrix}
0.750 &amp;amp; 1.00\\
-0.667 &amp;amp; 0.111
\end{bmatrix}.
\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;Notice that \(A\) does not have any particularly simple properties (triangularity, diagonal dominance&amp;hellip;) that would allow us to determine at a glance (unless we are rather speedy when it comes to calculating eigenvalues of \(2 \times 2\) matrices!) that the system \(x(t + 1) = Ax(t)\) is stable.  However, we can plug this matrix into our trusty semidefinite program and obtain a matrix&lt;/p&gt;
&lt;p&gt;\begin{equation}
P = \begin{bmatrix}
8.419 &amp;amp; 3.636\\
3.636 &amp;amp; 12.109
\end{bmatrix},
\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;which proves stability by Lyapunov&amp;rsquo;s method.&lt;/p&gt;


&lt;p&gt;I&amp;rsquo;ve plotted above a figure showing the value of various functions of the state \(x(t)\) for the example.  The main observation is that while they all are roughly &amp;ldquo;decreasing&amp;rdquo;, it is only the Lyapunov function which is decreasing &lt;em&gt;monotonically&lt;/em&gt;.  That is not to say that this Lyapunov function is the &lt;em&gt;only&lt;/em&gt; function with this property &amp;ndash; there are many Lyapunov functions.  More subtly, it may also be possible to find starting points such that some other arbitrary function decreases monotonically towards zero from that particular system initialization, but a Lyapunov function will do so from &lt;em&gt;any&lt;/em&gt; starting point.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;fast-convergence&#34;
    &gt;Fast Convergence&lt;a href=&#34;#fast-convergence&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Can we modify Lyapunov&amp;rsquo;s method to somehow establish &lt;em&gt;fast&lt;/em&gt; convergence?  The answer is yes.  Consider the stronger condition:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
V(Ax) - V(x) &amp;amp;&amp;lt; -\gamma V(x)\\
\implies V(Ax) &amp;amp;&amp;lt; (1 - \gamma)V(x)\\
\implies V(A^2 x) &amp;amp;&amp;lt; (1 - \gamma)V(Ax) &amp;lt; (1 - \gamma)^2 V(x)\\
&amp;amp;\vdots\\
\implies V(A^T x) &amp;amp;&amp;lt; (1 - \gamma)^T V(x).
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;which is establishing a &lt;em&gt;rate of convergence&lt;/em&gt; on the Lyapunov function &amp;ndash; after \(T\) iterations of the system, the value must have decreased by a factor of \((1 - \gamma)^T\).  It is quite convenient to recognize that the existence of a quadratic Lyapunov function \(V_\gamma\) which verifies this fast convergence is equivalent to finding an ordinary quadratic Lyapunov function for the modified system with \(A_\gamma = \frac{1}{\sqrt{1 - \gamma}} A\).&lt;/p&gt;
&lt;p&gt;How do we incorporate the search for some such \(\gamma\) into our SDP?  All we need is that \(A^{\mathsf{T}}PA - (1 - \gamma)P \prec 0\), and indeed, we would like to find the largest satisfactory \(\gamma\).  Unfortunately, I do not believe it is possible to directly encode optimization over \(\gamma\) into a convex SDP, since the product $γ P $, where both \(\gamma\) and \(P\) are variables, is non-convex.  Instead, since \(\gamma \in \R\) is just a single parameter, we can apply &lt;em&gt;bisection&lt;/em&gt; to find an optimal \(\gamma\):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; math &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; inf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;is_feasible&lt;/span&gt;(A, g, verbose&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;True&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Checks if P &amp;gt; 0, (1 - g) * P - A&amp;#39; P A &amp;gt; 0 is feasible.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    val, _ &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; solve_lyapunov_sdp(A, g, verbose&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;verbose)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; val &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;lt;&lt;/span&gt; inf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;find_optimal_convergence_rate&lt;/span&gt;(A, num_iter&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;20&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    right &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    left &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; it &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;range&lt;/span&gt;(num_iter):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        g &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; left &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; (right &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; left) &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; is_feasible(A, g, verbose&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;False&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            left &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; g
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            right &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; g
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; g
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;By solving the feasability SDP multiple times, we obtain a value of \(\gamma\) such that \(V(A^T x) &amp;lt; (1 - \gamma)^T V(x)\), establishing a fast convergence rate for the system.  It should be noted that specialized algorithms to solve the Lyapunov inequality \(A^{\mathsf{T}} P A - P \prec 0\), but doing so by semidefinite programming opens up a more general array of possibilities, which I turn to next.&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;em&gt;Remark&lt;/em&gt;: It is possible to represent this optimization problem as a joint optimization problem over $P, \gamma$, but not as a &lt;em&gt;convex&lt;/em&gt; optimization problem.  However, the resulting problem is &lt;em&gt;quasiconvex&lt;/em&gt;.&lt;/kbd&gt;&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;stability-of-uncertain-linear-systems&#34;
    &gt;Stability of Uncertain Linear Systems&lt;a href=&#34;#stability-of-uncertain-linear-systems&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;So far, I&amp;rsquo;ve talked about linear systems with a single and perfectly well known matrix \(A\).  However, in many applications it may be the case that the matrix \(A\) is not known exactly, or that it is only an approximation of a real system.  For this reason, we would like to establish some degree of &lt;em&gt;robust&lt;/em&gt; stability guarantee.  To this end, let us suppose that we have some &lt;strong&gt;uncertainty set&lt;/strong&gt; \(\mathbf{A} \subset \R^{n \times n}\) and that our system matrix \(A \in \mathbf{A} \subset \R^{n \times n}\).  If this is the case, then we can expect our linear system to satisfy a linear &lt;strong&gt;recursive inclusion&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
x(t + 1) \in \mathbf{A} x(t) \overset{\Delta}{=} \{A x(t)\ |\ A \in \mathbf{A}\}. \notag
\end{equation}&lt;/p&gt;
&lt;p&gt;That is, the next point \(x(t + 1)\) is obtained from \(x(t)\) by simple multiplication by a matrix \(A\), we just don&amp;rsquo;t know &lt;strong&gt;which&lt;/strong&gt; matrix \(A\); all we know is that \(A \in \mathbf{A}\).  Amazingly, establishing the stability of this system can still be done by looking for a Lyapunov function \(V(x) = x^{\mathsf{T}} P x\) with \(P \succ 0\) and such that \(\forall x \in \R^n, A \in \mathbf{A}:\ V(A x) - V(x) &amp;lt; 0\) and this is equivalent to the collection of LMIs: \[P \succ 0, \forall A \in \mathbf{A}:\ A^{\mathsf{T}} P A - P \prec 0.\]&lt;/p&gt;
&lt;p&gt;Of course, not all such LMIs can be solved &amp;ndash; it depends on the model of \(\mathbf{A}\).  A quite flexible model is furnished by &lt;strong&gt;polytopic&lt;/strong&gt; sets: \(\mathbf{A} = \mathsf{conv}\{ A_k\ |\ k \in [K] \}\), that is, the convex hull of a &lt;strong&gt;finite&lt;/strong&gt; number of matrices \(A_k\), &lt;em&gt;i.e.,&lt;/em&gt; for any \(A \in \mathbf{A}\) there exists some \(\mu \in [0, 1]\) and matrices \(A_i, A_j\) such that \(A = \mu A_i + (1 - \mu) A_j\).  These sets can be used to construct arbitrarily accurate approximations of any convex set \(\mathbf{A}\).&lt;/p&gt;
&lt;p&gt;Then, using the fact that \(V(x)\) is convex if \(P \succ 0\), we have \(V(\mu A_i x + (1 - \mu) A_j x) \le \mu V(A_i x) + (1 - \mu) V(A_j x)\) and therefore a sufficient condition for the stability of the recursive inclusion is to find some \(P \succ 0\) which satisfies the system of \(K\) LMIs: \[P \succ 0, \forall k \in [K]:\ A_k^{\mathsf{T}} P A_k - P \prec 0.\]&lt;/p&gt;
&lt;p&gt;Speaking intuitively, if such a \(P\) exists for some uncertain linear system, we can make a conclusion about the &lt;strong&gt;robust&lt;/strong&gt; stability of the system.&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;example-gain-margins-and-momentum-gradient-descent&#34;
    &gt;Example &amp;ndash; Gain Margins and Momentum Gradient Descent&lt;a href=&#34;#example-gain-margins-and-momentum-gradient-descent&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;One of the main motivations for considering uncertain linear systems arises from control theory.  A very common situation is that we have some system matrix \(A\), as well as a &lt;em&gt;gain matrix&lt;/em&gt; \(K\) which we have designed.  The gain matrix \(K\) is intended to be used to steer the system \(x(t + 1) = A x(t)\) towards zero by means of a &lt;em&gt;feedback control&lt;/em&gt; \(x(t + 1) = Ax(t) - Kx(t)\).  Importantly, the system \(A\) may very well be unstable, in which case the matrix \(K\) is absolutely essential.&lt;/p&gt;
&lt;p&gt;Suppose that we have some \(K\) in hand which makes the controlled system \(x(t + 1) = (A - K)x(t)\) stable.  The idea of a &lt;em&gt;gain margin&lt;/em&gt; is to determine if for some interval \([\alpha_1, \alpha_2]\) that the modified system \(x(t + 1) = (A - \alpha K) x(t)\) remains stable, for any \(\alpha \in [\alpha_1, \alpha_2]\).  If this interval is wide, then it may give us some assurance that the matrix \(K\) is robust, in some sense.  The collection of system matrices \[\mathbf{A} = \{A - \alpha K\ |\ \alpha_1 \le \alpha \le \alpha_2\}\] forms a natural polytopic set.&lt;/p&gt;
&lt;p&gt;Gradient descent constitutes a perfectly good example of this situation.  For a quadratic function \(f(x) = \frac{1}{2}x^{\mathsf{T}} Q x\) we have non-stable natural dynamics \(x(t + 1) = x(t)\), a controller \(Q\), and the stepsize \(\alpha\) fills in for the gain margin, giving us \(x(t + 1) = (I - \alpha Q)x(t)\).  To make this a bit more interesting, we can augment this system with a &lt;em&gt;momentum&lt;/em&gt; term by introducing a &lt;em&gt;velocity&lt;/em&gt; \(v(t)\):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
v(t + 1) &amp;amp;= \gamma v(t) + \alpha Q x(t)\\
x(t + 1) &amp;amp;= x(t) - v(t + 1).
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;These equations are known as &lt;a
    class=&#34;link&#34;
    href=&#34;https://arxiv.org/pdf/1609.04747.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient descent with momentum&lt;/a
&gt;
.  The idea is that if we are heading in some &amp;ldquo;nominal&amp;rdquo; direction \(v(t)\), we might as well keep going in that direction, and just add the gradient step \(\alpha Qx(t)\) to our velocity.  A slight modification of this momentum scheme, Nesterov&amp;rsquo;s momentum, is known to be optimal in a certain sense.&lt;/p&gt;
&lt;p&gt;In any case, the ordinary Momentum gradient descent algorithm corresponds to the (block) system matrix&lt;/p&gt;
&lt;p&gt;\begin{equation}
A_{\gamma, \alpha} =
\begin{bmatrix}
\gamma I &amp;amp; \alpha Q\\
-\gamma I &amp;amp; I - \alpha Q
\end{bmatrix}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;for the joint system \(\bigl(v(t), x(t)\bigr)\).&lt;/p&gt;
&lt;p&gt;Now, if you can construct a Lyapunov function (by solving a semidefinite program) for \(\gamma, \alpha \in \{\gamma_1, \gamma_2\} \times \{\alpha_1, \alpha_2\}\) (&lt;em&gt;i.e.&lt;/em&gt;, all four corners of a square) then you will have proof that the algorithm will converge for every pair \(\gamma, \alpha \in [\gamma_1, \gamma_2] \times [\alpha_1, \alpha_2]\).&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;: This example is quite simplistic, but it is illustrative of&lt;/kbd&gt; &lt;a
    class=&#34;link&#34;
    href=&#34;https://arxiv.org/pdf/1502.02009.pdf&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;general and powerful techniques&lt;/a
&gt;
. &lt;kbd&gt;Specifically, if you can construct and solve an appropriate SDP &lt;em&gt;analytically&lt;/em&gt;, then it can serve as a constructive proof of convergence.  As well, SDPs can potentially be used to find &lt;em&gt;optimal&lt;/em&gt; parameters for algorithms by similar means as were used earlier to calculate convergence rates by means of bisection search.&lt;/kbd&gt;&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;quadratic-integrals&#34;
    &gt;Quadratic Integrals&lt;a href=&#34;#quadratic-integrals&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;As a final illustration of the power of the Lyapunov approach, let&amp;rsquo;s consider how to evaluate infinite quadratic integrals (or summations, in the discrete case).  Say we have a linear system \(x(t + 1) = Ax(t)\), and are interested in computing the value of a of the path of this system according to the quadratic functional&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
J_Q(x_0) = \sum_{t = 0}^\infty x(t)^{\mathsf{T}} Q x(t); x(0) = x_0,
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;where \(Q \succ 0\) is some arbitrary positive definite matrix.  Such functions arise often in control theory.&lt;/p&gt;
&lt;p&gt;To see the connection with Lyapunov theory, recognize that we can fully expand out the summation in terms of \(x_0\) by using the fact that \(x(t) = A^t x_0\):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
J_Q(x_0)
&amp;amp;= \sum_{t = 0}^\infty \bigl(A^t x_0 \bigr)^{\mathsf{T}} Q \bigl(A^t x_0)\\
&amp;amp;= x_0^{\mathsf{T}} \Bigl( \sum_{t = 0}^\infty (A^t)^{\mathsf{T}} Q A^t \Bigr)x_0\\
&amp;amp;= x_0^{\mathsf{T}} Q x_0 + \Bigl( \sum_{t = 1}^\infty (A^t)^{\mathsf{T}} Q A^t \Bigr)x_0\\
&amp;amp;= x_0^{\mathsf{T}} \Bigl(Q + \sum_{t = 1}^\infty (A^t)^{\mathsf{T}} Q A^t \Bigr)x_0\\
&amp;amp;= x_0^{\mathsf{T}} \Bigl(Q + A^{\mathsf{T}} \bigl[\sum_{t = 0}^\infty (A^t)^{\mathsf{T}} Q A^t \bigr]A \Bigr)x_0\\
&amp;amp;= x_0^{\mathsf{T}} Q x_0 + J(Ax_0).
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;Thus, since \(J\) is a quadratic function of \(x_0\), there must be some \(P\) such that \(J(x_0) = x_0^{\mathsf{T}} P x_0\) and therefore \(J(x)\) satisfies, for any starting point \(x \in \R^n\):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
J(x)
&amp;amp;= x^{\mathsf{T}} P x\\
&amp;amp;= x^{\mathsf{T}}[Q + A^{\mathsf{T}} P A] x,
\end{aligned}\notag
\end{equation}&lt;/p&gt;
&lt;p&gt;or in other words, \(P - A^{\mathsf{T}} P A = Q\).  What this tells us is that if we have a matrix \(P \succ 0\) verifying the Lyapunov inequality \(P - A^{\mathsf{T}} P A \succ 0\), then not only do we have a Lyapunov function, but there is some positive definite matrix \(Q = P - A^{\mathsf{T}} P A\) such that \(x^\mathsf{T} P x\) is the value of the quadratic functional \(J_Q(x)\).  Conversely, if we can solve the Lyapunov equation \(P - A^{\mathsf{T}} P A = Q\) for some \(P \succ 0\), then we have a Lyapunov function, as well as a function for easily evaluating the value of \(J_Q(x)\), namely, \(J_Q(x) = x^{\mathsf{T}} P x\).&lt;/p&gt;
&lt;p&gt;I intend to explore the ideas surrounding quadratic functionals in more detail in a future post.  These functions can be used to tie together semidefinite programming duality, &lt;em&gt;stochastic&lt;/em&gt; linear systems (with random disturbances), as well as the classical methods of stochastic optimal control: the Kalman filter and the linear quadratic regulator.&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;: Lyapunov&amp;rsquo;s equation&lt;/kbd&gt; \(P - A^{\mathsf{T}} P A = Q\) &lt;kbd&gt;is a &lt;em&gt;linear&lt;/em&gt; equation in&lt;/kbd&gt; \(P\) &lt;kbd&gt;and can be solved efficiently.  In Python, the function&lt;/kbd&gt; &lt;code&gt;scipy.linalg.solve_discrete_lyapunov&lt;/code&gt; &lt;kbd&gt;can be used for this purpose.  Lyapunov equations arise in a number of different ways, so it is a useful pattern to have in mind.&lt;/kbd&gt;&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;conclusion&#34;
    &gt;Conclusion&lt;a href=&#34;#conclusion&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;My purpose in writing this has been to explore the Lyapunov approach to analyzing the stability of linear dynamical systems.  This is only one of many possible approaches to this analysis, but it also generalizes (in various directions) much more easily than does a direct analysis of the eigenvalues of the system matrix.  Indeed, the Lyapunov approach can also lead to &lt;em&gt;global&lt;/em&gt; convergence theorems for nonlinear systems, whereas the Hartman-Grobman theorem combined with an eigenvalue analysis can only lead to a &lt;em&gt;local&lt;/em&gt; convergence result.  Moreover, the semidefinite programming perspective on this problem can also generalize greatly, and opens the door to constructing &lt;em&gt;optimal&lt;/em&gt; algorithms.  The tradeoff with the Lyapunov approach is that constructing an appropriate Lyapunov function can be &lt;em&gt;extremely&lt;/em&gt; difficult, and there is no ready-made recipe for doing so &amp;ndash; it often comes down to your creativity!&lt;/p&gt;</description></item><item>
            <title>Solving Equations with Jacobi Iteration</title>
            <link>https://rjtk.github.io/posts/solving-equations-with-jacobi-iteration/</link>
            <pubDate>Sun, 11 Dec 2022 00:00:00 -0600</pubDate>
            <guid>https://rjtk.github.io/posts/solving-equations-with-jacobi-iteration/</guid><description>&lt;p&gt;Jacobi iteration is a natural idea for solving certain types of nonlinear equations, and reduces to a famous algorithm for linear systems.  This post discusses the algorithm, its convergence, benefits and drawbacks, along with a discussion of examples and pretty pictures 🖼️.&lt;/p&gt;
&lt;h2 class=&#34;group &#34; id=&#34;introduction&#34;
    &gt;Introduction&lt;a href=&#34;#introduction&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;The method of &lt;em&gt;Jacobi Iteration,&lt;/em&gt; named after the 19th century mathematician &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Carl_Gustav_Jacob_Jacobi&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carl Gustav Jacobi&lt;/a
&gt;
 (the same Jacobi for whom the &lt;em&gt;Jacobian&lt;/em&gt; in calculus is named after), is a numerical method for solving systems of equations.  It is particularly famous as a classical iterative algorithm for solving &lt;em&gt;linear&lt;/em&gt; systems.  One of the main reasons one might want to use Jacobi iteration in practice is that it admits of a naturally &lt;em&gt;parallel&lt;/em&gt; implementation, and can thus scale to very large and complex systems, and even to problems without any closed form representation of the system we want to solve (like a simulator, or a black-box machine learning algorithm).&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll introduce the basic mathematical intuition of Jacobi iteration, along with some examples of how and where it might arise.  I&amp;rsquo;ll also give a brief analysis of convergence for &lt;em&gt;linear&lt;/em&gt; systems of equations, where the concept of a &lt;em&gt;diagonally dominant&lt;/em&gt; matrix arises.  I also attempt to extend the intuition of the linear case to nonlinear systems by using techniques from the theory of ordinary differential equations.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;systems-of-equations&#34;
    &gt;Systems of Equations&lt;a href=&#34;#systems-of-equations&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;We are all familiar with the grade-school notion of a &lt;em&gt;mathematical equation&lt;/em&gt;.  Particularly famous are the quadratic polynomial equations, &lt;em&gt;e.g.,&lt;/em&gt; \(x^2 - x - 2 = 0\), which has exactly the solutions \(x = 2\) and \(x = -1\).  For another example, the trigonometric equation \(\mathsf{sin}(\frac{(2x + 1) \pi}{2}) - 1 = 0\) has any \(x\) being an integer multiple of \(2\), &lt;em&gt;i.e.,&lt;/em&gt; \(x \in 2\Z\) as a solution.&lt;/p&gt;
&lt;p&gt;What is meant by a &lt;em&gt;system&lt;/em&gt; of equations is simply a multitude of ordinary equations that need to be satisfied &lt;em&gt;simultaneously&lt;/em&gt;.  For instance, if we combined the above two examples into the system of &lt;em&gt;two equations&lt;/em&gt; and &lt;em&gt;one variable&lt;/em&gt; we would be left with the &lt;em&gt;system&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
x^2 + x - 2 &amp;amp;= 0\\
\mathsf{sin}\bigl(\frac{(2x + 1) \pi}{2}\bigr) - 1 &amp;amp;= 0,
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;which now has a single unique &lt;em&gt;simultaneous&lt;/em&gt; solution \(x = 2\).&lt;/p&gt;
&lt;p&gt;In general, we can write systems of equations using a single multivariate function \(F: \R^m \rightarrow \R^n\) which takes \(m\) variables \(x = (x_1, x_2, \ldots, x_m)\) as in \(F(x)\), and outputs \(n\) values \(F(x) = \bigl(F_1(x), F_2(x), \ldots, F_n(x)\bigr)\).  It is a &lt;em&gt;rule of thumb&lt;/em&gt; (certainly not an actual &lt;em&gt;rule&lt;/em&gt;) that if there are \(n\) equations, you can expect to be able to solve for \(n\) variables, &lt;em&gt;i.e.&lt;/em&gt;, the function \(F\) is &amp;ldquo;square&amp;rdquo; with \(m = n\).  Assuming this square case is by no-means essential, but it simplifies many of our examples, so we will run with this case.&lt;/p&gt;
&lt;p&gt;The ultimate goal of &lt;em&gt;solving&lt;/em&gt; systems of equations is to find some \(x \in \R^n\) such that \(F(x) = 0\).  We can motivate such problems with a few illustrative examples.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;minimizing-functions&#34;
    &gt;Minimizing Functions&lt;a href=&#34;#minimizing-functions&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Consider a function \(f: \R^n \rightarrow \R\) which we want to &lt;em&gt;minimize&lt;/em&gt;, &lt;em&gt;i.e.&lt;/em&gt;, to find an \(x^\star \in \R^n\) such that \(f(x^\star) \le f(x)\) for every other \(x\) in some neighbourhood of \(x^\star\).  One might want to think of \(f(x)\) perhaps as a design objective (find the best design according to the cost function \(f\)), or as a machine learning loss function, &lt;em&gt;etc.&lt;/em&gt;  It is a theorem (the &lt;em&gt;first order necessary conditions&lt;/em&gt;) that for differentiable functions \(f\), any minimizer \(x^\star\) must necessarily satisfy the derivative condition \(\mathsf{D} f(x^\star) = 0\), where \(\mathsf{D} f: \R^n \rightarrow \R^n\) is the derivative of \(f\).&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;economic-equilibria&#34;
    &gt;Economic Equilibria&lt;a href=&#34;#economic-equilibria&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;In economics, many believe that prices of goods in an economy are determined by the matching of supply and demand.  That is, if \(\mathcal{D}(p)\) is a &lt;em&gt;demand curve&lt;/em&gt; and \(\mathcal{S}(p)\) is a &lt;em&gt;supply curve&lt;/em&gt;, then we expect the price \(p\) to be found from solving the nonlinear equation \(\mathcal{D}(p) = \mathcal{S}(p)\).  I have much more to say about this example in Section &lt;a
    class=&#34;link&#34;
    href=&#34;#example-supply-and-demand-with-substitution&#34;&gt;Example: Supply and Demand with Substitution&lt;/a
&gt;
, where I apply Jacobi iteration to a simple coffee and tea economy where the two goods serve as partial substitutes for one and other.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;robotic-manipulators&#34;
    &gt;Robotic Manipulators&lt;a href=&#34;#robotic-manipulators&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Robotic arms with stiff linkages can be modeled by the angles \(\theta\) of their joints.  For example, the position of your hand (your &lt;em&gt;end affector&lt;/em&gt;) in space can be determined as a function of the length of your upper arm and your forearm, along with the angles formed at your elbow and shoulder (including the multiple dimensions of rotation your shoulder is capable of).  Specifically, we might wish to describe the position of your hand in space by a function \(F(\theta)\) of these joints.  The problem of determining the appropriate angular settings of your joints, in order to place your hand at a point \(p \in \R^3\) in space, is a problem of solving the system of equations \(F(\theta) - p = 0\), and one which your brain apparently solves with remarkable ease.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;jacobi-iteration&#34;
    &gt;Jacobi Iteration&lt;a href=&#34;#jacobi-iteration&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;The ideal of &lt;em&gt;Jacobi iteration&lt;/em&gt; is to split up the system \(F(x) = 0\) of \(n\) equations in \(n\) unknowns, into a sequence of simpler equations of one variable and one unknown.  For equation \(i\), we imagine that every variable &lt;em&gt;except&lt;/em&gt; \(x_i\), let&amp;rsquo;s call them \(x_{-i}\) is held fixed, and that we don&amp;rsquo;t care about the value of any function except \(F_i\).  We then solve the &lt;em&gt;univariate&lt;/em&gt; equation \(F_i(x_i; x_{-i}) = 0\) (This notation is common in game theory, I hope that it is understood) to find the single value of \(x_i\) that results in function \(F_i\) being satisfied.&lt;/p&gt;
&lt;p&gt;This process is carried out, &lt;em&gt;possibly in parallel&lt;/em&gt;, simultaneously for each equation to obtain a new set of points which we hope is closer to satisfying the full system of equations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: Proceeding sequentially, rather than simultaneously, by updating each \(x_i\) value immediately after finding a univariate solution, and before finding the next one, is an algorithm called &lt;em&gt;Gauss-Seidel Iteration&lt;/em&gt;.  The reader is encouraged to meditate upon the difference.&lt;/p&gt;
&lt;p&gt;A pseudo-code algorithm implementing Jacobi iteration is given as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;kbd&gt;Initialize&lt;/kbd&gt; \(x(0) \in \R^n\)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;for&lt;/kbd&gt; \(t = 1, 2, \ldots\)
&lt;ul&gt;
&lt;li&gt;&lt;kbd&gt;parallel for each&lt;/kbd&gt; \(i \in [n]\)
&lt;ul&gt;
&lt;li&gt;&lt;kbd&gt;find&lt;/kbd&gt; \(\tilde{x}_i\) &lt;kbd&gt;such that&lt;/kbd&gt; \(F_i(\tilde{x}_i, x_{-i}(t)) = 0\)  &lt;kbd&gt;// i.e., solve the $i^{th}$ equation&lt;/kbd&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;set&lt;/kbd&gt; \(x(t + 1) = \tilde{x}\)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order that this algorithm be an appropriate choice for your problem, it is &lt;em&gt;at least&lt;/em&gt; necessary that the step requiring that we &amp;ldquo;solve the \(i^{th}\) equation&amp;rdquo; can be reliably carried out.  However, even if solving each of the sub-problems is challenging, we may benefit from the straight-forward parallelism offered by finding each \(\tilde{x}_i\) simultaneously.&lt;/p&gt;
&lt;p&gt;The other major issue is &lt;em&gt;convergence&lt;/em&gt;, &lt;em&gt;i.e.,&lt;/em&gt; does the algorithm actually find a solution?  We can get fairly clear answers to this question in the linear case by applying dynamical systems theory.  As is so often the case, the theory for linear functions serves as a stepping stone to building intuition more generally.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;the-case-of-linear-equations&#34;
    &gt;The Case of Linear Equations&lt;a href=&#34;#the-case-of-linear-equations&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;Whether or not the algorithm is actually able to find some \(x\) such that \(F(x) = 0\) (assuming at the very least that such an \(x\) &lt;em&gt;exists&lt;/em&gt;!) is highly problem dependent and in practice may require various &amp;ldquo;clever tweaks&amp;rdquo;, or benefit from restarting the algorithm from a wide range of initial points \(x(0)\).  However, for the case of &lt;em&gt;linear&lt;/em&gt; systems of equations \(Ax = b\) (with \(A \in \R^{n \times n}\) a square matrix with real entries), the algorithm is both elegantly simple, and admits of an easily verifiable &lt;em&gt;sufficient condition&lt;/em&gt; for convergence known as &lt;em&gt;diagonal dominance&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To understand how this works, lets solve for \(x_i\) such that the \(i^{th}\) equation is satisfied.  That is,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
\sum_{j = 1}^n A_{ij} x_j &amp;amp;= b_i\\
\iff A_{ii} x_i &amp;amp;= b_i - \sum_{j: j \ne i} A_{ij} x_j\\
\iff x_i &amp;amp;= \frac{1}{A_{ii}}\bigl(b_i - \sum_{j: j \ne i}A_{ij} x_j \bigr),
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;where we should notice that we already require the diagonal elements of \(A\) to be non-zero (otherwise we couldn&amp;rsquo;t divide).&lt;/p&gt;
&lt;p&gt;Focusing for a moment on the term \(\sum_{j \ne i} A_{ij} x_j\), this is nothing but the \(i^{th}\) element in the matrix multiplication \(Ax\) minus the component \(A_{ii} x_i\), that is, \(\sum_{j \ne i} A_{ij} x_j = (Ax)_i - A_{ii} x_i\).  Using this, we can write the parallel updates to the entire vector \(x\), and the entire Jacobi iteration algorithm as:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
x(t + 1) = D^{-1} (b - Mx(t)),
\end{equation}&lt;/p&gt;
&lt;p&gt;where \(D = \mathsf{dg}(A)\) (the diagonal elements of \(A\)), \(M = A - D\) is all of the &lt;em&gt;off-diagonal&lt;/em&gt; elements of \(A\), and \(x(0) \in \R^n\) is an arbitrary starting point for the sequence of candidate solutions \(x(t)\).  When people refer to &amp;ldquo;Jacobi iteration&amp;rdquo;, it is usually this algorithm in particular that they are referring to.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;convergence&#34;
    &gt;Convergence&lt;a href=&#34;#convergence&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;The hope is that the iterations \(x(t)\) converge to a solution \(x^\star\), &lt;em&gt;i.e.,&lt;/em&gt; \(x(t) \rightarrow x^\star\), where \(Ax^\star = b\).&lt;/p&gt;
&lt;p&gt;To see why we might expect this to happen, imagine that \(x(t)\) converges to a &lt;em&gt;fixed point&lt;/em&gt; of the algorithm, &lt;em&gt;i.e.,&lt;/em&gt; some \(x(\infty)\) satisfying \(x(\infty) = D^{-1} (b - M x(\infty))\).  For such a point it holds that:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
x(\infty) &amp;amp;= D^{-1} (b - M x(\infty))\\
Dx(\infty) &amp;amp;= b - M x(\infty)\\
(D + M)x(\infty) &amp;amp;= b\\
Ax(\infty) &amp;amp;= b,
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;which is a solution to the equation \(Ax = b\)!&lt;/p&gt;
&lt;p&gt;To establish the convergence of the iterations, it is well known (this is likey to be a topic of a future post! 😜) that for linear systems \(x(t + 1) = a + K x(t)\), \(x(t)\) will converge whenever \(\rho(K) &amp;lt; 1\), where \(\rho(K) = \text{max}_i\ |\lambda_i(K)|\) is the largest eigenvalue magnitude, a quantity called the &lt;em&gt;spectral radius&lt;/em&gt;.  For the case of Jacobi iteration, we require that \(\rho(D^{-1} M) &amp;lt; 1\).&lt;/p&gt;
&lt;p&gt;Using the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Gershgorin_circle_theorem&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gershgorin circle theorem&lt;/a
&gt;
, and the fact that the diagonal values of \(M\) are all \(0\) (by construction), it holds that every eigenvalue of \(D^{-1} M\) lies within a distance \(|D_{ii}^{-1}|\sum_{j \ne i} |M_{ij}|\) of \(0\).  A condition on \(A\) which guarantees this is  &lt;em&gt;diagonal dominance&lt;/em&gt;: \(\sum_{j \ne i} |A_{ij}| &amp;lt; |A_{ii}|\) for each \(i\).  If this holds, then the eigenvalues \(\lambda_i\) of \(A\) are &amp;ldquo;close&amp;rdquo; to the diagonals of \(A_{ii}\), so such matrices are in some sense &amp;ldquo;almost diagonal&amp;rdquo;.  Since linear systems \(Dx = b\), for diagonal \(D\), are easy to solve, it is not surprising that there is a simple iterative algorithm for solving linear systems which are diagonally dominant.  Moreover, if the diagonals of \(A\) are also non-zero, \(A\) will necessarily have non-zero eigenvalues, and therefore it will be full-rank, invertible, and admit of unique solutions.  This is all summarized with the following theorem.&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A \in \R^{n \times n}$ be a square matrix with real entries.  Suppose that $A$ has a non-zero diagonal and is &lt;em&gt;diagonally dominant&lt;/em&gt;.  Then, for any vector $b \in \R^n$, there exists a unique solution $x^\star$ to the linear equation $Ax = b$ and Jacobi iteration converges, from any initial condition, to the solution $x^\star$.&lt;/kbd&gt;&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;linear-2-times-2-example&#34;
    &gt;Linear \(2 \times 2\) Example&lt;a href=&#34;#linear-2-times-2-example&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Some straightforward Python code implementing linear Jacobi iteration is provided in the listing below.  A realistic implementation should have a method of detecting divergence.  As well, checking the norm of the distance to the solution on every iteration is relatively expensive &amp;ndash; it essentially doubles the computational effort.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;solve_linear_system&lt;/span&gt;(A, b, x0&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;None&lt;/span&gt;, eps&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1e-6&lt;/span&gt;, maxiter&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;inf):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Solves the linear system Ax = b by Jacobi iteration.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    The algorithm&amp;#39;s starting point is x0.  The algorithm is only guaranteed
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    to converge if A is diagonally dominant.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Dinv_M, Dinv_b &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; (A &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;diagflat(D)) &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;diag(A)[:, &lt;span style=&#34;color:#ff79c6&#34;&gt;None&lt;/span&gt;], b &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;diag(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;array(x0) &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; x0 &lt;span style=&#34;color:#ff79c6&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;else&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;zeros_like(b)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    it &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;while&lt;/span&gt; it &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;lt;&lt;/span&gt; maxiter &lt;span style=&#34;color:#ff79c6&#34;&gt;and&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;norm(A &lt;span style=&#34;color:#ff79c6&#34;&gt;@&lt;/span&gt; x &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; b) &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;gt;&lt;/span&gt; eps:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; Dinv_b &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; Dinv_M &lt;span style=&#34;color:#ff79c6&#34;&gt;@&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        it &lt;span style=&#34;color:#ff79c6&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;For problems in \(\R^2\) (&lt;em&gt;i.e.,&lt;/em&gt; with just two variables) it is quite straight-forward to produce nice-looking figures.  The following figure plots the &lt;em&gt;flow&lt;/em&gt; of an associated ODE, as well as an &amp;ldquo;SOR&amp;rdquo; modification (both to be explained shortly 💁), along with the discrete iterates of the Jacobi iteration algorithm for the matrix&lt;/p&gt;
&lt;p&gt;\[
A = \begin{bmatrix} 4/5 &amp;amp; 3 / 5 \\ -6 / 5 &amp;amp; 7/5\end{bmatrix},
\]&lt;/p&gt;
&lt;p&gt;and \(b = 0\).  The case \(b = 0\) is without loss of generality for the purpose of plotting.  Even though finding a solution to the equation \(Ax = 0\) is trivial in this case, it is equivalent to re-orienting the center of our coordinate system upon the solution \(A^{-1} b\).  It is also worth noting that this matrix is row-wise diagonally dominant (corresponding to our definition) but &lt;em&gt;not&lt;/em&gt; column wise.&lt;/p&gt;


&lt;p&gt;If you squint closely at this figure, you might even believe that the lines joining the discrete iterates are &lt;em&gt;tangent&lt;/em&gt; to the flow lines in the background&amp;hellip;&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;relaxation-and-associated-flows&#34;
    &gt;Relaxation and Associated Flows&lt;a href=&#34;#relaxation-and-associated-flows&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Our figure above is constructed by plotting the &lt;em&gt;vector field&lt;/em&gt; from the &lt;em&gt;flow&lt;/em&gt; of a closely related ordinary differential equation.  Let&amp;rsquo;s use \(t\) to denote time, and as in the previous example, assume without loss that \(b = 0\).&lt;/p&gt;
&lt;p&gt;Merely as a device to construct a differential equation, imagine that each step of the algorithm takes \(\Delta\) &amp;ldquo;algorithm time&amp;rdquo;.  Precisely, let us write&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
x(t + \Delta) &amp;amp;= -D^{-1} Mx(t)\\
\iff x(t + \Delta) - x(t) &amp;amp;= -(I + D^{-1} M)x(t))\\
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;Now, we have an analogy with continually &lt;em&gt;taking steps&lt;/em&gt; \(-(I + D^{-1} M)x(t)\) to update the value of \(x(t)\).  If we reduce the size of these steps by \(\Delta\) and then take a limit as \(\Delta \rightarrow 0\)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
x(t + \Delta) - x(t) &amp;amp;= -\Delta(I + D^{-1} M)x(t))\\
\iff  \frac{1}{\Delta} [x(t + \Delta) - x(t)] &amp;amp;= -(I + D^{-1} M)x(t))\\
\overset{\Delta \rightarrow 0}{\implies} \dot{x}(t)  &amp;amp;= -(I + D^{-1} M)x(t)),
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;we obtain a &lt;em&gt;linear&lt;/em&gt; ordinary differential equation.&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Remark&lt;/strong&gt;:  The stablity of linear ODEs can be established by checking that the eigenvalues of the corresponding system matrix have negative real part.  In our case, due to the negative sign, we require that $\mathfrak{R}[\lambda_i(I + D^{-1} M)] &amp;gt; 0$ hold for every $i$, where $\lambda_i(A)$ is the $i^{th}$ eigenvalue of $A$.  Since $D^{-1}M$ has $0$ on the main diagonal (by construction) the Gershgorin circle theorem tells us that the eigenvalues lie within Gershgorin discs centered at $1$.  Clearly, the condition $\rho(D^{-1} M) &amp;lt; 1$ is &lt;em&gt;sufficient&lt;/em&gt; for the stability of this ODE.  But, thinking intuitively, should one expect that this ODE be &amp;ldquo;more likely&amp;rdquo; to converge than the discrete algorithm?  The reader is encouraged to visualize this situation, and to think about this stability condition.&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s play with this a bit.  What if we didn&amp;rsquo;t take a full limit towards \(\Delta \rightarrow 0\)?  Rearranging the above equations results in another discrete algorithm&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
x(t + \Delta) = (1 - \Delta) x(t) -\Delta D^{-1} Mx(t)),
\end{equation}&lt;/p&gt;
&lt;p&gt;where \(\Delta &amp;gt; 0\) is a &lt;em&gt;parameter&lt;/em&gt; of the algorithm, with \(\Delta = 1\) corresponding to ordinary Jacobi iteration.&lt;/p&gt;
&lt;p&gt;This algorithm is now picking the next point to lie somewhere along the line connecting \(x\) and the nominal next step \(s(x)\), that is: as \(x \leftarrow (1 - \Delta) x + \Delta s(x)\).  This is a general pattern called &lt;em&gt;Successive Over-relaxation&lt;/em&gt; (SOR), and can be applied to any iterative algorithm which takes as input a point \(x\), and outputs the next point as \(x \leftarrow s(x)\).  When \(\Delta &amp;lt; 1\), the next point will lie somewhere in the interval \([x, s(x)]\) between the current point and the nominal next step (the &amp;ldquo;relaxation&amp;rdquo; part of SOR); and \(\Delta &amp;gt; 1\) means that the step goes &lt;em&gt;beyond&lt;/em&gt; \(s(x)\) to move a further distance (the &amp;ldquo;over&amp;rdquo; part).  The value \(\Delta = 0\) corresponds to the &lt;em&gt;vanilla&lt;/em&gt; version of the algorithm.  I don&amp;rsquo;t think that the case \(\Delta &amp;lt; 0\) has any sensible use (at least not directly in this context) as you would be going &lt;em&gt;backwards&lt;/em&gt; in some sense, but maybe it&amp;rsquo;s an interesting possibility to think about.&lt;/p&gt;
&lt;p&gt;The reason that the SOR algorithm can be expected to converge for a broader collection of \(A\) matrices than ordinary Jacobi iteration, is that the space of matrices satisfying \(\mathfrak{R}[\lambda_i(I + D^{-1} M)] &amp;gt; 0\) is larger than those which satisfy \(\rho(D^{-1} M) &amp;lt; 1\) (can you see why?).  However, for a matrix \(A\) such that \(D^{-1}M\) has an eigenvalue with real part less than \(-1\), both algorithms will diverge.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;the-case-of-nonlinear-equations&#34;
    &gt;The case of Nonlinear Equations&lt;a href=&#34;#the-case-of-nonlinear-equations&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;For the case of nonlinear equations, we can use tools from ordinary differential equations to say something about the convergence of the associated Jacobi flow, and then extend that intuition through successive over-relaxation to try to understand something about how the discrete algorithm will behave.&lt;/p&gt;
&lt;p&gt;To this end, let us go back to the &lt;em&gt;nonlinear&lt;/em&gt; equation \(F(x) = 0\) that we want to solve.  Supposing that the Jacobi iteration algorithm can be implemented for \(F\) (&lt;em&gt;i.e.,&lt;/em&gt; each equation in the system can be solved in the &amp;ldquo;diagonal&amp;rdquo; variable), we can write the algorithm with the notation&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
x(t + 1) = \mathcal{M}(x(t)),
\end{equation}&lt;/p&gt;
&lt;p&gt;where for any \(x\), \(y = \mathcal{M}(x)\) is a vector such that \(F_i(x_{-i}, y_i) = 0\), that is, solves the \(i^{th}\) equation in the \(i^{th}\) variable.  Following the same trick as we had in the linear case, we can construct an ODE:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
\frac{x(t + \Delta) - x(t)}{\Delta} &amp;amp;= \mathcal{M}(x(t)) - x(t)\\
\overset{\Delta \rightarrow 0}{\implies} \dot{x}(t) &amp;amp;= \mathcal{M}(x(t)) - x(t).
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;Thus, we can hope to understand the convergence of nonlinear Jacobi iteration by analyzing the convergence of the nonlinear system of ODEs associated to the function \(\mathcal{M}\).  Firstly, a fixed point of this ODE is, similarly to the linear case, a solution to the nonlinear equation.  So, perhaps we can understand the convergence of \(x(t)\) for points where \(\mathcal{M}(x(t)) \approx x(t)\)?  The method for doing this type of &lt;em&gt;local&lt;/em&gt; analysis of ODE convergence is the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hartman-Grobman Theorem&lt;/a
&gt;
 (HGT from hereon).  Intuitively, our hope is that if we initialize the algorithm somewhere &amp;ldquo;close&amp;rdquo; to a solution, that it will actually converge to that solution.  That is, we hope that the ODE is &lt;em&gt;locally asymptotically stable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The HGT essentially tells us that if the &lt;em&gt;Jacobian&lt;/em&gt; \(A = \mathsf{D}\mathcal{M}(x^\star) - I\) of the system at an equilibrium point \(x^\star\) is stable, &lt;em&gt;i.e.,&lt;/em&gt; \(\dot{x} = Ax\) converges to \(x^\star\), then there is a local neighbourhood around \(x^\star\) such that the original &lt;em&gt;nonlinear&lt;/em&gt; ODE \(\dot{x} = \mathcal{M}(x) - x\) converges to \(x^\star\).&lt;/p&gt;
&lt;p&gt;This statement is still pretty abstract, since we don&amp;rsquo;t really have a handle on what \(\mathcal{M}\) is explicitly.  Another abstract tool that we could apply here is the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Implicit_function_theorem&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Implicit Function Theorem&lt;/a
&gt;
, which would allow us to calculate \(\mathsf{D}\mathcal{M}\) in terms of derivatives of the original function \(F\).  While this approach can certainly get us somewhere, I&amp;rsquo;d like to make a closer analogy with the linear case.  To this end, let&amp;rsquo;s make a simplifying assumption about the structure of \(F\):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
F_i(x) = f_i(x_i) + g_i(x_{-i})\ \forall i,
\end{equation}&lt;/p&gt;
&lt;p&gt;that is, the \(i^{th}\) equation consists of an individual function \(f_i: \mathbb{R} \rightarrow \mathbb{R}\) of \(x_i\), along with additional &lt;em&gt;coupling&lt;/em&gt; function \(g_{i}: \mathbb{R}^{n - 1} \rightarrow \mathbb{R}^{}\) involving the remainder of the variables.  We can now write down what \(\mathcal{M}_i\) is &lt;em&gt;explicitly&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\mathcal{M}_i(x) = f_i^{-1} (-g_i(x_{-i})).
\end{equation}&lt;/p&gt;
&lt;p&gt;In the linear case, \(f_i^{-1}(z) = z / A_{ii}\) and \(g_i(x_{-i}) = \sum_{j: j \ne i} A_{ij} x_j - b_i\).  Considering now the derivatives of \(\mathcal{M}_i,\)  we just need to combine the &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Inverse_function_theorem&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inverse Function Theorem&lt;/a
&gt;
 along with the chain rule to obtain&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\mathsf{D}\mathcal{M}_i(x) = -\frac{1}{f_i^\prime \circ \mathcal{M}_i(x)} \begin{bmatrix}\frac{\partial g_i(x_{-i})}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial g_i(x_{-i})}{\partial x_{i - 1}} &amp;amp; 0 &amp;amp; \frac{\partial g_i(x_{-i})}{\partial x_{i + 1}} &amp;amp; \cdots &amp;amp;\frac{\partial g_i(x_{-i})}{\partial x_{n}}\end{bmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;where \(f^\prime \circ \mathcal{M}_i(x) = f^\prime (f_i^{-1} (-g_{i}(x_{-i})))\).  Thus, in exact analogy with the linear case, we can write&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\mathsf{D}\mathcal{M}(x) = -D(x)^{-1} M(x),
\end{equation}&lt;/p&gt;
&lt;p&gt;where \(D(x)\) is a diagonal matrix consisting of the derivatives \(f_i^\prime\) evaluated at the point \(\mathcal{M}_i(x)\) and \(M(x)\) is the Jacobian \(\mathsf{D}F\) of \(F\) itself, but with the diagonal elements zeroed out.&lt;/p&gt;
&lt;p&gt;Returning to the HGT, we need to consider the stability of the linear ODE defined by the matrix \(A = \mathsf{D}\mathcal{M}(x^\star) - I\).  Since \(\mathcal{M}(x^\star) = x^\star\), we have that \(D_i(x^\star) = f_i^\prime(x^\star_i)\).  Using this fact, we are inspired to make a definition of a &lt;em&gt;locally diagonally dominant&lt;/em&gt; function.  We will say that \(F\) is a locally diagonally dominant (around an equilibrium point \(x^\star\)) if it has the form \(F_i(x) = f_i(x_i) + g_i(x_{-i})\) and \[|f_i^\prime(x_i^\star)| &amp;gt; \sum_{j: j \ne i} |\frac{\partial g_i (x^\star_{-i})}{\partial x_j}|\] for every \(i\).&lt;/p&gt;
&lt;p&gt;Using what we know from the linear case, combined with the HGT, and the fact that the SOR scheme is an &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Euler_method&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Euler Method&lt;/a
&gt;
 for the ODE (which converges towards the ODE itself as \(\Delta \rightarrow 0\)), we have a theorem about local convergence of nonlinear Jacobi iteration:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $F: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a smooth function.  Suppose there exists some $x^\star$ such that $F(x^\star) = 0$ and that $F(x^\star)$ is locally diagonally dominant in a neighbourhood of $x^\star$.  Then, there exists a neighbourhood $\mathcal{N}$ of $x^\star$ and a $\Delta &amp;gt; 0$ such that SOR nonlinear Jacobi iteration with step-size $\Delta$ converges to $x^\star$ from any initial point $x \in \mathcal{N}$.&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;Of course, the linear case corresponds to the functions \(f_i(x_i) = A_{ii} x_i\) and \(g_i(x_{-i}) = \sum_{j: j \ne i} A_{ij} x_j - b_i\), wherein we recover from above the convergence theorem of linear Jacobi iteration.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;example-supply-and-demand-with-substitution&#34;
    &gt;Example: Supply and Demand with Substitution&lt;a href=&#34;#example-supply-and-demand-with-substitution&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s cook up an example from economics where we&amp;rsquo;ll try to work out the equilibrium prices of goods in an economy.  A typical economic model (at least so far as I know&amp;hellip; I&amp;rsquo;m no economist!), is to find the prices at which the supply of a good is matched to the demand for that good.  The story is that when prices are high, manufacturers will scramble to produce that good in order to sell it for a large profit, which will drive down the price for the good through competition.  Similarly, when prices are low, demand will be very high, since you can get a lot of utility out of consuming the good in comparison to keeping the small amount of money.  This can be expected to drive up prices.  Through these competing effects pushing prices up and down, it is hoped that prices will &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/General_equilibrium_theory&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;equilibriate&lt;/em&gt;&lt;/a
&gt;
 at some fixed value through a process of &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Walrasian_auction&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;tâtonnement&lt;/em&gt;&lt;/a
&gt;
.&lt;/p&gt;
&lt;p&gt;The critical aspect that will make this model an interesting example for solving nonlinear equations is a &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Substitute_good&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;substitution effect&lt;/em&gt;&lt;/a
&gt;
.  To get more specific, suppose that when the prices for all \(n\) goods are given by \(p \in \R^n\), the supply for good \(i\) is described by a function \(\mathcal{S}_i(p) = S_i(p_i)\) and the demand for this good is \[\mathcal{D}_i(p) = D_i(p_i) + T_i(p_{-i}).\]  What this latter equation says is that there is some &lt;em&gt;nominal&lt;/em&gt; demand \(D_i(p_i)\) given by a scalar function of the price of that good, as well as the functions \(T_i(p_{-i})\) which serves to model the &lt;em&gt;substitution effect&lt;/em&gt;: the demand for good \(i\) also depends upon the prices of other goods in the economy.  A set of equilibrium prices \(p^\star \in \R^n\) (which may not be unique!) in this economy are prices such that \[\mathcal{S_i}(p^\star) - \mathcal{D}_i(p^\star) = 0\ \forall i \in [n],\] a nonlinear equation.&lt;/p&gt;
&lt;p&gt;To make this more concrete, suppose we have an economy with two goods: coffee and tea.  As the price of coffee increases, demand for coffee also falls.  But, the demand for tea &lt;em&gt;rises&lt;/em&gt;, since some coffee drinkers that are tightening their belt and drinking less coffee, start drinking more tea.  Let&amp;rsquo;s write this all down mathematically.  For the &lt;em&gt;demand&lt;/em&gt; I&amp;rsquo;ll use functions \(D_i(p_i) = \frac{k_i}{1 + p_i}\), modelling the fact that demand should be decreasing toward zero as price increases, and that there is some maximum demand \(k_i\) when the good is free (I don&amp;rsquo;t think it&amp;rsquo;s possible to consume an infinite amount of tea or coffee! ☕).  Picking another perfectly good function, I&amp;rsquo;ll model the substitution effect with a hyperbolic tangent: \(T_i(p_{-i}) = c_i \mathsf{tanh}(p_{-i})\).  This function has a plateau at \(c_i\), as \(p_{-i} \rightarrow \infty\), so the ratio \(c_c / k_\tau\) is measuring the proportion of tea drinkers that would switch to coffee as the price of tea increases.  On the supply side, I&amp;rsquo;ll use a nice S-shaped function \(S_i(p_i) = \frac{m_i p_i}{1 + p_i}\) where the maximum producible amount of the good reaches another plateau at \(m_i\).  This results in the nonlinear system of equations (using \(c\) for coffee and \(\tau\) for tea):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
\text{coffee:}\quad&amp;amp; \frac{m_c p_c}{1 + p_c} - \frac{k_c}{1 + p_c} - c_c \mathsf{tanh}(p_\tau) = 0,\\
\text{tea:}\quad&amp;amp; \frac{m_\tau p_\tau}{1 + p_\tau} - \frac{k_\tau}{1 + p_\tau} - c_\tau \mathsf{tanh}(p_c) = 0.
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;Solving these equations individually for \(p_c, p_\tau\) we obtain a nonlinear Jacobi algorithm:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\notag
\begin{aligned}
p_c(t + 1) &amp;amp;= \frac{k_c + c_c \mathsf{tanh}(p_\tau(t))}{m_c - c_c \mathsf{tanh}(p_\tau(t))}\\
p_\tau(t + 1) &amp;amp;= \frac{k_\tau + c_\tau \mathsf{tanh}(p_c(t))}{m_\tau - c_\tau \mathsf{tanh}(p_c(t))}.
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;In order for this to constitute a locally diagonally dominant system, we need to check the derivatives.  It is a fairly straightforward calculation which, after cancelling some terms, results in the requirements \[\frac{m_i + k_i}{(1 + p_i^\star)^2} &amp;gt; c_i(1 - \mathsf{tanh}^2 (p_{-i}^\star)).\]  The idea of &amp;ldquo;diagonal dominance&amp;rdquo; is clear in this equation: \(m_i, k_i\) are coefficients controlling the importance of the &amp;ldquo;diagonal&amp;rdquo; part of the function, with \(c_i\) controlling the coupling.  If the coupling is weak: \(m_i + k_i \gg c_i\), then it can be expected that the system is diagonally dominant, and an equilibrium should be close to the &amp;ldquo;nominal&amp;rdquo; equilibrium \(p_i^\star \approx k_i / m_i\).  Incidentally, this should give us a decent starting point for initializing Jacobi iteration.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a generic implementation of nonlinear Jacobi iteration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;18
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;19
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;20
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;21
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;22
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;23
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;24
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;25
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;26
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;27
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;28
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;29
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;30
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;31
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;32
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;33
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;34
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;35
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;36
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;37
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;38
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;39
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; typing &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; List, Callable
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;jacobi_iteration&lt;/span&gt;(x, f_inv: List[Callable], g: List[Callable]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x_next &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;zeros_like(x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    all_i &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;len&lt;/span&gt;(x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; i, (f_inv_i, g_i) &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;enumerate&lt;/span&gt;(&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;zip&lt;/span&gt;(f_inv, g)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x_next[i] &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; f_inv_i(&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;g_i(x[all_i &lt;span style=&#34;color:#ff79c6&#34;&gt;!=&lt;/span&gt; i]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; x_next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;jacobi_flow&lt;/span&gt;(x, f_inv: List[Callable], g: List[Callable]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; jacobi_iteration(x, f_inv, g) &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;solve_nonlinear_system&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    F: Callable,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f_inv: List[Callable],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    g: List[Callable],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x0&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    eps&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1e-6&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    maxiter&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;inf,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    D&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Solves the nonlinear system F_i(x) = f_i(x_i) + g_i(x_{-i}) = 0.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    The algorithm&amp;#39;s starting point is x0.  The algorithm is only guaranteed
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    to converge if F is locally diagonally dominant around an equilibrium, and
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    x0 is chosen nearby that equilibrium point.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;array(x0) &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; x0 &lt;span style=&#34;color:#ff79c6&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;else&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;zeros(&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;len&lt;/span&gt;(g))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _iter &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; partial(jacobi_iteration, f_inv&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;f_inv, g&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;g)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    all_i &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;len&lt;/span&gt;(x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    it &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;while&lt;/span&gt; it &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;lt;&lt;/span&gt; maxiter &lt;span style=&#34;color:#ff79c6&#34;&gt;and&lt;/span&gt; np&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;norm(F(x)) &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;gt;&lt;/span&gt; eps:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; D &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; x &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; (&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; D) &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; _iter(x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        it &lt;span style=&#34;color:#ff79c6&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The code follows a functional programming style that mimics the mathematical description as closely as possible.  A particular implementation of our coffee-tea economy, including useful functions for plotting the progress of the algorithm, might look something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;18
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;19
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;20
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;21
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;22
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;23
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;24
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;25
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;26
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;27
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;28
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;29
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;30
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;31
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;32
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;33
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;34
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;35
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;36
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; math &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; tanh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; functools &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; partial
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;coffee_tea_solver&lt;/span&gt;(k, m, c, D&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;0.0&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&amp;#34;&amp;#34;A particular example with coffee and tea...&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;F&lt;/span&gt;(p):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x0 &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; (m[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; p[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; k[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;]) &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; p[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;]) &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; c[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; tanh(p[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x1 &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; (m[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; p[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; k[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; p[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; c[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; tanh(p[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; (x0, x1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;f_inv&lt;/span&gt;(z, _k, _m):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; (_k &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; z) &lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt; (_m &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt; z)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;g&lt;/span&gt;(p, _c):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;_c &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; tanh(p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f_invs &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        partial(f_inv, _k&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;k[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;], _m&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;m[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        partial(f_inv, _k&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;k[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;], _m&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;m[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    gs &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; [partial(g, _c&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;c[&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;]), partial(g, _c&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;c[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;])]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _jacobi_iterator &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; partial(jacobi_iteration, f_inv&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;f_invs, g&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;gs)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _jacobi_flow &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; partial(jacobi_flow, f_inv&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;f_invs, g&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;gs)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _solver &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; partial(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        solve_nonlinear_system,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        F&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;F,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        f_inv&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;f_invs,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        g&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;gs,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        eps&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1e-6&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        maxiter&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;1000&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        D&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;D
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; _jacobi_iterator, _jacobi_flow, _solver, F
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Here&amp;rsquo;s a plot of the flow of this system for the parmeter settings&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
&amp;amp;k_c = 3/2, k_\tau = 2\\
&amp;amp;m_c = 5/3, m_\tau = 1/2\\
&amp;amp;c_c = 4/3, c_\tau = 1/3,
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;p&gt;which, at least intuitively, results in a system corresponding to our intuition of diagonal dominance.  Increasing the values of \(c_c, c_\tau\) results in a breakdown of the algorithm and a failure to converge, either as a result of domain errors, or running to infinity.  The SOR technique described earlier can potentially be used to increase the domain of convergence, but that is a matter for experimentation.&lt;/p&gt;


&lt;p&gt;For this particular coffee-tea system, there is surely a lot more analysis that one could do in order to determine clearer convergence criteria, the ranges of parameters that will result in convergence, &lt;em&gt;etc.&lt;/em&gt;  Such analysis is worthwhile in actual practice, but we&amp;rsquo;re here for fun 🙃.&lt;/p&gt;


&lt;h2 class=&#34;group &#34; id=&#34;conclusion&#34;
    &gt;Conclusion&lt;a href=&#34;#conclusion&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;p&gt;Our main point is in understanding the Jacobi iteration algorithm for solving systems of equations.  The key assumption for this algorithm is that the system be &lt;em&gt;diagonally dominant&lt;/em&gt;.  Intuitively, this means that the equation associated to the \(i^{th}\) variable be only weakly coupled with the remaining variables \(x_{-i}\).  We&amp;rsquo;ve seen how to formalize this in the case of linear equations, and how similar &amp;ldquo;rough guide&amp;rdquo; criteria can be obtained from the Hartman-Grobman Theorem in the nonlinear case.&lt;/p&gt;
&lt;p&gt;In actual practice, Jacobi iteration is not likely to be the best nonlinear equation solver to use, though it depends upon the problem domain.  The fact that it breaks down a system of equations into the &lt;em&gt;parallel&lt;/em&gt; solution of &lt;em&gt;univariate&lt;/em&gt; equations (for which completely general black-box algorithms like &lt;a
    class=&#34;link&#34;
    href=&#34;https://en.wikipedia.org/wiki/Bisection_method&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the Bisection Method&lt;/a
&gt;
 are available) gives it a rather large potential domain of applicability.  If you find yourself confronted with the need to solve a large system of nonlinear equations, available only through slow and black-box evaluation, and where the equations can be expected to be weakly coupled, Jacobi iteration might be for you 🤞.&lt;/p&gt;</description></item><item>
            <title>Introduction</title>
            <link>https://rjtk.github.io/posts/introduction/</link>
            <pubDate>Fri, 09 Dec 2022 00:00:00 -0600</pubDate>
            <guid>https://rjtk.github.io/posts/introduction/</guid><description>&lt;p&gt;My name is Ryan and I like math.&lt;/p&gt;
&lt;p&gt;The title of this blog, &lt;strong&gt;Quant out of Water&lt;/strong&gt;, is more-or-less the first thing that came to mind.  However, as I (as of 2022) work as a quant at an hedge fund, and I wanted to write a blog that was &lt;strong&gt;&lt;strong&gt;not explicitly about finance&lt;/strong&gt;&lt;/strong&gt;, this title reflects that motivation.  I produced the fish with money using a stable diffusion model.  I hope you enjoy some of my writings.&lt;/p&gt;
&lt;p&gt;You can learn more on the &lt;a
    class=&#34;link&#34;
    href=&#34;https://rjtk.github.io/about/&#34;&gt;&amp;ldquo;about me&amp;rdquo;&lt;/a
&gt;
 page.&lt;/p&gt;
</description></item></channel>
</rss>