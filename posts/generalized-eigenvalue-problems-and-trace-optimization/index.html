<!DOCTYPE html>
<html lang="en-us" class="lang-en-us">
    <head><meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="Most people are familiar with the concept of matrix eigenvalues. Less well known is that this concept can be fruitfully expanded to the generalized eigenvalues of pairs of matrices. Closely related are matrix trace optimization problems, which extremizes the trace of certain matrix products. Trace optimization constites a large class of practically solvable non-convex optimization problems commonly useful for dimensionality reduction and which includes the unsupervised weighted principle component analysis and the supervised method of Fisher&amp;rsquo;s Linear Discriminant. The purpose of this post is to explore some of these problems, their intuition, and their applications.
" />
<meta itemprop="description" content="Most people are familiar with the concept of matrix eigenvalues. Less well known is that this concept can be fruitfully expanded to the generalized eigenvalues of pairs of matrices. Closely related are matrix trace optimization problems, which extremizes the trace of certain matrix products. Trace optimization constites a large class of practically solvable non-convex optimization problems commonly useful for dimensionality reduction and which includes the unsupervised weighted principle component analysis and the supervised method of Fisher&amp;rsquo;s Linear Discriminant. The purpose of this post is to explore some of these problems, their intuition, and their applications.
" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<meta itemprop="name" content="Generalized Eigenvalue Problems and Trace Optimization - Quant Out of Water" />
<link href="https://rjtk.github.io/index.xml" title="Generalized Eigenvalue Problems and Trace Optimization - Quant Out of Water" type="application/rss+xml" rel="alternate" />
<title>Generalized Eigenvalue Problems and Trace Optimization - Quant Out of Water</title><link rel="stylesheet" href="/main.1fcc545d2ef04212761e07c486eaac8b9c8e1c46af8fc344804b7383bb9116bc.css" integrity="sha256-H8xUXS7wQhJ2HgfEhuqsi5yOHEavj8NEgEtzg7uRFrw=" /><meta property="og:site_name" content="Generalized Eigenvalue Problems and Trace Optimization - Quant Out of Water" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Generalized Eigenvalue Problems and Trace Optimization" />
<meta property="og:description" content="Most people are familiar with the concept of matrix eigenvalues. Less well known is that this concept can be fruitfully expanded to the generalized eigenvalues of pairs of matrices. Closely related are matrix trace optimization problems, which extremizes the trace of certain matrix products. Trace optimization constites a large class of practically solvable non-convex optimization problems commonly useful for dimensionality reduction and which includes the unsupervised weighted principle component analysis and the supervised method of Fisher&amp;rsquo;s Linear Discriminant. The purpose of this post is to explore some of these problems, their intuition, and their applications.
" />
<meta property="og:url" content="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/" /><meta property="article:publisher" content="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/" /><meta property="article:published_time" content="2023-07-18T00:00:00-07:00" /><meta property="article:modified_time" content="2023-09-24T23:49:56-07:00" /><meta name="theme-color" content="#dd6065" />
<meta name="mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-title" content="Generalized Eigenvalue Problems and Trace Optimization - Quant Out of Water" />
<meta name="apple-mobile-web-app-status-bar-style" content="white" />

<meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="Generalized Eigenvalue Problems and Trace Optimization - Quant Out of Water" />
<meta name="twitter:description" content="Most people are familiar with the concept of matrix eigenvalues. Less well known is that this concept can be fruitfully expanded to the generalized eigenvalues of pairs of matrices. Closely related are matrix trace optimization problems, which extremizes the trace of certain matrix products. Trace optimization constites a large class of practically solvable non-convex optimization problems commonly useful for dimensionality reduction and which includes the unsupervised weighted principle component analysis and the supervised method of Fisher&amp;rsquo;s Linear Discriminant. The purpose of this post is to explore some of these problems, their intuition, and their applications.
" />
<meta name="twitter:url" content="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/" />





  <meta property="og:image" content="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/stretched_space.svg" />



<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" /><link rel="manifest" href="/manifest.json" /><link href="/icon.png" rel="shortcut icon" />
<link href="/icon.png" rel="Bookmark" />
<link rel="apple-touch-icon" href="/icon.png" /><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Generalized Eigenvalue Problems and Trace Optimization",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/rjtk.github.io\/posts\/generalized-eigenvalue-problems-and-trace-optimization\/"
  },"genre": "posts","wordcount": 6167 ,
  "url": "https:\/\/rjtk.github.io\/posts\/generalized-eigenvalue-problems-and-trace-optimization\/","datePublished": "2023-07-18T00:00:00-07:00","dateModified": "2023-09-24T23:49:56-07:00","author": {
    "@type": "Person",
    "name": "qoow"
  },"description": ""
}
</script>


    </head>

    <body class="dark:bg-darkBg dark:text-darkText">
        <div class="relative mx-auto shadow-lg md:max-w-4xl xl:max-w-4xl 2xl:max-w-4xl">
            <div class="main flex flex-col justify-between bg-white dark:bg-darkFg"><header
    class="relative"
><div class=" border-b py-6 text-center dark:border-darkBorder"><div class="text-3xl">Quant Out of Water</div></div><nav id="nav" class="navbar top-0 z-10 flex items-center border-b px-5 py-6 dark:border-darkBorder md:px-10">
            <div class="route-items flex w-full items-center justify-around"><a
                        title="Home"
                        data-active-link="/"
                        href="/"
                        class=" relative flex cursor-pointer flex-col items-center text-gray-700 transition duration-150 ease-[ease]"
                    >
                        <i
                            class="eva eva-home flex h-8 w-8 items-center justify-center text-3xl leading-none text-gray-500 transition duration-150 ease-[ease] dark:text-darkTextPlaceholder"
                        ></i>
                        <span class="mt-1 block text-xs text-gray-400 dark:text-darkText sm:text-sm md:hidden">Home</span>
                    </a><a
                        title="About"
                        data-active-link="/about/"
                        href="/about/"
                        class=" relative flex cursor-pointer flex-col items-center text-gray-700 transition duration-150 ease-[ease]"
                    >
                        <i
                            class="eva eva-people flex h-8 w-8 items-center justify-center text-3xl leading-none text-gray-500 transition duration-150 ease-[ease] dark:text-darkTextPlaceholder"
                        ></i>
                        <span class="mt-1 block text-xs text-gray-400 dark:text-darkText sm:text-sm md:hidden">About</span>
                    </a>
            </div>
        </nav><nav class="sub-navbar z-10 flex items-center border-b px-5 py-2 dark:border-darkBorder md:px-10"><a
                    title="Tags"
                    data-active-link="/tags/"
                    href="/tags/"
                    class=" relative mr-4 flex cursor-pointer items-center text-gray-400 transition duration-150 ease-[ease] hover:text-theme dark:text-darkText"
                >
                    <i class="eva eva-pricetags mr-1 leading-none"></i>
                    <span>Tags</span>
                </a></nav><div class="dark-mode-switch absolute right-0 top-0 z-10 cursor-pointer pr-2 pt-2 text-xl leading-none">
    <i class="eva eva-moon opacity-20 dark:opacity-70"></i>
</div>
</header>
<main class=" relative flex flex-grow" id="swup"><style>:root {
        --font:Roboto, "Helvetica Neue", Helvetica, Arial, sans-serif;
    }</style>
<div class="type-posts layout- w-full"><div class="relative h-full">
    <div class="relative h-full">
        <div class="page-view-article flex h-full flex-col bg-white dark:bg-darkFg"><div class="relative"><div
    style="padding-bottom:75%;"
    class="article-cover h-0Page(/posts/generalizedEigenvalues/index.md) relative w-full"
>
    <picture class="noscript-hidden"><img
            data-src="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/stretched_space.svg"
            src="/images/outload.svg"
            
            
            data-
            alt="Generalized Eigenvalue Problems and Trace Optimization"
            class="absolute left-0 top-0 h-full w-full object-cover object-center"
            data-lazyload data-lazyload-blur

        />
    </picture>
    <noscript>
        <picture><img
                src="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/stretched_space.svg"
                
                alt="Generalized Eigenvalue Problems and Trace Optimization"
                
                class="absolute left-0 top-0 h-full w-full object-cover object-center"
            />
        </picture>
    </noscript>
</div>
<h1 class="article-title absolute bottom-0 left-0 w-full px-6 pb-8 pt-32 text-3xl text-white dark:text-darkText md:px-10 md:text-4xl">
    Generalized Eigenvalue Problems and Trace Optimization
</h1>
</div>
<div class="article-info border-b p-6 pb-3 text-sm dark:border-darkBorder md:px-10">
    <div>
        <div class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
            <i class="eva eva-clock-outline mr-1"></i>
            <span>
                <time
                    title="Published in 18 Jul 2023 00:00:00"datetime="2023-07-18T00:00:00-07:00">18 Jul 2023</time
                >
            </span>
        </div><span class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
                    <i class="eva eva-edit-2-outline mr-1"></i>
                    <span>
                        <time
                            title="Last modified on: 24 Sep 2023 23:49:56"datetime="2023-09-24T23:49:56-07:00">24 Sep 2023</time
                        >
                    </span>
                </span><div class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
                <i class="eva eva-flag-outline mr-1"></i>
                <span>29 mins read</span>
            </div><div class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
                <i class="eva eva-bar-chart-outline mr-1"></i>
                <span>6167 words</span>
            </div></div></div>
<aside
            class="toc border-b border-gray-300 px-5 py-5 dark:border-darkBorder md:px-10 2xl:fixed 2xl:top-10 2xl:m-0 2xl:-ml-72 2xl:w-72 2xl:border-none 2xl:p-0 2xl:py-4 2xl:pr-4 "
        >
            <header>
                <h1 class="mb-3 text-2xl font-bold 2xl:mb-4">Table of Contents</h1>
            </header>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#spectral-theory">Spectral Theory</a>
      <ul>
        <li><a href="#the-spectral-theorem">The Spectral Theorem</a></li>
        <li><a href="#the-courant-fischer-variational-characterization">The Courant-Fischer Variational Characterization</a></li>
        <li><a href="#generalized-eigenvalues">Generalized Eigenvalues</a></li>
      </ul>
    </li>
    <li><a href="#applications-and-computations">Applications and Computations</a>
      <ul>
        <li><a href="#principle-component-analysis">Principle Component Analysis</a></li>
        <li><a href="#weighted-principle-component-analysis">Weighted Principle Component Analysis</a></li>
        <li><a href="#rayleigh-quotients-and-fisher-s-linear-discriminant">Rayleigh Quotients and Fisher&rsquo;s Linear Discriminant</a></li>
        <li><a href="#optimizing-signal-to-noise-ratio">Optimizing Signal to Noise Ratio</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        </aside><section class="article-content typo relative flex-grow px-6 py-5 md:px-10"><i title="Faster Reads" id="bionicReading" class="absolute right-0 top-0 cursor-pointer p-3"
            ><svg class="w-4 h-4 fill-current text-gray-400" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10811" ><path d="M480 192c-19.2 0-32-12.8-32-32v-128c0-19.2 12.8-32 32-32s32 12.8 32 32v128c0 19.2-12.8 32-32 32zM160 1024c-6.4 0-19.2 0-25.6-6.4l-128-128c0-6.4-6.4-19.2-6.4-25.6s6.4-19.2 6.4-25.6l512-512c12.8-6.4 38.4-6.4 51.2 0l128 128c0 6.4 6.4 19.2 6.4 25.6s-6.4 19.2-6.4 25.6l-512 512c-6.4 6.4-19.2 6.4-25.6 6.4z m-83.2-160l83.2 83.2 467.2-467.2-83.2-83.2-467.2 467.2zM992 576h-128c-19.2 0-32-12.8-32-32s12.8-32 32-32h128c19.2 0 32 12.8 32 32s-12.8 32-32 32zM768 288c-6.4 0-19.2 0-25.6-6.4-12.8-12.8-12.8-32 0-44.8l96-96c12.8-12.8 32-12.8 44.8 0s12.8 32 0 44.8l-96 96c0 6.4-12.8 6.4-19.2 6.4zM256 288c-6.4 0-19.2 0-25.6-6.4L134.4 185.6c-6.4-12.8-6.4-38.4 0-51.2s32-12.8 44.8 0l96 96c12.8 12.8 12.8 32 0 44.8 0 12.8-12.8 12.8-19.2 12.8zM864 896c-6.4 0-19.2 0-25.6-6.4l-96-96c-12.8-12.8-12.8-32 0-44.8s32-12.8 44.8 0l96 96c12.8 12.8 12.8 32 0 44.8 0 6.4-12.8 6.4-19.2 6.4z" p-id="10812"></path><path d="M544 640c-6.4 0-19.2 0-25.6-6.4l-128-128c-6.4-12.8-6.4-38.4 0-51.2s32-12.8 44.8 0l128 128c12.8 12.8 12.8 38.4 6.4 51.2-6.4 6.4-19.2 6.4-25.6 6.4z" p-id="10813"></path></svg></i
        ><p>Most people are familiar with the concept of matrix eigenvalues.  Less well known is that this concept can be fruitfully expanded to the <em>generalized eigenvalues</em> of <em>pairs</em> of matrices.  Closely related are matrix trace optimization problems, which extremizes the trace of certain matrix products.  Trace optimization constites a large class of practically solvable <em>non-convex</em> optimization problems commonly useful for dimensionality reduction and which includes the unsupervised <em>weighted</em> principle component analysis and the supervised method of Fisher&rsquo;s Linear Discriminant.  The purpose of this post is to explore some of these problems, their intuition, and their applications.</p>
<h2 class="group " id="introduction"
    >Introduction<a href="#introduction"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>The eigenvalues of a matrix are profoundly important quantities and are usually encountered early on in one&rsquo;s mathematical career.  The concept of the eigenvalues of a single matrix \(A\) can be expanded to the <em>generalized</em> eigenvalues of a <em>pair</em> of matrices \((A, B)\).  My purpose here is to summarize and distill some of what I&rsquo;ve learned about the theory and application of generalized eigenvalues.</p>


<h2 class="group " id="spectral-theory"
    >Spectral Theory<a href="#spectral-theory"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>Before discussing eigenvalue problems, we need to set up some basic notation and review critical facts about matrices.</p>
<p>Consider an \(n \times n\) matrix \(A \in \R^{n \times n}\).  The <em>eigenvalues</em> \(\lambda_i \in \mathbb{C}\) and <em>eigenvectors</em> \(v_i \in \mathbb{C}^n\) of the matrix \(A\) are those (complex!) scalars and vectors such that \[Av_i = \lambda_i v_i.\]  That is, the eigenvectors are the &ldquo;directions&rdquo; in which the operation of the matrix \(A\) are &ldquo;simple&rdquo;, <em>i.e.</em>, nothing but multiplication by a scalar.  It is one of the hallmarks of linear algebra that the eigenvectors of an \(n \times n\) matrix will often (not always) happen to provide us with a basis for \(\mathbb{R}^n\), i.e., we can use the eigenvectors as coordinate axes.  This is incredibly useful when working with the matrix \(A\), since the matrix does nothing but scale the axes in this basis, a rather simple operation.  When this is possible, we say that the matrix is <em>diagonalized</em> by the change of basis.  Conditions of matrix diagonalizability are captured by the <em>Spectral Theorem</em>, so called because the collection of eigenvalues of a matrix are often referred to as &ldquo;the spectrum&rdquo; (of the matrix).</p>


<h3 class="group " id="the-spectral-theorem"
    >The Spectral Theorem<a href="#the-spectral-theorem"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>To <em>diagonalize</em> the matrix \(A\) means that we can re-write it as \(A = V \Lambda V^{-1}\) where \(\Lambda = \mathsf{Dg}(\mathbf{\lambda})\) is a diagonal matrix of the eigenvalues of \(A\), and \(V = \begin{bmatrix}v_1 &amp; v_2 &amp; \cdots &amp; v_n \end{bmatrix} \in \mathbb{C}^{n \times n}\) are the eigenvectors. The matrix \(V^{-1}\) represents a transformation <em>into</em> an <em>eigenbasis</em> and \(V\) is a transformation <em>out</em> of it.  Unfortunately, diagonalizing a matrix is not always possible, and matrix diagonalizability is quite a slippery issue.  Fortunately though, there is a simple criteria, occuring frequently for matrices of interest in practice, that guarantees that \(A\) is diagonalizable.  The condition is that \(A\) be real-valued and <em>symmetric</em>: \(A = A^{\mathsf{T}}\) (more generally, \(A\) can be <em>normal</em>: \(AA^{\mathsf{T}} = A^{\mathsf{T}} A\)).</p>
<p><kbd><strong>Spectral Theorem:</strong> Let $A \in \R^{n \times n}$ be a symmetric matrix.  Then, there exist $n$ <em>real</em> eigenvalues $\lambda_i$ of $A$ and $n$ eigenvectors $v_i \in \R^n$ such that $A_i v_i = \lambda_i v_i$.  Moreover, the vectors $v_i$ constitute an orthonormal basis of $\R^n$</kbd>.</p>
<p>If we express \(x\) in the eigenbasis as \(x = \sum_{i = 1}^n \langle v_i, x \rangle v_i\) then the matrix-vector product is \(Ax = \sum_{i = 1}^n \lambda_i \langle v_i, x \rangle v_i\), and in matrix notation this is simply \(Ax = V\Lambda V^{\mathsf{T}}x\) where \(V = \begin{bmatrix}v_1\ \cdots v_n \end{bmatrix}\).   In fact, this shows that \(A = V\Lambda V^{\mathsf{T}}\), and since \(V\) is an orthonormal basis we have \(V^{\mathsf{T}}V = I_n = VV^{\mathsf{T}}\) meaning that \(V^{-1} = V^{\mathsf{T}}\).  So the eigenvectors of \(A\) diagonalize \(A\) itself.</p>
<p>The spectral theorem is one of the most important results in mathematics and generalizes in various ways to certain sorts of infinite dimensional operators.</p>
<p><kbd><strong>Remark</strong>: Conventionally, the eigenvalues are <em>sorted</em> from low to high as in $\lambda_1(A) \le \cdots \le \lambda_n(A)$ (the opposite sorting convention is also common).</kbd></p>


<h3 class="group " id="the-courant-fischer-variational-characterization"
    >The Courant-Fischer Variational Characterization<a href="#the-courant-fischer-variational-characterization"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>The <a
    class="link"
    href="https://en.wikipedia.org/wiki/Min-max_theorem"target="_blank" rel="noopener">Courant-Fischer</a
>
 theorem provides a <em>variational characterization</em> of the eigenvalues of a symmetric matrix.  Basically, a variational characterization is a way to calculate some quantity in terms of an optimization problem, the term &ldquo;variational&rdquo; presumably arising from <a
    class="link"
    href="https://en.wikipedia.org/wiki/Calculus_of_variations"target="_blank" rel="noopener">the Calculus of Variations</a
>
, and the idea of an infinitestimal &ldquo;variation&rdquo; of a function.  Specifically, the smallest eigenvalue \(\lambda_1(A)\) of an \(n \times n\) symmetric matrix can be characterized by the optimization problem</p>
<p>\begin{equation}
\begin{aligned}
\underset{x \in \R^n}{\text{maximize}}&amp;\ x^{\mathsf{T}} A x\\
\text{subject to}&amp;\ ||x||_2 = 1,
\end{aligned}\tag{$\lambda_n(A)$}
\end{equation}</p>
<p>where the maximum is obtained by an eigenvector corresponding to the smallest eigenvalue \(\lambda_n\), which is also the value of the problem.  Variational characterizations are incredibly useful in practice &ndash; they are often providing one or both of: (a) a concrete procedure for calculating some quantity by means of an optimization algorithm or (b) a way to solve seemingly difficult optimization problems by directly calculating the quantity known to be the solution.  The optimization problem I called \((\lambda_n(A))\) above falls into the second category.  Particularly since it is not a convex problem, directly applying an iterative optimization algorithm to this problem is not likely to pan out well, but there do exist reliable algorithms for calculating matrix eigenvalues.</p>
<p>To see <em>why</em> the problem \((\lambda_n(A))\) provides us with the smallest eigenvalue, we recognize that it is exactly equivalent to the following:</p>
<p>\begin{equation}
\begin{aligned}
\underset{u \in \R^n}{\text{maximize}}&amp;\ u^{\mathsf{T}} \Lambda u\\
\text{subject to}&amp;\ ||u||_2 = 1,
\end{aligned}\notag
\end{equation}</p>
<p>where \(\Lambda\) is the diagonal matrix of eigenvalues from \(A = V\Lambda V^{\mathsf{T}}\).  This equivalence follows since \(V\) is an orthonormal basis: for any \(x\) with \(||x||_2 = 1\) there exists a \(v\) such that \(x = Vu\) and \(||u||_2 = 1\) since \(||Vu||_2 = \sqrt{\lang Vu, Vu\rang} = \sqrt{\lang u, V^{\mathsf{T}} Vu\rang} = ||u||_2\).  The optimizer of this latter problem is clearly \(u^\star = (0, 0, \ldots, 1)\) (selecting out the largest eigenvalue) with the value \(\lambda_n\) and the corresponding solution of the original problem is \(x = Vu^\star = v_n\), an eigenvector associated with \(\lambda_n\).</p>
<p><kbd><strong>Remark</strong>: Incidentally, this also shows that $\lambda_n(A)$ is a convex function of</kbd> \(A\) <kbd>since</kbd> \(A \mapsto x^{\mathsf{T}} A x\) <kbd>is a linear function of the matrix</kbd> $A,$ <kbd>and the maximum over a family of linear functions is a convex function.  Similarly, it can be shown that</kbd> \(\lambda_1(A)\) <kbd>is a <strong>concave</strong> function of $A,$ and can be obtained by replacing &ldquo;maximize&rdquo; with &ldquo;minimize&rdquo; in the variational problem.</kbd></p>
<p>The Courant-Fischer theorem takes this one step further by providing a variational characterization of <em>any</em> of the eigenvalues.  Specifically,</p>
<p><kbd><strong>Theorem (Courant-Fischer)</strong>: Let</kbd> \(A \in \R^{n \times n}\) <kbd>be a real symmetric matrix.  Then</kbd></p>
<p>\begin{equation}
\lambda_k = \underset{U}{\text{min}}\ \Bigl\{\underset{x \in \R^n}{\text{max}}\ x^{\mathsf{T}} A x\ \big|\ ||x||_2 = 1, x \in U, \mathsf{dim}(U) = k \Bigr\},\notag
\end{equation}</p>
<p><kbd>where the minimization is over subspaces of</kbd> \(\R^n\).</p>
<p>Intuitively, if \(x\) can live in a \(k\) dimensional space, then the maximization over \(x\) can always attain <em>at least</em> the value of the \(k^{\text{th}}\) smallest eigenvalue, no matter which \(k\) dimensional space it is confined to.  From the optimization problem described earlier, the maximizing \(x\) is given by the eigenvector corresponding to the largest eigenvalue, subject to the constraint that that eigenvector be in the space \(U\).  The minimizing space \(U\) is then of course given by the space spanned by the \(k\) smallest eigenvectors \(U = \mathsf{span}\{v_1, \ldots, v_k\}\).</p>
<p>To finally see the connection with trace optimization problems, it follows from the Courant-Fisher theorem that the value of the optimization problem (generalizing the first variational characterization above)</p>
<p>\begin{equation}
\begin{aligned}
\underset{U \in \R^{n \times k}}{\text{minimize}}&amp;\ \mathsf{tr}\ U^{\mathsf{T}} A U\\
\text{subject to}&amp;\ U^{\mathsf{T}} U = I
\end{aligned}\tag{${\mathcal{E}_k}$}
\end{equation}</p>
<p>is given by \(\lambda_1 + \cdots + \lambda_k\), and that an optimizing \(U = \begin{bmatrix}v_1\ \cdots\ v_k \end{bmatrix}\) is given by the \(n \times k\) matrix formed from the first \(k\) eigenvectors.  When reading this problem, be careful to remember that \(U^{\mathsf{T}} U = I_k\) definitely does not imply \(UU^{\mathsf{T}} = I_n\), unless \(n = k\).</p>
<p>This optimization problem can be obtained from the Courant-Fischer theorem more-or-less by simply taking the summation of the first \(k\) eigenvalues and stacking separate &ldquo;\(x\)&rdquo; optimization variables together into a matrix, along with elementary facts about traces.</p>
<p><kbd><strong>Main Point</strong>: The main point I am making here is that Problem</kbd> $(\mathcal{E}_k),$ (parameterized by the number \(k\) of vectors in \(U\)) <kbd>which is a <em>non-convex</em> optimization problem, can be solved by computing an eigendecomposition of the symmetric matrix</kbd> $A.$  <kbd>This is remarkable, since there exist efficient algorithms for computing this factorization.</kbd></p>


<h3 class="group " id="generalized-eigenvalues"
    >Generalized Eigenvalues<a href="#generalized-eigenvalues"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>The eigenvalue problem of finding a pair \((\lambda, v)\) for a matrix \(A\) such that \(Av = \lambda v\) can be generalized in various ways.  One particular direction is to consider a generalized eigenvalue problem (GEVP) associated to a <em>pair</em> of matrices \((A, B)\) wherein we seek to find \((\lambda, v)\) such that \(Av = \lambda Bv\).  Such pairs are now referred to as <em>generalized</em> eigenpairs associated with \((A, B)\).</p>
<p>The case of most interest in applications is where \(A\) is symmetric, and \(B \succ 0\) is <em>positive definite</em>.  Recall that a positive definite matrix can be characterized by the fact that they admit a <em>Cholesky factorization</em> \(B = LL^{\mathsf{T}}\) where \(L\) is an invertible lower-triangular matrix &ndash; this is a particular type of matrix square root.  Using this Cholesky decomposition, the GEVP can be reduced to an ordinary EVP:</p>
<p>\begin{equation}
\begin{aligned}
Av &amp;= \lambda Bv\\
\iff A L^{-\mathsf{T}} L^{\mathsf{T}} v &amp;= \lambda LL^{\mathsf{T}} v\\
\iff \bigl(L^{-1} A L^{-\mathsf{T}}) w &amp;= \lambda w;\ w = L^{\mathsf{T}} v,
\end{aligned}\notag
\end{equation}</p>
<p>wherein \(S \overset{\Delta}{=} L^{-1} A L^{-\mathsf{T}}\) is a symmetric matrix with eigenpairs \((\lambda, w)\) from which the generalized eigenvectors \(v = L^{-\mathsf{T}} w\) can be recovered.  (<kbd><strong>Remark</strong>: I avoid the computation $B^{-1} A$ since this need not be symmetric.</kbd>)</p>
<p>To understand the intuition of this situation, we first apply the fact that the eigenvectors \(W = \begin{bmatrix} w_1\ \cdots\ w_n \end{bmatrix}\) of \(S\) will form an orthogonal basis for \(\R^n\).  Thus, since \(W = L^{\mathsf{T}} V\) we must have</p>
<p>\begin{equation}
\begin{aligned}
W^{\mathsf{T}} W &amp;= I\\
\implies V^{\mathsf{T}} LL^{\mathsf{T}} V &amp;= I\\
\implies V^{\mathsf{T}} B V &amp;= I.
\end{aligned}\notag
\end{equation}</p>
<p>Thus, it becomes natural to construct the associated trace optimization problems for the pair \((A, B)\)</p>
<p>\begin{equation}
\begin{aligned}
\underset{U \in \R^{n \times k}}{\text{minimize}}&amp;\ \mathsf{tr}\ U^{\mathsf{T}} A U\\
\text{subject to}&amp;\ U^{\mathsf{T}} B U = I.
\end{aligned}\tag{${\mathcal{G}_k}$}
\end{equation}</p>
<p>To reiterate, the point here is that this is a class of <em>non-convex</em> optimization problems which can be efficiently solved by eigenvalue algorithms.  Indeed, the value of this problem is given by \[\lambda_1(A, B) + \cdots + \lambda_k(A, B),\] the sum of the first \(k\) generalized eigenvalues, and the optimizing matrix \(U \in \R^{n \times k}\) constitutes the first \(k\) generalized eigenvectors of \((A, B)\).  Keep in mind that the problem is only well defined when \(A\) is symmetric, and \(B\) is positive-definite (though a reasonable solution for the semidefinite case can probably still be worked out).  If these assumptions do not hold, the situation becomes substantially more complicated.</p>


<h4 class="group " id="interpretations-and-intuition"
    >Interpretations and Intuition<a href="#interpretations-and-intuition"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h4>

<p>The generalized eigenvectors \(V = \begin{bmatrix}v_1\ \cdots\ v_n \end{bmatrix}\) are still a basis for \(\R^n\) (since \(L\) is invertible) but they are no longer orthogonal.  Instead, they are orthogonal with respect to an inner product modified by the matrix $B.$  That is, if we write \(x^{\mathsf{T}} y \overset{\Delta}{=} \lang x, y \rang\) then we might generalize this by writing \[\lang x, y \rang_{B^{-1}} = x^{\mathsf{T}} B y = \lang L^{\mathsf{T}} x, L^{\mathsf{T}} y \rang,\] where the inner product \(\lang \cdot, \cdot \rang_{B^{-1}}\) (which, recall, describes the geometry of the space through the relationship to angles \(||x|| ||y|| \mathsf{cos}\ \theta_{xy} = \lang x, y \rang\) and distances \(||x|| = \sqrt{\lang x, x \rang}\)) has modified the geometry of \(\R^n\).  Using a subscript \(B^{-1}\), rather than just \(B\) for this inner product is, I believe, the convention.</p>
<p>We&rsquo;ve now equipped \(\R^n\) with two different inner products &ndash; the ordinary one \(\langle x, y \rangle = x^{\mathsf{T}}y\), and the new one \(\langle x, y \rangle_{B^{-1}} = x^{\mathsf{T}} B y\).  To gain some intuition for this new inner product, consider the ordinary euclidean ball \(\mathbb{B} = \{x \in \R^n\ |\ ||x||_2 \le 1\}\) and the ball in the norm \(||x||_{B^{-1}} = \sqrt{x^{\mathsf{T}} B^{} x}\), namely \[\mathbb{B}_B = \{x \in \R^n\ |\ ||x||_{B^{-1}} \le 1\},\] which is an ellipsoid.  Since if \(y = L^{-\mathsf{T}} x\) and \(||x||_2 \le 1\) then \(||L^{\mathsf{T}} y||_2 = ||y||_{B^{-1}} \le 1\) the mapping \(x \mapsto L^{-\mathsf{T}}\) transforms the ordinary euclidean ball into \(\mathbb{B}_B\) as in \(L^{-\mathsf{T}} \mathbb{B} = \mathbb{B}_B\).</p>
<p>To visualize the ball \(L^{-\mathsf{T}} \mathbb{B}\) recognize that if \(u_i\) is an (ordinary) eigenvector of \(B\), then \(u_i^{\mathsf{T}} B^{} u_i = \lambda_i(B)\) and therefore any vector proportional to \(u_i\) and with an euclidean length between \(0\) and \(1 / \sqrt{\lambda_i(B)}\) will be inside of the ball.  Essentially what this means is that the eigenvectors \(u_i\) of \(B\) define the <em>principle axes</em> of the ellipse, each of which have a length \(1 / \sqrt{\lambda_i(B)}\).  After stretching the space through the mapping \(x \mapsto L^{-\mathsf{T}}x\) the directions associated with large eigenvalues are shortened, and directions associated with small eigenvalues of \(B\) are lengthened.  Another way to think about it is through a topographic map &ndash; the terrain in the direction of small eigenvalues is very steep.</p>
<figure><img src="stretched_space.svg"
         alt="Figure 1: Mappings of various shapes from Euclidean space into the stretched space."/><figcaption>
            <p><span class="figure-number">Figure 1: </span>Mappings of various shapes from Euclidean space into the stretched space.</p>
        </figcaption>
</figure>

<p>The figure above depicts the transformation of Euclidean space \(\R^n\) by the mapping \(x \mapsto L^{-\mathsf{T}}x\).  The dotted lines are points in the original space \(\R^n\), and the solid lines are the result of applying the transformation.  The ellipse in particular includes eigenvectors of \(B\) (as calculated by <kbd>scipy.linalg.eigh</kbd>) scaled by \(1 / \sqrt{\lambda_i(B)}\) in white.  The contours are drawn for the function \(x \mapsto \frac{1}{2}x^{\mathsf{T}} B x\) and I&rsquo;ve used the particular matrix \[B = \begin{bmatrix}5 &amp; 1 \\ 1 &amp; 1 \end{bmatrix},\] which has an eigendecomposition \[B \approx \begin{bmatrix}0.230 &amp; -0.973 \\ -0.973 &amp; -0.230 \end{bmatrix}\begin{bmatrix}0.764 &amp; 0 \\ 0 &amp; 5.236 \end{bmatrix}\begin{bmatrix}0.230 &amp; -0.973 \\ -0.973 &amp; -0.230 \end{bmatrix}^{\mathsf{T}}.\]</p>


<h2 class="group " id="applications-and-computations"
    >Applications and Computations<a href="#applications-and-computations"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>Practically speaking, a reliable means of solving generalized eigenvalue problems is by means of <kbd>scipy.linalg.eig</kbd> (which will try to solve \(Av = \lambda B v\) for any square \(A, B\)) and <kbd>scipy.linalg.eigh</kbd> (which assumes \(A\) is symmetric, \(B\) is positive definite and always returns a <em>real</em> factorization).  These functions also provide the option to (efficiently) compute and return only a subset of eigenvalues (along with corresponding eigenvectors).  Moreover, this subset can be defined either by <em>indices</em> (<em>e.g.,</em> return the third, fourth, and fifth) or by <em>values</em> (<em>e.g.,</em> return eigenvalues between \(0\) and \(1\)).  These are incredibly useful featuers in practice.</p>


<h3 class="group " id="principle-component-analysis"
    >Principle Component Analysis<a href="#principle-component-analysis"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>The most famous application of trace optimization problems is the feature-extraction or dimensionality-reduction technique known as <a
    class="link"
    href="https://en.wikipedia.org/wiki/Principal_component_analysis"target="_blank" rel="noopener">Principle Component Analysis</a
>
 (PCA).  Suppose we have a random variable \(X \in \R^n\) with mean zero \(\mathbb{E}[X] = 0\) and variance \(\mathbb{V}[X] = \Sigma \succ 0\).  The goal of PCA is to find a matrix \(V \in \R^{n \times k}\) having orthogonal columns (so that \(V^{\mathsf{T}} V = I\)) and such that the total variance of \(Y = V^{\mathsf{T}} X\) is <em>maximized</em>.  The number \(k\) of columns of \(V\) is the key here &ndash; we will usually choose \(k \ll n\) so that the idea of maximizing the variance corresponds to searching for the \(k\) &ldquo;most explanatory&rdquo; directions in \(\R^n\).  Another way to think about this is that we are finding a subspace of dimension \(k\) which &ldquo;explains&rdquo; the greatest amount of variability in \(X\).</p>
<p>It is easy to set this problem up as a trace optimization problem.  The total variance of \(Y\) (which is necessarily also zero mean) is given by</p>
<p>\begin{equation}
\begin{aligned}
\mathbb{E} Y^{\mathsf{T} }Y
&amp;= \mathbb{E} \mathsf{tr}\ YY^{\mathsf{T}}\\
&amp;= \mathbb{E} \mathsf{tr}\ V^{\mathsf{T}} X X^{\mathsf{T}} V\\
&amp;= \mathsf{tr}\ V^{\mathsf{T}} \Sigma V.\\
\end{aligned}\notag
\end{equation}</p>
<p>Thus, the problem comes down to solving:</p>
<p>\begin{equation}
\begin{aligned}
\underset{V \in \R^{n \times k}}{\text{maximize}}&amp;\ \mathsf{tr}\ V^{\mathsf{T}} \Sigma V\\
\text{subject to}&amp;\ V^{\mathsf{T}} V = I,
\end{aligned}\notag
\end{equation}</p>
<p>which is a standard trace optimization problem.  The solution of which is, famously, given by \(k\) eigenvectors corresponding to the \(k\) largest eigenvalues of \(\Sigma\).</p>
<p><kbd><strong>Remark</strong>: Given an actual data matrix $X \in \R^{N \times n}$, one might naturally think to just estimate a covariance matrix $\widehat{\Sigma} \approx \frac{1}{N} X^{\mathsf{T}} X$.  However, this is a mistake.  The eigendecomposition of $\widehat{\Sigma}$ can be computed directly from a</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Singular_value_decomposition"target="_blank" rel="noopener">Singular Value Decomposition</a
>
 <kbd>of the matrix $X$ itself.  Indeed, for most purposes I am aware of, the actual computation of $X^{\mathsf{T}} X$ is unnecessary and should not be performed in practice.</kbd></p>


<h3 class="group " id="weighted-principle-component-analysis"
    >Weighted Principle Component Analysis<a href="#weighted-principle-component-analysis"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>The choice in the last section of maximizing the total variance of \(Y = V^{\mathsf{T}} X\) was arbitrary.  There is a natural generalization of this formulation where we will still find a matrix \(V\) to compute some projection \(Y = V^{\mathsf{T}} X\) as usual, but now we will weight the &ldquo;importance&rdquo; of the various components of \(X\) by some matrix \(W\) (which is often likely to be diagonal, though this is not a requirement).  A natural objective is then the quantity \(\mathbb{E}||V^{\mathsf{T}} W^{1/2} X||_2^2 = \mathsf{tr}\ V^{\mathsf{T}} W^{1/2} \Sigma W^{1/2} V\).  This objective can be plugged into another trace optimization problem with constraint \(V^{\mathsf{T}} V = I\).</p>
<p>Interestingly, if we absorb the factor \(W^{1/2}\) into \(V\) as in \(\tilde{V}_W = W^{1/2} V\) we can re-write the constraint in terms of \(\tilde{V}_W\) as in \(V^{\mathsf{T}} V = \tilde{V}_W W^{-1} \tilde{V}_W = I\).  That is, the matrix \(\tilde{V}_W\) will be orthogonal with respect to the inner product \(\lang\cdot, \cdot\rang_W\).</p>
<figure><img src="faces_pca.svg"
         alt="Figure 2: Comparing ordinary PCA with a weighted PCA that emphasizes the right half of the Olivetti Faces. The overall reconstruction error is lower for the unweighted version, but the reconstruction error for the right half of the image is lower in the weighted version. Notice that the right eye of the weighted reconstruction has a less pronouned &amp;ldquo;glasses&amp;rdquo; artifact than does the left eye, or the right eye of the unweighted version."/><figcaption>
            <p><span class="figure-number">Figure 2: </span>Comparing ordinary PCA with a weighted PCA that emphasizes the right half of the Olivetti Faces.  The overall reconstruction error is lower for the unweighted version, but the reconstruction error for the right half of the image is lower in the weighted version.  Notice that the right eye of the weighted reconstruction has a less pronouned &ldquo;glasses&rdquo; artifact than does the left eye, or the right eye of the unweighted version.</p>
        </figcaption>
</figure>

<p>The PCA weighting scheme will not necessarily make any significant difference in regards to the estimated or reconstructed vector \(\widehat{X} = \sum_{k = 1}^K \lang v_i, X \rang_W W^{-1} v_i\), particularly if both the covariance matrix of \(X\) and the weight matrix \(W\) are diagonal.  The differences arise when there is significant correlation structure in \(X\), in which case the weighting scheme can substantively impact the correlation structure of the factors \(v_i\).</p>
<p><kbd><strong>Remark</strong>: When reconstructing an estimate of an image (or other data) from a weighted PCA, one must be careful to carry out the linear projections with respect to the weighted inner product, otherwise the results will be nonsensical.</kbd></p>


<h3 class="group " id="rayleigh-quotients-and-fisher-s-linear-discriminant"
    >Rayleigh Quotients and Fisher&rsquo;s Linear Discriminant<a href="#rayleigh-quotients-and-fisher-s-linear-discriminant"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>Principle component analysis is a ubiquitous technique for dimensionality reduction.  However, it suffers a major drawback in that it is completely <em>unsupervised</em>, whereas data often come attached with <em>class labels</em>.  For example, we may have a random variable \(X\) and, associated with \(X\) is a random <em>class label</em> \(Y \in [C] = \{1, \ldots, C\}\) indicating that \(X\) belongs to one of \(C\) separate classes.  These classes may represent a data source, a target label, or be simply some discrete feature of the data.</p>
<p>For some typical concrete examples, ponder these possibilities:</p>
<ul>
<li>The data \(X\) are natural language embeddings and the labels \(Y\) are the language (English, French, Persian, &hellip;)</li>
<li>Data \(X\) represent stock returns, labels \(Y\) indicate the stock&rsquo;s industry</li>
<li>\(X\) is real-valued features of a house, and the class label \(Y\) is a categorical variable such as the neighbourhood.</li>
</ul>
<p>In PCA, the projection matrix \(V\) is constructed from analyzing \(X\) alone, disregarding the class label \(Y\).  Ideally, we would use the information available in \(Y\) to construct \(V\), and Fisher&rsquo;s Linear Discriminant (FLD) is one particular method to do so.  The idea is to find the projection \(V\) such that samples \(X_i\) which share a class label \(Y_i\) are clustered close to each other, and at the same time that clusters formed by samples with different class labels are far apart.</p>
<p>Quantitatively, first suppose that we&rsquo;ve centered our data \(\mu = \mathbb{E}[X] = 0\).  Then, to establish more notation, let \(\mu_c = \mathbb{E}[X\ |\ Y = c]\) and \(\Sigma_c = \mathbb{E}[(X - \mu_c)(X - \mu_c)^{\mathsf{T}}\ |\ Y = c]\) be the <em>class conditional</em> mean and variance of \(X\), respectively.  Finally, \(\mathbb{P}(Y = c) = \pi_c\) be the probability that each data sample is from class \(c\).</p>
<p>Now, just as we did for principle component analysis, let&rsquo;s consider the variance of the projection \(V^{\mathsf{T}} X\).  To bring out the class labels, we will condition on the value of \(Y\):</p>
<p>\begin{equation}
\begin{aligned}
\mathbf{Var}[V^\mathsf{T} X]
&amp;= V^{\mathsf{T}} \mathbb{E}(X - \mathbb{E}[X])(X - \mathbb{E}[X])^{\mathsf{T}} V\\
&amp;= V^{\mathsf{T}} \mathbb{E}[XX^{\mathsf{T}}] V\\
&amp;= V^{\mathsf{T}} \mathbb{E}\mathbb{E}[XX^{\mathsf{T}} \ |\ Y]V\\
&amp;= V^{\mathsf{T}} \sum_c \pi_c \mathbb{E}[XX^{\mathsf{T}} \ |\ Y = c]V\\
&amp;\overset{(a)}{=} V^{\mathsf{T}} \sum_c \pi_c \bigl(\mathbf{Var}[X\ |\ Y = c] + \mathbb{E}[X\ |\ Y = c]\mathbb{E}[X\ |\ Y = c]^{\mathsf{T}}\bigr) V\\
&amp;= V^{\mathsf{T}} \sum_c \pi_c \bigl(\Sigma_c + \mu_c \mu_c^{\mathsf{T}} \bigr) V,
\end{aligned}\notag
\end{equation}</p>
<p>where \((a)\) is rearranging the variance formula \(\mathbf{Var}(Z) = \mathbb{E}[ZZ^{\mathsf{T}}] - \mathbb{E}[Z]\mathbb{E}[Z]^{\mathsf{T}}\).  The same result could be got by applying the <a
    class="link"
    href="https://en.wikipedia.org/wiki/Law_of_total_variance"target="_blank" rel="noopener">Law of Total Variance</a
>
, which is what I did first before seeing that the above calculation is easier.</p>
<p>In the context of FLD, the term \(\Sigma_w = \sum_c \pi_c \Sigma_c\) is a measure of the average <em>within-class</em> variance (<em>i.e.,</em> the variance of the data conditioned on a class label) and \(\Sigma_b = \sum_c \pi_c \mu_c \mu_c^{\mathsf{T}}\) is the <em>between-class</em> variance (<em>i.e.</em>, a measure of the separation between the classes).</p>
<p>The key idea of FLD is to <em>maximize</em> the within class variance (of the projection) \(V^{\mathsf{T}} \Sigma_w V\), and <em>minimize</em> the between class variance \(V^{\mathsf{T}} \Sigma_b V\).  Projections which achieve this goal are identifying the axes which simultaneously explain the differences and similarities between classes.</p>
<p>To operationalize what it means to minimize the within-class variance, and maximize the between-class variance we need two things: (1) we need to turn the matrix-valued variances into a scalar and (2) we need to define the tradeoff between maximizing and minimizing.  There are surely many ways of doing this, but the choice of objective function made by FLD is to minimize the following scalar cost:</p>
<p>\begin{equation}
J(V) = \mathsf{tr} (V^{\mathsf{T}} \Sigma_w V)^{-1} (V^{\mathsf{T}} \Sigma_b V).
\end{equation}</p>
<p>The trace serves to turn the objective into a scalar, and inverting the within-class variance means that the minimization objective \(J\) will lead us to attempt to <em>maximize</em> that wiithin-class variance.</p>
<p>While the function \(J(V)\) obviously looks like it has a lot of interesting structure, it may not be immediately clear what to do with it or how to optimize it (certainly, it is not a convex function of \(V\)).  However, we can recognize an <em>analogy</em> &ndash; suppose that \(V\) is just a vector, and that \(\Sigma_b = I\).  In this case, \(J(v) = \frac{v^{\mathsf{T}} \Sigma_w v}{v^{\mathsf{T}} v}\), which is a <a
    class="link"
    href="https://en.wikipedia.org/wiki/Rayleigh_quotient"target="_blank" rel="noopener">Rayleigh Quotient</a
>
 for the matrix \(\Sigma_w\).  The reader familiar with how to connect Rayleigh quotients to eigenvalues should be inspired &ndash; the Rayleigh Quotient is invariant to rescaling of the vector, so let&rsquo;s consider a change of basis matrix \(C \in \mathbb{R}^{k \times k}\) (<em>i.e.</em>, an invertible matrix) and check the objective:</p>
<p>\begin{equation}
\begin{aligned}
J(VC)
&amp;= \mathsf{tr}\ (C^{\mathsf{T}} V^{\mathsf{T}} BV C)^{-1} (C^{\mathsf{T}} V^{\mathsf{T}} A V C)\\
&amp;= \mathsf{tr}\ C^{-1} (V^{\mathsf{T}} BV)^{-1} C^{-\mathsf{T}} C^{\mathsf{T}} (V^{\mathsf{T}} A V) C\\
&amp;= \mathsf{tr}\ C^{-1} (V^{\mathsf{T}} BV)^{-1} C^{-\mathsf{T}} C^{\mathsf{T}} (V^{\mathsf{T}} A V) C\\
&amp;= \mathsf{tr}\ (V^{\mathsf{T}} BV)^{-1} (V^{\mathsf{T}} A V) \\
&amp;= J(V).
\end{aligned}\notag
\end{equation}</p>
<p>Therefore, the objective is invariant to a change of basis (just like ordinary Rayleigh Quotients are invariant to rescalings) so we might as well impose the convenient constraint that \(V^{\mathsf{T}} \Sigma_w V = I_k\) since if \(U\) is some minimizer of \(J\) which does not satisfy this condition, we can rescale \(U\) so that it does.  Thus, we are left with the GEVP:</p>
<p>\begin{equation}
\begin{aligned}
\underset{V \in \mathbb{R}^{n \times k}}{\text{minimize}}&amp;\ \mathsf{tr}(V^{\mathsf{T}} \Sigma_b V)\\
\text{subject to}&amp;\ V^{\mathsf{T}} \Sigma_w V = I_k,
\end{aligned}\notag
\end{equation}</p>
<p>the solutions of which provide a matrix \(V \in \R^{n \times k}\) which can be used to reduce the dimensionality of the data \(X\) such that the different classes are well separated in the lower dimensional space.</p>
<p><kbd>Remark: Notice that optimizing the Rayleigh quotient provides another <em>variational characterization</em> of the generalized eigenvalues!</kbd></p>
<p>There is however an important caveat.  Notice that \(\Sigma_w = \sum_c \pi_c \mu_c \mu_c^{\mathsf{T}}\) is a sum of \(C\) rank-1 matrices.  This matrix therefore has rank at most \(C\), and then \(V^{\mathsf{T}} \Sigma_w V\) must also have rank at most \(C\), though it is of dimension \(k \times k\) (where \(V\) is \(n \times k\) and \(k \le n\)).  Thus, we need to be careful that \(k \le C\) (smaller for degenerate cases), otherwise the inverse is not well defined &ndash; this places a restriction on the dimensionality of the space onto which we project.</p>


<h4 class="group " id="example"
    >Example<a href="#example"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h4>

<p>One of the most natural datasets on which to apply this technique is the classic <a
    class="link"
    href="https://en.wikipedia.org/wiki/MNIST_database"target="_blank" rel="noopener">MNIST handwritten digits data</a
>
, for which I&rsquo;ve just used the data <a
    class="link"
    href="https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset"target="_blank" rel="noopener">shipped with scikit-learn</a
>
.  This dataset consists of \(1797\) examples of \(8 \times 8\) images of written digits between \(0\) and \(9\).  Pixel intensities are between \(0\) and \(16\).</p>
<p>Simply flattening out the \(8 \times 8\) images into a vector gives us a data matrix \(X \in \R^{1797 \times 64}\), which I&rsquo;ll project into a lower dimensional subspace using either FLD or PCA &ndash; the labels \(y \in \{0, \ldots, 9\}^{1797}\) are natural class labels for FLD.</p>
<p>Calculating the projections onto a \(K \ll 64\) dimensional subspace (after centering and standardizing the \(X\) matrix) results in two new datasets \(\widehat{X}_{\text{pca}} \in \R^{1797 \times K}\) and \(\widehat{X}_{\text{fld}} \in \R^{1797 \times K}\), the former being the projection onto the axes which maximizes the variance, and the latter being constructed to try to separate the clumps of classes.</p>
<p>In order to make a nice visual comparison between these two projections, I&rsquo;d like to try to &ldquo;line up&rdquo; the projected data as well as possible so that if the data were to fall into similar clumps for PCA and FLD, that these clumps would appear in similar locations in a visualization.  To do so, I&rsquo;ve rotated the matrix \(\widehat{X}_{\text{fld}}\) with an orthogonal matrix \(Q\) obtained from solving an <a
    class="link"
    href="https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem"target="_blank" rel="noopener">Orthogonal Procrustes Problem</a
>
.</p>
<details>
  <summary>Orthogonal Procrustes Problem</summary>
  <p>This is a <em>bonus</em> problem which is similar in spirit to a trace optimization problem, but takes on a different form and has a different solution than the problems I&rsquo;ve considered so far.  Specifically, we solve</p>
<p>\begin{equation}
\begin{aligned}
\underset{Q \in \R^{n \times n}}{\text{minimize}} &amp;\quad \frac{1}{2}||Q\widehat{X}_{\text{fld}} - \widehat{X}_{\text{pca}}||_F^2\\
\text{subject to} &amp;\quad Q^\mathsf{T} Q = I,
\end{aligned}\notag
\end{equation}</p>
<p>so that \(\widehat{X}_{\text{fld}}\) is rotated (and possibly reflected) to better line up with \(\widehat{X}_{\text{pca}}\).  We can make this look like a trace optimization problem by expanding the objective as in</p>
<p>\begin{equation}
\begin{aligned}
\frac{1}{2}||Q\widehat{X}_{\text{fld}} - \widehat{X}_{\text{pca}}||_F^2
&amp;= \frac{1}{2}\mathsf{tr}(\widehat{X}_{\text{fld}}^{\mathsf{T}}Q^{\mathsf{T}}Q\widehat{X}_{\text{fld}} + \widehat{X}_{\text{pca}}^{\mathsf{T}}\widehat{X}_{\text{pca}}) - 2\mathsf{tr}(Q^{\mathsf{T}} \widehat{X}_{\text{pca}}\widehat{X}_{\text{fld}}^{\mathsf{T}})\\
&amp;\overset{(a)}{=} \frac{1}{2}\mathsf{tr}(\widehat{X}_{\text{fld}}^{\mathsf{T}}\widehat{X}_{\text{fld}} + \widehat{X}_{\text{pca}}^{\mathsf{T}}\widehat{X}_{\text{pca}}) - \mathsf{tr}(Q^{\mathsf{T}} \widehat{X}_{\text{pca}}\widehat{X}_{\text{fld}}^{\mathsf{T}})\\
\end{aligned}\notag
\end{equation}</p>
<p>where \((a)\) uses the orthogonality constraint.  This leaves us with a sort of &ldquo;one-sided&rdquo; trace optimization problem, distinct from the problems encountered so far:</p>
<p>\begin{equation}
\begin{aligned}
\underset{Q \in \R^{n \times n}}{\text{maximize}} &amp;\quad \mathsf{tr}(Q^{\mathsf{T}} \widehat{X}_{\text{pca}}\widehat{X}_{\text{fld}}^{\mathsf{T}})\\
\text{subject to} &amp;\quad Q^\mathsf{T} Q = I.
\end{aligned}\notag
\end{equation}</p>
<p>To actual see the solution of this problem, expand out the related term</p>
<p>\begin{equation}
\frac{1}{2}||Q - \widehat{X}_{\text{pca} }\widehat{X}_{\text{fld}}^{\mathsf{T}}||_F^2 = \frac{1}{2}\mathsf{tr}(Q^{\mathsf{T}}Q + \widehat{X}_{\text{fld}}\widehat{X}_{\text{pca}}^{\mathsf{T}}\widehat{X}_{\text{pca}} \widehat{X}_{\text{fld}}^{\mathsf{T}}) - \mathsf{tr}(Q^{\mathsf{T}} \widehat{X}_{\text{pca}}\widehat{X}_{\text{fld}}^{\mathsf{T}}),\notag
\end{equation}</p>
<p>which similarly involves constants and the above trace objective.  We can therefore equivalently solve \[\underset{Q: Q^{\mathsf{T}}Q = I}{\text{minimize}}\ \frac{1}{2}||Q - \widehat{X}_{\text{pca} }\widehat{X}_{\text{fld}}^{\mathsf{T}}||_F^2.\] Apply a <a
    class="link"
    href="https://en.wikipedia.org/wiki/Singular_value_decomposition"target="_blank" rel="noopener">Singular Value Decomposition</a
>
 \(\widehat{X}_{\text{pca} }\widehat{X}_{\text{fld}}^{\mathsf{T}} = U\Sigma V^{\mathsf{T}}\) to the product, and using the fact that \(U, V\) are orthogonal, and therefore we can freely multiply by \(U, V\) inside the norm without change its value, leads to the solution \(Q = UV^{\mathsf{T}}\), with cost \(\sum_{i = 1}^m (1 - \sigma_i)^2\) for singular values \(\sigma_i\) of \(\widehat{X}_{\text{pca} }\widehat{X}_{\text{fld}}^{\mathsf{T}}\).  Precisely,</p>
<p>\begin{equation}
\begin{aligned}
\underset{Q: Q^{\mathsf{T}}Q = I}{\text{min}}\ \frac{1}{2}||Q - \widehat{X}_{\text{pca}}\widehat{X}_{\text{fld}}^{\mathsf{T}}||_F^2
&amp;= \underset{Q: Q^{\mathsf{T}}Q = I}{\text{min}}\ \frac{1}{2}||Q - U\Sigma V^{\mathsf{T}}||_F^2\\
&amp;\overset{(a)}{=} \underset{D: D^{\mathsf{T}}D = I}{\text{min}}\ \frac{1}{2}||UDV^{\mathsf{T}} - U\Sigma V^{\mathsf{T}}||_F^2\\
&amp;\overset{(b)}{=} \underset{D: D^{\mathsf{T}}D = I}{\text{min}}\ \frac{1}{2}||D - \Sigma||_F^2\\
&amp;\overset{( c)}{=} \frac{1}{2}\sum_{i = 1}^m (1 - \sigma_i)^2,
\end{aligned}\notag
\end{equation}</p>
<p>where \((a)\) expresses \(Q\) in the same basis (\(D\) is not yet necessarily diagonal! <em>e.g.,</em> consider \(D = U^{\mathsf{T}} Q V\)), and \((b)\) uses the fact that \(U, V\) are orthogonal.  Minimizing the approximation error of the diagonal matrix \(\Sigma\) after equality \((b)\) <em>now</em> requires that \(D\) be diagonal, and meeting the equality (which is derived from the requirement that \(Q^{\mathsf{T}}Q = I\)) necessitates (equality \(( c)\)) that $D = I.$   \(\ \square\)</p>

</details>

<p>I&rsquo;ve used this matrix \(Q\) to better align the data in the animation below.  Notice that each coloured clump (corresponding to different digits) is roughly aligned in space &ndash; there is nothing intrinsic about PCA and FLD that would encourage this alignment.</p>
<figure><img src="pca_fld_animation.gif"
         alt="Figure 3: FLD and PCA projections of MNIST digits data"/><figcaption>
            <p><span class="figure-number">Figure 3: </span>FLD and PCA projections of MNIST digits data</p>
        </figcaption>
</figure>

<p>In addition to the figure, I also fit a simple <kbd>sklearn.svm.LinearSVC</kbd> to the projected data in order to verify that the clusters are in fact well informed by the class labels.  The <a
    class="link"
    href="https://en.wikipedia.org/wiki/Phi_coefficient"target="_blank" rel="noopener">MCC</a
>
 value is annotated in the title of the each figure.</p>


<h3 class="group " id="optimizing-signal-to-noise-ratio"
    >Optimizing Signal to Noise Ratio<a href="#optimizing-signal-to-noise-ratio"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>As a final example, let&rsquo;s examine a problem in signal processing.  Let \(X \in \R^{n}\) be an <em>unobserved</em> zero mean random vector with variance \(\Sigma\) and \(E \in \R^{n}\) be another zero-mean and unobserved random vector with variance \(\Omega\).  The observed quantity is given by \(Y = X + E\), <em>i.e.</em>, a noisy measurement of the signal of interest.</p>
<p>Similarly to PCA, we will find a matrix \(V \in \R^{n \times k}\) to project \(Y\) onto some lower dimensional space, but this time we will optimize the <em>signal-to-noise ratio</em> (SNR), where the &ldquo;signal&rdquo; is \(V^{\mathsf{T}} X\) (containing information about the vector \(X\) that we care about) and the &ldquo;noise&rdquo; is \(V^{\mathsf{T}} E\) (which is just corrupted by \(E\)).  This leads to the optimization problem:</p>
<p>\begin{equation}
\begin{aligned}
\underset{V \in \R^{n \times k}}{\text{maximize}}&amp;\ \frac{\mathbb{E}||V^{\mathsf{T}} X||_2^2}{\mathbb{E}||V^{\mathsf{T}} E||_2^2}\\
\text{subject to}&amp;\ V^{\mathsf{T}} V = I.
\end{aligned}\tag{$\mathcal{P}$}
\end{equation}</p>
<p>This is a generalization of PCA since if \(E\) is isotropic, \(\mathbb{E}||V^{\mathsf{T}} E||_2^2\) is a constant given the constraint, and can be dropped.  Moreover, this problem is different from the GEVPs encountered earlier, because we enforce the constraint that \(V^{\mathsf{T}}V = I\), as opposed to \(V^{\mathsf{T}}\Omega V = I\).</p>
<p>While this problem doesn&rsquo;t immediately take the form of an eigenvalue problem, we can relate it to one with some clever manipulations (essentially drawn from <a
    class="link"
    href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=0d185e6de595bd3844909d3606e9218a498a9bd8"target="_blank" rel="noopener">Kokiopoulou et. al.</a
>
).  Supposing that \(s^\star &gt; 0\) is the optimal SNR, it must be the case that \[\frac{\mathbb{E}||V^{\mathsf{T}} X||_2^2}{\mathbb{E}||V^{\mathsf{T}} E||_2^2} \le s^\star\] for every orthogonal matrix $V.$  Therefore, denoting by \(\mathcal{O}_k\) the set of \(n \times k\) matrices with orthogonal columns, we have</p>
<p>\begin{equation}
\begin{aligned}
\forall V \in \mathcal{O}_k:\ \mathbb{E}||V^{\mathsf{T}} X||_2^2 - s^\star \mathbb{E}||V^{\mathsf{T}} E||_2^2 &amp;\le 0\\
\implies \forall V \in \mathcal{O}_k:\ \mathbb{E} \mathsf{tr}\ V^{\mathsf{T}} XX^{\mathsf{T}} V - s^\star \mathbb{E} \mathsf{tr}\ V^{\mathsf{T}} EE^{\mathsf{T}} V &amp; \le 0\\
\implies \forall V \in \mathcal{O}_k:\ \mathsf{tr}\ V^{\mathsf{T}} \Sigma V - s^\star \mathsf{tr}\ V^{\mathsf{T}} \Omega V &amp;\le 0\\
\implies \forall V \in \mathcal{O}_k:\ \mathsf{tr}\ V^{\mathsf{T}} \bigl(\Sigma - s^\star \Omega\bigr) V &amp;\le 0\\
\implies \underset{V \in \mathcal{O}_k}{\text{max}}\ \mathsf{tr}\ V^{\mathsf{T}} \bigl(\Sigma - s^\star \Omega\bigr) V &amp;\le 0.\\
\end{aligned}
\end{equation}</p>
<p>From what we know of eigenvalue problems, the maximizer \(V\) in \(\mathcal{O}_k\) of \(\mathsf{tr}\ V^{\mathsf{T}} \bigl(\Sigma - s \Omega \bigr) V\) is given by the last \(k\) eigenvectors of the matrix \(\Sigma - s \Omega\).  Lets call these \(V_s\).  We want to maximize the SNR, and thus find the largest \(s\) which satisfies the above inequalities &ndash; i.e., the optimal value \(s^\star\) should be a root of the function \[\mathcal{S}(s) = \mathsf{tr}\ V^{\mathsf{T}}_s \bigl( \Sigma - s \Omega \bigr) V_s,\] a function which can be evaluated through an eigendecomposition of \(\Sigma - s \Omega\).  (<kbd><strong>Remark</strong>: expressions like $\Sigma - s\Omega$ involving a matrix minus a scalar times another matrix are often referred to as <em>matrix pencils</em>.  I am told they are named after the concept</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Pencil_%28geometry%29"target="_blank" rel="noopener">in geometry</a
>
.)</p>
<p>To check that this can actually occur, notice that since \(\Omega \succ 0\), the function \(\mathcal{S}\) must be <em>monotone decreasing</em> on \(\R_+\).  To establish the existence of an \(s^\star\) such that \(\mathcal{S}(s^\star) = 0\), and to apply bisection search, we need to show that \(\mathcal{S}\) does actually cross \(0\).  To this end, \(\mathcal{S}(0) &gt; 0\) (since \(\Sigma \succ 0\)).  Since \(\mathcal{S}\) is strictly monotone, this is enough, but we&rsquo;d like to find a compact interval where we know \(s^\star\) lies.  I show in the detail below that \(\mathcal{S}(s) &lt; 0\) for any \(s &gt; \lambda_n(\Sigma, \Omega)\), the largest generalized eigenvalue.</p>
<details>
  <summary>Generalized Eigenvalue Decomposition</summary>
  <p>Supposing that \(\Lambda = \mathsf{Dg}(\lambda_1, \ldots, \lambda_n)\) is a diagonal matrix of generalized eigenvalues and \(V \in \mathbb{R}^{n \times n}\) contains the generalized eigenvectors in the columns, we can write the GEVP as \[\Sigma V = \Omega V \Lambda.\]  Then, since \(V^{\mathsf{T}} \Omega V = I\) we have \(VV^{\mathsf{T}} \Omega V = V\) which, since \(V\) is invertible (we know it is a basis for \(\R^n\)) implies that \(VV^{\mathsf{T}} \Omega = I\).  Multiplying the GEVP on the right by \(V^{\mathsf{T}} \Omega\) then provides to us a generalized eigenvalue decomposition of \(\Sigma\) with respect to \(\Omega\): \[\Sigma = \Omega V \Lambda V^{\mathsf{T}} \Omega.\]</p>
<p>We can use these facts to analyze the function \(\mathcal{S}\) as in</p>
<p>\begin{equation}
\begin{aligned}
\mathcal{S}(s)
&amp;= \mathsf{tr}\ (\Sigma - s\Omega)\\
&amp;= \mathsf{tr}\ (\Omega V \Lambda V^{\mathsf{T}} \Omega - s\Omega)\\
&amp;= \mathsf{tr}\ \Omega (V \Lambda V^{\mathsf{T}} \Omega - sI)\\
&amp;= \mathsf{tr}\ \Omega (V \Lambda V^{\mathsf{T}} \Omega - sVV^{\mathsf{T}} \Omega)\\
&amp;= \mathsf{tr}\ \Omega V (\Lambda - sI) V^{\mathsf{T}} \Omega \\
&amp;= \mathsf{tr}\ (\Lambda - sI)V^{\mathsf{T}} \Omega^2 V, \\
\end{aligned}\notag
\end{equation}</p>
<p>and since \(V^{\mathsf{T}} \Omega^2 V \succ 0\) we must have \(\mathcal{S}(s) &lt; 0\) for any \(s &gt; \lambda_n\).  \(\ \square\)</p>

</details>

<p>Now, it follows that \(s^\star\) is the <em>unique</em> root of the monotone function \(\mathcal{S}\) and it lies in the interval \([0, \lambda_n(\Sigma, \Omega)]\).  Thus, we can find \(s^\star\) by performing bisection on \(\mathcal{S}\), and then recover the optimal matrix \(V_{s^\star}\) by computing the last \(k\) eigenvectors of \(\Sigma - s^\star \Omega\).  The value \(s^\star\), remember, is also the value of the optimal SNR.</p>
<p><kbd><strong>For the Reader</strong>: I&rsquo;ve glossed over another property that should be verified for $\mathcal{S}$ before making these conclusions.  What is it and why does it hold?</kbd></p>
<p>An example of this technique is given for the same faces dataset as the PCA example, where I&rsquo;ve corrupted the images with noise that is locally correlated in space.  Clearly the result is not particularly <strong>good</strong>, in an absolute sense, but the PCA reconstruction is completely corrupted by the anisotropic noise distribution.</p>
<figure><img src="faces_snr.svg"
         alt="Figure 4: Optimal SNR reconstruction compared to PCA. We can hardly say that the optimal SNR reconstruction is particularly good, though it at least still looks like a face!"/><figcaption>
            <p><span class="figure-number">Figure 4: </span>Optimal SNR reconstruction compared to PCA.  We can hardly say that the optimal SNR reconstruction is particularly <strong>good</strong>, though it at least still looks like a face!</p>
        </figcaption>
</figure>

<p>Some code to implement this technique in <a
    class="link"
    href="https://julialang.org/"target="_blank" rel="noopener">Julia</a
>
 is below.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#ff79c6">using</span> LinearAlgebra;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">function</span> snr_pca(Σ, Ω, V)
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;The objective function.&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> tr(V<span style="color:#ff79c6">&#39;</span> <span style="color:#ff79c6">*</span> Σ <span style="color:#ff79c6">*</span> V) <span style="color:#ff79c6">/</span> tr(V<span style="color:#ff79c6">&#39;</span> <span style="color:#ff79c6">*</span> Ω <span style="color:#ff79c6">*</span> V)
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">function</span> optimal_snr_pca(Σ, Ω, k, eps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1e-6</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;&#34;&#34;Computes the optimal projection matrix.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    n <span style="color:#ff79c6">=</span> size(Σ)[<span style="color:#bd93f9">1</span>]
</span></span><span style="display:flex;"><span>    S <span style="color:#ff79c6">=</span> s <span style="color:#ff79c6">-&gt;</span> (sum <span style="color:#ff79c6">∘</span> eigvals)(Σ <span style="color:#ff79c6">-</span> s <span style="color:#ff79c6">*</span> Ω, n <span style="color:#ff79c6">-</span> k <span style="color:#ff79c6">+</span> <span style="color:#bd93f9">1</span> <span style="color:#ff79c6">:</span> n)
</span></span><span style="display:flex;"><span>    V <span style="color:#ff79c6">=</span> s <span style="color:#ff79c6">-&gt;</span> eigvecs(Σ <span style="color:#ff79c6">-</span> s <span style="color:#ff79c6">*</span> Ω)[<span style="color:#ff79c6">:</span>, n <span style="color:#ff79c6">-</span> k <span style="color:#ff79c6">+</span> <span style="color:#bd93f9">1</span> <span style="color:#ff79c6">:</span> n]
</span></span><span style="display:flex;"><span>    s_max <span style="color:#ff79c6">=</span> eigvals(Σ, Ω)[n]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    s_star <span style="color:#ff79c6">=</span> bisection(S, s_max, eps)
</span></span><span style="display:flex;"><span>    V_star <span style="color:#ff79c6">=</span> V(s_star)
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> V_star, s_star
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">function</span> bisection(S, s_max, eps<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1e-6</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;&#34;&#34;Finds the zero of a monotone decreasing function in [0, s_max].&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    s_left <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0.0</span>
</span></span><span style="display:flex;"><span>    s_right <span style="color:#ff79c6">=</span> s_max
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">while</span> abs(s_right <span style="color:#ff79c6">-</span> s_left) <span style="color:#ff79c6">&gt;</span> eps
</span></span><span style="display:flex;"><span>        s <span style="color:#ff79c6">=</span> s_left <span style="color:#ff79c6">+</span> (s_right <span style="color:#ff79c6">-</span> s_left) <span style="color:#ff79c6">/</span> <span style="color:#bd93f9">2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">if</span> S(s) <span style="color:#ff79c6">&gt;</span> <span style="color:#bd93f9">0</span>
</span></span><span style="display:flex;"><span>            s_left <span style="color:#ff79c6">=</span> s
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">else</span>
</span></span><span style="display:flex;"><span>            s_right <span style="color:#ff79c6">=</span> s
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">return</span> s_left
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">end</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

<h2 class="group " id="conclusion"
    >Conclusion<a href="#conclusion"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>This blog examines the theory and applications of eigenvalue problems and generalized eigenvalue problems.  We&rsquo;ve seen through the Spectral Theorem that the symmetry of the matrix is essential for obtaining real-valued eigenvalue decompositions (and thus an ordering to the eigenvalues).  These eigenvalues can be characterized by variational problems, and useful applied problems (particularly PCA and its weighted version, and FLD) can be formulated in the same optimization problems.</p>
<p>The key implication of this is that these optimization problems admit of efficient solutions: eigenvalue decompositions.  These optimization problems are by no means convex, yet they still admit of efficient solutions through the use of <kbd>scipy.linalg.eigh</kbd>.</p>
<p>We&rsquo;ve also seen some &ldquo;bonus&rdquo; relationships with the singular value decomposition in the form of the Orthogonal Procrustes Problem, and the fact that you can compute the eigenvalues needed for PCA directly from \(X\), rather than having to form \(\widehat{\Sigma} = \frac{1}{N}X^{\mathsf{T}}X\).  As well, the detailed computations for signal to noise ratio optimization show us the generalized eigenvalue decomposition \(\Sigma = \Omega V \Lambda V^{\mathsf{T}}\Omega\) of \(\Sigma\) with respect to \(\Omega\).  This is another useful decomposition which generalizes the &ldquo;vanilla&rdquo; eigenvalue decomposition \(\Sigma = U \Lambda U^{\mathsf{T}}\).</p>
<p>These problems present plenty of further interesting avenues for exploration, which I invite the reader to meditate upon :)</p></section>
<div class="flex items-center justify-center px-6 pb-5 pt-4 text-center text-xl text-gray-500 md:px-10 md:pb-10 md:pt-14"><a
                class="mr-4 inline-flex h-5 w-5 items-center"
                title="Share to twitter"
                href="https://twitter.com/share?&text=Most%20people%20are%20familiar%20with%20the%20concept%20of%20matrix%20eigenvalues.%20Less%20well%20known%20is%20that%20this%20concept%20can%20be%20fruitfully%20expanded%20to%20the%20generalized%20eigenvalues%20of%20pairs%20of%20matrices.%20Closely%20related%20are%20matrix%20trace%20optimization%20problems,%20which%20extremizes%20the%20trace%20of%20certain%20matrix%20products.%20Trace%20optimization%20constites%20a%20large%20class%20of%20practically%20solvable%20non-convex%20optimization%20problems%20commonly%20useful%20for%20dimensionality%20reduction%20and%20which%20includes%20the%20unsupervised%20weighted%20principle%20component%20analysis%20and%20the%20supervised%20method%20of%20Fisher&amp;rsquo;s%20Linear%20Discriminant.%20The%20purpose%20of%20this%20post%20is%20to%20explore%20some%20of%20these%20problems,%20their%20intuition,%20and%20their%20applications.&url=https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/"
                target="_blank"
                rel="noopener noreferrer"
                ><i class="eva eva-twitter hover:text-theme"></i
            ></a><a
                class="mr-4 inline-flex h-5 w-5 items-center"
                title="Share to weibo"
                href="https://service.weibo.com/share/share.php?url=https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/&title=Generalized%20Eigenvalue%20Problems%20and%20Trace%20Optimization&sudaref=rjtk.github.io"
                target="_blank"
                rel="noopener noreferrer"
            ><svg class="inline-block h-5 w-5 fill-current hover:text-theme" viewBox="0 0 1193 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2681" width="200" height="200"> <path d="M456.991736 557.336482c-107.598583 0-194.644628 74.956316-194.644629 166.838252s87.046045 166.838253 194.644629 166.838253c107.598583 0 194.644628-74.956316 194.644628-166.838253s-87.046045-166.838253-194.644628-166.838252zM391.707202 822.101535c-36.269185 0-66.493506-27.806375-66.493507-62.866588s29.015348-62.866588 66.493507-62.866588c36.269185 0 66.493506 27.806375 66.493506 62.866588S427.976387 822.101535 391.707202 822.101535z m93.090909-91.881936c-14.507674 0-26.597403-12.089728-26.597403-26.597403s12.089728-26.597403 26.597403-26.597403 26.597403 12.089728 26.597403 26.597403-12.089728 26.597403-26.597403 26.597403zM239.376623 281.690673C97.9268 394.125148-18.134593 600.859504 30.224321 661.308146c32.642267 41.105077 59.239669-19.343566 124.524203-89.46399 30.224321-32.642267 77.374262-54.403778 122.106258-90.672964 20.552538-16.92562 197.062574-35.060213 230.913813-35.060212 97.9268-99.135773 114.85242-207.943329 74.956316-258.720189-48.358914-59.239669-201.898465-16.92562-343.348288 94.299882z" p-id="2682" ></path> <path d="M808.802834 560.9634C906.729634 461.827627 911.565525 377.199528 870.460449 326.422668c-47.149941-60.448642-211.570248-32.642267-351.811098 78.583235" p-id="2683" ></path> <path d="M605.695396 353.020071c-14.507674 8.46281-25.38843 12.089728-29.015349 7.253837-1.208973-2.417946-1.208973-6.044864 1.208973-9.671783-16.92562-1.208973-33.85124-1.208973-50.776859-1.208973C235.749705 349.393152 0 500.514758 0 686.696576s235.749705 337.303424 527.112161 337.303424 527.112161-151.121606 527.11216-337.303424c0-170.465171-194.644628-309.497048-448.528925-333.676505zM481.171192 959.924439C275.645809 959.924439 108.807556 847.489965 108.807556 708.458087s166.838253-251.466352 372.363636-251.466351 372.363636 112.434475 372.363637 251.466351-166.838253 251.466352-372.363637 251.466352z" p-id="2684" ></path> <path d="M1021.582054 423.140496c-3.626919 0-6.044864 0-9.671782-1.208973-18.134593-4.835891-29.015348-24.179457-22.970485-42.31405 2.417946-7.253837 3.626919-16.92562 3.626919-29.015348 0-65.284534-53.194805-119.688312-119.688312-119.688312-19.343566 0-33.85124-15.716647-33.851239-33.851239s15.716647-33.85124 33.851239-33.85124c103.971665 0 187.390791 84.628099 187.390791 187.390791 0 18.134593-2.417946 33.85124-6.044864 48.358914-4.835891 14.507674-18.134593 24.179457-32.642267 24.179457z" p-id="2685" ></path> <path d="M1146.106257 473.917355c-2.417946 0-6.044864 0-8.46281-1.208972-20.552538-4.835891-32.642267-25.38843-27.806375-45.940969 6.044864-24.179457 8.46281-47.149941 8.46281-71.329397C1118.299882 201.898465 991.357733 74.956316 836.609209 74.956316c-20.552538 0-37.478158-16.92562-37.478158-37.478158S816.056671 0 836.609209 0c197.062574 0 356.646989 159.584416 356.646989 356.646989 0 29.015348-3.626919 59.239669-10.880755 88.255018-3.626919 18.134593-19.343566 29.015348-36.269186 29.015348z" p-id="2686" ></path> </svg></a><i class="eva eva-copy mr-4 cursor-pointer hover:text-theme" title="Copy Link" data-clipboard-text="https://rjtk.github.io/posts/generalized-eigenvalue-problems-and-trace-optimization/"></i></div>
<div class="border-b dark:border-darkBorder"></div><div class="flex justify-between px-2 py-4 text-xl md:px-6 md:text-2xl">
    <div>
        <a
            href="/posts/natures-dartboard-the-axioms-of-probability/"
            title="Nature&#39;s Dartboard: The Axioms of Probability"
            class="invisible flex cursor-pointer items-center text-gray-500 transition duration-300 ease-[ease] hover:text-theme dark:text-darkTextPlaceholder dark:hover:text-darkText"style="visibility: visible;">
            <span class="flex items-center text-5xl opacity-70 dark:bg-opacity-100">
                <i class="eva eva-chevron-left-outline"></i>
            </span>
            <span>Prev</span>
        </a>
    </div><div class="hidden items-center text-xs xl:flex">
        Unless otherwise stated, this blog is licensed under 「
        <a href="https://creativecommons.org/licenses/by-nc-sa/4.0" target="_blank" rel="noopener noreferrer" class="text-theme">CC BY-NC-SA 4.0</a>
        」
    </div>
    <div class="flex items-center text-sm xl:hidden">
        <a href="https://creativecommons.org/licenses/by-nc-sa/4.0" target="_blank" rel="noopener noreferrer" class="text-theme">
            <img src="/Cc-by-nc-sa.svg" alt="CC BY-NC-SA 4.0" title="CC BY-NC-SA 4.0" class="w-24" />
        </a>
    </div>

    <a
        href="/posts/all-about-quadratic-forms/"
        title="All About Quadratic Forms"
        class="invisible flex cursor-pointer items-center text-gray-500 transition duration-300 ease-[ease] hover:text-theme dark:text-darkTextPlaceholder dark:hover:text-darkText"style="visibility: visible;">
        <span>Next</span>
        <span class="flex items-center text-5xl opacity-70 dark:bg-opacity-100">
            <i class="eva eva-chevron-right-outline"></i>
        </span>
    </a>
</div>


        </div>
    </div>
</div>

</div>
                </main><footer>
    <div
        class="com-footer flex flex-col items-center border-t py-4 px-4 text-sm leading-none text-gray-600 dark:border-darkBorder dark:text-darkTextPlaceholder md:flex-row md:justify-between"
    >
        <div class="mb-2 flex items-center justify-between text-center md:mb-0">
            <span class="">© 2022 - 2023</span>
            <span class="mx-1.5 opacity-50"> | </span>Powered by <a data-no-swup class="mx-1 font-bold hover:text-theme" href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> <span class="text-xs opacity-70">❤</span> <a data-no-swup class="mx-1 font-bold hover:text-theme" href="https://github.com/Ice-Hazymoon/hugo-theme-luna" target="_blank" rel="noopener noreferrer">Luna</a></div>

        <div class="flex items-center"><span class="noscript-hidden mx-1.5 hidden opacity-50 md:block"> | </span><a data-no-swup href="https://rjtk.github.io/index.xml" target="_blank" class="mr-1.5 hover:text-theme">
                    <span class=" md:hidden lg:inline"><svg t="1650887361919" class="mr-0.5 w-3 fill-current text-inherit inline-block align-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3091"><path d="M320.16155 831.918c0 70.738-57.344 128.082-128.082 128.082S63.99955 902.656 63.99955 831.918s57.344-128.082 128.082-128.082 128.08 57.346 128.08 128.082z m351.32 94.5c-16.708-309.2-264.37-557.174-573.9-573.9C79.31155 351.53 63.99955 366.21 63.99955 384.506v96.138c0 16.83 12.98 30.944 29.774 32.036 223.664 14.568 402.946 193.404 417.544 417.544 1.094 16.794 15.208 29.774 32.036 29.774h96.138c18.298 0.002 32.978-15.31 31.99-33.58z m288.498 0.576C943.19155 459.354 566.92955 80.89 97.00555 64.02 78.94555 63.372 63.99955 77.962 63.99955 96.032v96.136c0 17.25 13.67 31.29 30.906 31.998 382.358 15.678 689.254 322.632 704.93 704.93 0.706 17.236 14.746 30.906 31.998 30.906h96.136c18.068-0.002 32.658-14.948 32.01-33.008z" p-id="3092"></path></svg></span>
                    <span>RSS</span>
                </a><a data-no-swup href="https://rjtk.github.io/sitemap.xml" target="_blank" class="mr-1.5 hover:text-theme">
                    <span class=" md:hidden lg:inline"><svg t="1650887940556" class="mr-0.5 w-3 fill-current text-inherit inline-block align-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6291"><path d="M950.044 625.778h-68.266v-56.89c0-45.51-39.822-85.332-85.334-85.332h-256v-85.334h68.267c39.822 0 73.956-34.133 73.956-73.955V130.844c0-39.822-34.134-73.955-73.956-73.955H415.29c-39.822 0-73.956 34.133-73.956 73.955v193.423c0 39.822 34.134 73.955 73.956 73.955h68.267v85.334h-256c-45.512 0-85.334 39.822-85.334 85.333v56.889H73.956C34.133 625.778 0 659.91 0 699.733v193.423c0 39.822 34.133 73.955 73.956 73.955h193.422c39.822 0 73.955-34.133 73.955-73.955V699.733c0-39.822-34.133-73.955-73.955-73.955H199.11v-56.89c0-17.066 11.378-28.444 28.445-28.444h568.888c17.067 0 28.445 11.378 28.445 28.445v56.889h-68.267c-39.822 0-73.955 34.133-73.955 73.955v193.423c0 39.822 34.133 73.955 73.955 73.955h193.422c39.823 0 73.956-34.133 73.956-73.955V699.733c0-39.822-34.133-73.955-73.956-73.955z" p-id="6292"></path></svg></span>
                    <span>Sitemap</span>
                </a></div><div id="run-time" class="mt-2 flex-grow text-right md:mt-0">
    <span>Run time: </span><b id="run-time-d">0</b>
    <span class="text-xs">days</span>
    <b id="run-time-h">0</b>
    <span class="text-xs">h</span>
    <b id="run-time-m">0</b>
    <span class="text-xs">m</span>
    <b id="run-time-s">0</b>
    <span class="text-xs">s</span>
</div>
</div>

    <script type="text/javascript">
window.__theme = {
    cdn: '',
    pjax: true ,
    isServer: false ,
    $version:"",
    lang: 'en-us',
    imageZoom: true ,
    lazyload: true ,
    bionicReading: {
        enabled: true ,
        skipLinks: false ,
        autoBionic: false ,
        excludeWords:[],
        excludeClasses:[],
        excludeNodeNames:[],
    },
    katex: true ,
    search: true ,
    backtop: true ,
    pangu: false ,
    autoDarkMode: false ,
    googleAnalytics:false,
    hugoEncrypt: {
        wrongPasswordText: 'Password is incorrect',
        userStorage:window['sessionStorage'],
    },
    console: {
        enabled: true ,
        leftColor: '#dd6065',
        rightColor: '#feb462',
        leftText: 'Hugo Theme Luna',
        rightText: 'Powered by Hugo ❤ Luna',
    },
    assets: {
        error_svg: "\/images\/error.svg",
        search_svg: "\/images\/search.svg",
    },
    i18n: {
        copy: {
            success: "Copy success",
            failed: "Copy failed",
            copyCode: "Copy code",
        },
        search: {
            untitled: "Untitled",
            loadFailure: "Initialization of the search engine failed",
            input: "Type something...",
        },
        darkMode: {
            dark: "Switch to dark mode",
            light: "Switch to light mode"
        }
    },creatTime: "2022\/12\/11"}
</script>
<script type="text/javascript" src="/ts/main.3e551c0bbea8d18cea412abc67113efa9fd16c3bba1eb4867a7e762260ee32dc.js" defer integrity="sha256-PlUcC76o0YzqQSq8ZxE&#43;&#43;p/RbDu6HrSGen52ImDuMtw="></script>





<script>
    
    
</script>

<script data-swup-reload-script>
    
    
</script>
</footer>
</div>
        </div><a
        href="#nav"
        title="Back to top"
        id="back-top"
        class="fixed right-6 bottom-9 z-10 translate-y-3 scale-90 cursor-pointer rounded-full bg-white opacity-0 transition duration-300 ease-[ease] dark:bg-darkBgAccent sm:scale-100"
    >
        <div class="relative">
            <div class="absolute left-0 top-0 flex h-full w-full items-center justify-center text-xl">
                <i class="eva eva-arrow-upward-outline"></i>
            </div>
            <svg id="svg" width="54" height="54" viewBox="0 0 54 54" preserveAspectRatio="xMinYMin meet">
                <circle
                    transform="rotate(-90, 27 , 27)"
                    style="stroke-dasharray: 157, 157; stroke-dashoffset: 157;"
                    cx="27"
                    cy="27"
                    r="25"
                    fill="none"
                    stroke-width="4"
                    stroke-linecap="round"
                    stroke="var(--theme)"
                />
            </svg>
        </div>
    </a><noscript>
    <style>
        .dark-mode-switch,
        #run-time,
        #bionicReading,
        .noscript-hidden,
        [data-clipboard-text],
        [data-lazyload] {
            display: none;
        }
        #back-top {
            opacity: 1;
        }
        .noscript-show {
            display: initial;
        }
    </style>
</noscript>
</body>
</html>
