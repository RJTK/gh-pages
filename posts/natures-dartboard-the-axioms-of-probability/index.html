<!DOCTYPE html>
<html lang="en-us" class="lang-en-us">
    <head><meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="The basic axioms of probability and the idea of a random variable as a function from the sample space to \(\mathbb{R}\) are quite abstract and rather confusing at first pass. However, the axioms can be well motivated from our intuition, and defining random variables simply as functions turns out to be a brilliant and intuitive abstraction. My goal in this post is to try to explain the ideas behind the axiomatization of probability theory, and hopefully make the study of measure-theoretic probability seem a bit less intimidating.
" />
<meta itemprop="description" content="The basic axioms of probability and the idea of a random variable as a function from the sample space to \(\mathbb{R}\) are quite abstract and rather confusing at first pass. However, the axioms can be well motivated from our intuition, and defining random variables simply as functions turns out to be a brilliant and intuitive abstraction. My goal in this post is to try to explain the ideas behind the axiomatization of probability theory, and hopefully make the study of measure-theoretic probability seem a bit less intimidating.
" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<meta itemprop="name" content="Nature&#39;s Dartboard: The Axioms of Probability - Quant Out of Water" />
<link href="https://rjtk.github.io/index.xml" title="Nature&#39;s Dartboard: The Axioms of Probability - Quant Out of Water" type="application/rss+xml" rel="alternate" />
<title>Nature&#39;s Dartboard: The Axioms of Probability - Quant Out of Water</title><link rel="stylesheet" href="/main.efa856a7e699d6b8dc8748086f759f5a7bff94ff5428a65430ef983ad3b92d56.css" integrity="sha256-76hWp+aZ1rjch0gIb3WfWnv/lP9UKKZUMO+YOtO5LVY=" /><meta property="og:site_name" content="Nature&#39;s Dartboard: The Axioms of Probability - Quant Out of Water" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Nature&#39;s Dartboard: The Axioms of Probability" />
<meta property="og:description" content="The basic axioms of probability and the idea of a random variable as a function from the sample space to \(\mathbb{R}\) are quite abstract and rather confusing at first pass. However, the axioms can be well motivated from our intuition, and defining random variables simply as functions turns out to be a brilliant and intuitive abstraction. My goal in this post is to try to explain the ideas behind the axiomatization of probability theory, and hopefully make the study of measure-theoretic probability seem a bit less intimidating.
" />
<meta property="og:url" content="https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/" /><meta property="article:publisher" content="https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/" /><meta property="article:published_time" content="2023-04-23T00:00:00-07:00" /><meta property="article:modified_time" content="2023-04-23T20:23:55-07:00" /><meta name="theme-color" content="#dd6065" />
<meta name="mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-title" content="Nature&#39;s Dartboard: The Axioms of Probability - Quant Out of Water" />
<meta name="apple-mobile-web-app-status-bar-style" content="white" />
<meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="Nature&#39;s Dartboard: The Axioms of Probability - Quant Out of Water" />
<meta name="twitter:description" content="The basic axioms of probability and the idea of a random variable as a function from the sample space to \(\mathbb{R}\) are quite abstract and rather confusing at first pass. However, the axioms can be well motivated from our intuition, and defining random variables simply as functions turns out to be a brilliant and intuitive abstraction. My goal in this post is to try to explain the ideas behind the axiomatization of probability theory, and hopefully make the study of measure-theoretic probability seem a bit less intimidating.
" />
<meta name="twitter:url" content="https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/" /><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" /><link rel="manifest" href="/manifest.json" /><link href="/icon.png" rel="shortcut icon" />
<link href="/icon.png" rel="Bookmark" />
<link rel="apple-touch-icon" href="/icon.png" /><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Nature\u0027s Dartboard: The Axioms of Probability",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/rjtk.github.io\/posts\/natures-dartboard-the-axioms-of-probability\/"
  },"genre": "posts","wordcount": 4490 ,
  "url": "https:\/\/rjtk.github.io\/posts\/natures-dartboard-the-axioms-of-probability\/","datePublished": "2023-04-23T00:00:00-07:00","dateModified": "2023-04-23T20:23:55-07:00","author": {
    "@type": "Person",
    "name": "qoow"
  },"description": ""
}
</script>


    </head>
    
    <body class="dark:bg-darkBg dark:text-darkText">
        <div class="relative mx-auto shadow-lg md:max-w-4xl xl:max-w-4xl 2xl:max-w-4xl">
            <div class="main flex flex-col justify-between bg-white dark:bg-darkFg"><header class="relative"><div class=" text-center py-6 border-b dark:border-darkBorder"><div class="text-3xl">Quant Out of Water</div></div><nav id="nav" class="navbar top-0 z-10 flex items-center border-b px-5 py-6 dark:border-darkBorder md:px-10">
        <div class="route-items flex w-full items-center justify-around"><a
                    title="Home"
                    data-active-link="/"
                    href="/"
                    class=" relative flex cursor-pointer flex-col items-center text-gray-700 transition duration-150 ease-[ease]"
                >
                    <i
                        class="eva eva-home flex h-8 w-8 items-center justify-center text-3xl leading-none text-gray-500 transition duration-150 ease-[ease] dark:text-darkTextPlaceholder"
                    ></i>
                    <span class="mt-1 block text-xs text-gray-400 dark:text-darkText sm:text-sm md:hidden">Home</span>
                </a><a
                    title="About"
                    data-active-link="/about/"
                    href="/about/"
                    class=" relative flex cursor-pointer flex-col items-center text-gray-700 transition duration-150 ease-[ease]"
                >
                    <i
                        class="eva eva-people flex h-8 w-8 items-center justify-center text-3xl leading-none text-gray-500 transition duration-150 ease-[ease] dark:text-darkTextPlaceholder"
                    ></i>
                    <span class="mt-1 block text-xs text-gray-400 dark:text-darkText sm:text-sm md:hidden">About</span>
                </a>
        </div>
    </nav><nav class="sub-navbar z-10 flex items-center border-b px-5 py-2 dark:border-darkBorder md:px-10"><a
                title="Tags"
                data-active-link="/tags/"
                href="/tags/"
                class=" relative flex cursor-pointer items-center mr-4 text-gray-400 dark:text-darkText transition duration-150 ease-[ease] hover:text-theme"
            >
                <i
                    class="eva eva-pricetags leading-none mr-1"
                ></i>
                <span>Tags</span>
            </a></nav><div class="dark-mode-switch absolute right-0 top-0 z-10 cursor-pointer pr-2 pt-2 text-xl leading-none">
    <i class="eva eva-moon opacity-20 dark:opacity-70"></i>
</div>
</header>
<main class="relative flex flex-grow " id="swup"><style>:root {
        --font:Roboto, "Helvetica Neue", Helvetica, Arial, sans-serif;
    }</style><div class="type-posts layout- w-full"><div class="relative h-full">
    <div class="relative h-full">
        <div class="page-view-article flex h-full flex-col bg-white dark:bg-darkFg"><div class="relative"><div
    style="background-image: linear-gradient(180deg,#464320, #85863f);padding-bottom:75%;"
    class="article-cover relative h-0Page(/posts/naturesDartboard/index.md) w-full"
>
    <picture class="noscript-hidden"><source data-srcset="/posts/natures-dartboard-the-axioms-of-probability/mother_nature2_hu5cffa319ffdfc98ebd614509f276a524_1972409_1024x0_resize_q95_h2_catmullrom_3.webp 1080w" type="image/webp" /><img
            data-src="https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/mother_nature2.png"
            src="/posts/natures-dartboard-the-axioms-of-probability/mother_nature2_hu5cffa319ffdfc98ebd614509f276a524_1972409_328d964026d5963201125f9a36f034c8.jpg"
            width="1024"
            height="1024"
            data-srcset="/posts/natures-dartboard-the-axioms-of-probability/mother_nature2_hu5cffa319ffdfc98ebd614509f276a524_1972409_1024x0_resize_catmullrom_3.png 1080w"
            alt="Nature&#39;s Dartboard: The Axioms of Probability"
            class="absolute left-0 top-0 h-full w-full object-cover object-center"
            data-lazyload data-lazyload-blur

        />
    </picture>
    <noscript>
        <picture><source srcset="/posts/natures-dartboard-the-axioms-of-probability/mother_nature2_hu5cffa319ffdfc98ebd614509f276a524_1972409_1024x0_resize_q95_h2_catmullrom_3.webp 1080w" type="image/webp" /><img
                src="https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/mother_nature2.png"
                srcset="/posts/natures-dartboard-the-axioms-of-probability/mother_nature2_hu5cffa319ffdfc98ebd614509f276a524_1972409_1024x0_resize_catmullrom_3.png 1080w"
                alt="Nature&#39;s Dartboard: The Axioms of Probability"width="1024"
                height="1024"
                class="absolute left-0 top-0 h-full w-full object-cover object-center"
            />
        </picture>
    </noscript>
</div>
<h1 class="article-title absolute bottom-0 left-0 w-full px-6 pb-8 pt-32 text-3xl text-white dark:text-darkText md:px-10 md:text-4xl">
    Nature&#39;s Dartboard: The Axioms of Probability
</h1></div><div class="article-info border-b p-6 pb-3 text-sm dark:border-darkBorder md:px-10">
    <div>
        <div class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
            <i class="eva eva-clock-outline mr-1"></i>
            <span>
                <time
                    title="Published in 23 Apr 2023 00:00:00"datetime="2023-04-23T00:00:00-07:00">23 Apr 2023</time
                >
            </span>
        </div><div class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
                <i class="eva eva-flag-outline mr-1"></i>
                <span>22 mins read</span>
            </div><div class="mb-3 mr-4 inline-flex items-center sm:rounded-full">
                <i class="eva eva-bar-chart-outline mr-1"></i>
                <span>4490 words</span>
            </div></div></div><aside
            class="toc border-b border-gray-300 px-5 py-5 dark:border-darkBorder md:px-10 2xl:fixed 2xl:top-10 2xl:m-0 2xl:-ml-72 2xl:w-72 2xl:border-none 2xl:p-0 2xl:py-4 2xl:pr-4 "
        >
            <header>
                <h1 class="mb-3 text-2xl font-bold 2xl:mb-4">Table of Contents</h1>
            </header>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#mathematics-and-axiomatization">Mathematics and Axiomatization</a></li>
    <li><a href="#the-axioms-of-probability">The Axioms of Probability</a>
      <ul>
        <li><a href="#samples-and-events">Samples and Events</a></li>
        <li><a href="#nature-s-dartboard">Nature&rsquo;s Dartboard</a></li>
        <li><a href="#the-banach-tarski-paradox-volume-and-measure">The Banach-Tarski Paradox: Volume and Measure</a></li>
      </ul>
    </li>
    <li><a href="#random-variables">Random Variables</a>
      <ul>
        <li><a href="#expectation">Expectation</a></li>
      </ul>
    </li>
    <li><a href="#data-and-sampling-the-passage-to-statistics">Data and Sampling &ndash; the Passage to Statistics</a>
      <ul>
        <li><a href="#independent-and-identically-distributed-random-variables">Independent and Identically Distributed Random Variables</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        </aside><section class="article-content typo relative flex-grow px-6 py-5 md:px-10"><i title="Faster Reads" id="bionicReading" class="absolute right-0 top-0 cursor-pointer p-3"
            ><svg class="w-4 h-4 fill-current text-gray-400" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10811" ><path d="M480 192c-19.2 0-32-12.8-32-32v-128c0-19.2 12.8-32 32-32s32 12.8 32 32v128c0 19.2-12.8 32-32 32zM160 1024c-6.4 0-19.2 0-25.6-6.4l-128-128c0-6.4-6.4-19.2-6.4-25.6s6.4-19.2 6.4-25.6l512-512c12.8-6.4 38.4-6.4 51.2 0l128 128c0 6.4 6.4 19.2 6.4 25.6s-6.4 19.2-6.4 25.6l-512 512c-6.4 6.4-19.2 6.4-25.6 6.4z m-83.2-160l83.2 83.2 467.2-467.2-83.2-83.2-467.2 467.2zM992 576h-128c-19.2 0-32-12.8-32-32s12.8-32 32-32h128c19.2 0 32 12.8 32 32s-12.8 32-32 32zM768 288c-6.4 0-19.2 0-25.6-6.4-12.8-12.8-12.8-32 0-44.8l96-96c12.8-12.8 32-12.8 44.8 0s12.8 32 0 44.8l-96 96c0 6.4-12.8 6.4-19.2 6.4zM256 288c-6.4 0-19.2 0-25.6-6.4L134.4 185.6c-6.4-12.8-6.4-38.4 0-51.2s32-12.8 44.8 0l96 96c12.8 12.8 12.8 32 0 44.8 0 12.8-12.8 12.8-19.2 12.8zM864 896c-6.4 0-19.2 0-25.6-6.4l-96-96c-12.8-12.8-12.8-32 0-44.8s32-12.8 44.8 0l96 96c12.8 12.8 12.8 32 0 44.8 0 6.4-12.8 6.4-19.2 6.4z" p-id="10812"></path><path d="M544 640c-6.4 0-19.2 0-25.6-6.4l-128-128c-6.4-12.8-6.4-38.4 0-51.2s32-12.8 44.8 0l128 128c12.8 12.8 12.8 38.4 6.4 51.2-6.4 6.4-19.2 6.4-25.6 6.4z" p-id="10813"></path></svg></i
        ><p>The basic axioms of probability and the idea of a random variable as a function from the sample space to \(\mathbb{R}\) are quite abstract and rather confusing at first pass.  However, the axioms can be well motivated from our intuition, and defining random variables simply as functions turns out to be a brilliant and intuitive abstraction.  My goal in this post is to try to explain the ideas behind the axiomatization of probability theory, and hopefully make the study of measure-theoretic probability seem a bit less intimidating.</p>
<h2 class="group " id="mathematics-and-axiomatization"
    >Mathematics and Axiomatization<a href="#mathematics-and-axiomatization"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>Exactly what is <em>Mathematics</em> is not necessarily an easy thing to pin down, and to try to give some sort of clear formal definition would be senseless.  However, it is easy to say what it is <em>not</em>: it is not simply a prescribed set of rules for calculating numbers, or &ldquo;solving for \(x\)&rdquo; &ndash; a prescribed set of calculating rules has another name: an <em>algorithm</em>.</p>
<p>In contrast to the mechanical process described by an algorithm, mathematics is a highly creative endeavour.  Indeed, many mathematicians are motivated largely by the aesthetics and the &ldquo;beauty&rdquo; of their results.  To many mathematicians, it is the abstraction and the concepts themselves that are of interest, and they may not even be entirely interested in their results and calculations are \(100\%\) correct!  A great book about the process of doing math (which can be got for rather low prices) is <a
    class="link"
    href="https://amzn.to/41Rgdeb"target="_blank" rel="noopener">The Mathematical Experience</a
>
 💁‍♂️</p>
<p>Still, many mathematicians are at the same time concerned with correctness and <em>rigor</em>.  That is, they want to be able to say with supreme confidence that their results are true and correct, with no room for doubt.  Essential to this is the idea of a mathematical <em>proof</em> &ndash; a sequence of logical deductions which begin from some specified premises \(P\) and lead inexorably to some conclusion \(Q\).  If the logical deductions are correct, then the mathematician has rigorously obtained a proof of the implication \(P \implies Q\).  That is, given that the premises \(P\) are true, it must be the case that the conclusions \(Q\) are true.  Mathematicians call this \(P \implies Q\) implication a <em>Theorem</em>.</p>
<p>Once a mathematician (or a group of them, perhaps spanning continents and generations) has amassed a rather sizable number of implications, and understands something about the various connections amongst them, the mathematician might attempt to lay down a <em>fundamental</em> set of premises from which all of their conclusions can be derived.  This set of fundamental premises, which cannot be derived (as far as the mathematician can tell) from any other still more fundamental premises, are called <em>axioms</em>.  The axioms are the basic starting point for a <em>theory</em>, that is, for all the further conclusions the mathematician has derived.  Axioms are ultimately just premises, but the former term tends to imply that it is a <em>foundational</em> premise &ndash; <em>i.e.</em>, something that is self-evident that can serve as a starting point.</p>
<p>The most famous set of axioms are <a
    class="link"
    href="https://en.wikipedia.org/wiki/Euclidean_geometry"target="_blank" rel="noopener">Euclid&rsquo;s Axioms</a
>
 laying the classical foundation for geometry.  Another being those of <a
    class="link"
    href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory"target="_blank" rel="noopener">ZF Set Theory</a
>
, which lay the foundations for set theory, and therefore of more-or-less all of modern &ldquo;mainstream&rdquo; mathematics.  The axioms I care about discussing in this post are the <a
    class="link"
    href="https://en.wikipedia.org/wiki/Probability_axioms"target="_blank" rel="noopener">axioms of Probability Theory</a
>
.</p>
<p>As opposed to the rigorous series of logical deductions by which proofs proceed, the way that the <em>idea</em> of a Theorem comes about (<em>i.e.</em>, the thought that some implication might possibly be true) is much more creative and messy.  Often, one begins with a conclusion \(Q\) that &ldquo;seems true&rdquo;, and then proceeds to search for the set of premises \(P\) that <em>make</em> it true, possibly going back and forth many times tweaking the premises and the conclusion &ndash; a process reminiscent of Rawl&rsquo;s <a
    class="link"
    href="https://en.wikipedia.org/wiki/Reflective_equilibrium"target="_blank" rel="noopener">reflective equilibrium</a
>
 in philosphy.  The mathematician is done with this inquiry when they have obtained a Theorem which they think is &ldquo;good&rdquo;.  I have seen it claimed that a Theorem is <em>good</em> if it is surprising, deep, and can be used to derive other good theorems.  My memory tells me that this should be attributed to Polya, but I cannot find a reference (please email me if you know where this is from!).</p>
<p>As well, we usually come up with axioms long after having figured out most of the conclusions (possibly using stronger or more numerous premises).  Axiomatization is a process of <em>cleaning up</em> the theory and placing it upon a nice, beautiful, and simple foundation.  Thus, I think it can be rather confusing, from a didactic standpoint, to write textbooks or teach classes which simply begin with the axioms and proceed to derive all of the conclusions &ndash; the student is left confused and wondering what the axioms mean, where did they come from, <em>etc</em>.  To be comfortable in following this process straight from the axioms requires some amount of experience and mathematical maturity.</p>
<p>Indeed, the process of derivation from a set of axioms is not reflective of how the theory was developed, and it is hardly even reflective of how mathematicians think about the theory themselves.  My hope is that the rest of this post can clarify some of the intuition behind the axioms of probability.</p>


<h2 class="group " id="the-axioms-of-probability"
    >The Axioms of Probability<a href="#the-axioms-of-probability"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>The purpose of probability theory is to provide a set of mathematical tools for reasoning about events which are <em>random</em>.  Exactly what random <em>means</em> is on one hand left to intuition (<em>e.g.</em>, we think of a dice roll as random, because we have no practical means of determining what the outcome will be), and on the other hand, is rigorously axiomatized by the theory of probability.  My purpose here is to help to make the axiomatization itself more intuitive.</p>
<p>The rigorous theory of probability was axiomatized in 1933 by Russian mathematician <a
    class="link"
    href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov"target="_blank" rel="noopener">Andrey Kolmogorov</a
>
 (1903-1987).  This axiomatization begins with a set \(\Omega\) called the <em>sample space</em>.  What this set is exactly need not be fully specified &ndash; it is just some abstract set, and it may be a finite set like \(\{\mathsf{H}, \mathsf{T}\}\) (to model flipping a coin), it may be a countably infinite set like \(\{1, 2, \ldots\}\), it could be an uncountably infinite set like \(\mathbb{R}\), or some even more abstract set.</p>
<p>To set the stage, the following sections will introduce the probability triple \((\Omega, \mathcal{F}, \mathbb{P})\) consisting of a sample space \(\Omega\), a $σ$-algebra \(\mathcal{F}\) thereof, and the probability measure \(\mathbb{P}\).  On this triple we axiomatize probability with the following three premises:</p>
<ol>
<li>The domain of \(\mathbb{P}\) is \(\mathcal{F}\) and for any \(S \in \mathcal{F}\) we have \(P(S) \ge 0\) (non-negativity)</li>
<li>\(\mathbb{P}(\Omega) = 1\) (Normalization)</li>
<li>If \(S_1, S_2, \ldots \in \mathcal{F}\) are disjoint, then \(\mathbb{P}\bigl(\bigcup_{i = 1}^\infty S_i \bigr) = \sum_{i = 1}^\infty \mathbb{P}(S_i)\) (Countable Additivity)</li>
</ol>
<p>By the end of this section, I hope the reader will have some understanding of what all of this means, and why these axioms are so beautiful.  It is remarkable that probability theory flows from these three simple statements!</p>


<h3 class="group " id="samples-and-events"
    >Samples and Events<a href="#samples-and-events"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>In probability theory, an element \(\omega \in \Omega\) of the sample space is called a <em>sample</em>.  We don&rsquo;t know what this sample is, but it encodes the <strong>state of the universe</strong>.  Subsets \(S \subseteq \Omega\) are called <em>events</em> (actually, they must be elements \(S \in \mathcal{F}\), which is a set of subsets of \(\Omega\) &ndash; more on this later), and we imagine that an event has <em>occurred</em> if \(\omega \in S\).</p>
<p>Intuitively, there are many states of the universe that might lead to the same event; e.g., the outcome of a coin toss ultimately depends upon, at least, the arrangement of molecules in the air through which the coin travels, and there are many more than two such arrangements.  Thus, there are many \(\omega\) (arrangements of molecules) which result in the &ldquo;Heads&rdquo; outcome of the coin toss.</p>
<p>When we do modelling in practice, we usually don&rsquo;t work directly with \(\Omega\).  Instead, we work with <em>random variables</em>, which are functions from the sample space into \(\mathbb{R}\).  The sample space is kept as an abstract entity &ldquo;in the background&rdquo;.  But, before we discuss this additional layer of abstraction, we need to understand \((\Omega, \mathcal{F}, \mathbb{P})\).</p>


<h3 class="group " id="nature-s-dartboard"
    >Nature&rsquo;s Dartboard<a href="#nature-s-dartboard"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>On an <em>intuitive</em> level, I think of the set \(\Omega\) as being <em>Nature&rsquo;s Dartboard</em> &ndash; it is just a disc, and &ldquo;Mother Nature&rdquo; is throwing a dart at it.  The exact location at which the dart lands selects for us a state of the universe \(\omega \in \Omega\).</p>
<figure><img src="mother_nature2.png"
         alt="Figure 1: Nature&amp;rsquo;s Dartboard &amp;ndash; Generated by OpenAI&amp;rsquo;s DALL-E 2 Generative Model"/><figcaption>
            <p><span class="figure-number">Figure 1: </span>Nature&rsquo;s Dartboard &ndash; Generated by <a
    class="link"
    href="https://openai.com/product/dall-e-2"target="_blank" rel="noopener">OpenAI&rsquo;s DALL-E 2</a
>
 Generative Model</p>
        </figcaption>
</figure>

<p>In addition to the set \(\Omega\), we are given a <em>function</em> \(\mathbb{P}\) called a <em>probability measure</em>.  The function \(\mathbb{P}\) is a bit more abstract than the sort of function (a curve) that one might usually encounter in calculus.  Instead, \(\mathbb{P}\) is a function which maps from <em>subsets</em> of \(\Omega\) into some number between \([0, 1]\).  That is, if we have some set \(S \subset \Omega\), we would like \(\mathbb{P}\) to assign some number to \(S\) as in \(\mathbb{P}(S)\).  This number is called the probability of the event (subset), and is directly analogous to the <em>volume</em> of \(S\).</p>
<p>Indeed, if we imagine that Mother Nature throws her dart at the dartboard \(\Omega\) completely &ldquo;at random&rdquo; (following our intuition) then our intuitive idea of the probability of the dart landing within some region \(S \subseteq \Omega\) of the dartboard is nothing but \(\text{vol}(S) / \text{vol}(\Omega)\), where \(\text{vol}(S)\) measures the <em>volume</em> (in this case, just an <em>area</em>) of the set \(S\).</p>
<p>Now, if we imagine that \(\mathbb{P}(S) = \text{vol}(S) / \text{vol}(\Omega)\), then the axiom \(\mathbb{P}(S) \ge 0\) is natural since volumes cannot be negative, and \(\mathbb{P}(\Omega) = 1\) is elementary algebra.  <strong><strong>This is the basic intuition</strong></strong> of the axioms of probability &ndash; we measure the probability of <strong>events</strong> \(S\), which are nothing but subsets of the sample space \(\Omega\), by appealing to the volume of the dartboard that they occupy.</p>
<p>Continuing this line of reasoning, imagine we have some set \(S \subseteq \Omega\) which has some probability \(\mathbb{P}(S)\).  If we break this set into two pieces (<em>i.e.,</em> two disjoint sets \(S_1, S_2\) with \(S_1 \cup S_2 = S\) and \(S_1 \cap S_2 = \emptyset\)) then it stands to reason that we should have \[\mathbb{P}(S) = \mathbb{P}(S_1) + \mathbb{P}(S_2),\] since the volume of one set must be the sum of the volumes of any <strong>partition</strong> of that set.  This is the idea behind Axiom 3, <strong>additivity</strong>.</p>
<p>That Axiom 3 is stated for <em>countably infinite</em> partitions of events (<em>i.e.</em>, infinite sums) is a technical condition: <strong>countable additivity</strong>.  Simple finite additivity turns out to not be strong enough to result in a rich enough theory.  Making the axioms works <em>rigorously</em> for these infinite sums, and for infinite sample spaces, is the whole point of the formalism surrounding the $σ$-algebra \(\mathcal{F}\).  We turn to this next.</p>


<h3 class="group " id="the-banach-tarski-paradox-volume-and-measure"
    >The Banach-Tarski Paradox: Volume and Measure<a href="#the-banach-tarski-paradox-volume-and-measure"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>The object \(\mathcal{F}\) is called a $σ$-algebra of subsets of \(\Omega\), and it is in general a subset of the set of all subsets of \(\Omega\).  Understanding this $σ$-algebra is part of the difficult technical work that goes into learning rigorous probability theory, but the <strong>purpose</strong> of \(\mathcal{F}\) is to eliminate certain pathological sets in \(2^\Omega\) which are not <strong>measurable</strong> (see: <a
    class="link"
    href="https://en.wikipedia.org/wiki/Measure_%28mathematics%29"target="_blank" rel="noopener">Measure Theory</a
>
).  That is, there are some sets (<em>e.g.</em>, <a
    class="link"
    href="https://proofwiki.org/wiki/Vitali_Theorem"target="_blank" rel="noopener">Vitali Sets</a
>
) for which it simply makes no sense to assign a notion of volume, and the issue arises directly as a result of dealing with infinite sets.</p>
<p>A famous example illustrating this issue is the <strong>Banach-Tarski</strong> paradox.  This paradox shows that we can take a sphere, partition it into six pieces, and then, after rotating some of those pieces in
a clever way, reassemble <strong>two</strong> copies of the original sphere.  Here is a <a
    class="link"
    href="https://www.youtube.com/watch?v=s86-Z-CbaHA"target="_blank" rel="noopener">Video explanation</a
>
 of the procedure as well as a <a
    class="link"
    href="https://www.quantamagazine.org/how-a-mathematical-paradox-allows-infinite-cloning-20210826/"target="_blank" rel="noopener">Quanta Magazine article</a
>
.  The fact that we can take a set, partition that set, and then reassemble <strong>two</strong> identical copies of that set is rather problematic when it comes to assigning volumes!  It seems that we can derive \(\mathbb{P}(S) = 2\mathbb{P}(S)\) where \(S\) is a sphere!  This is a pathology that needs to be eliminated.</p>
<p>Indeed, we would really like for it to be the case that \(\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B)\) if \(A \cap B = \emptyset\) and where \(\mathbb{P}(S) = \text{vol}(S) / \text{vol}(\Omega)\) is consistent with our notion of volume.  But, this can only be done if we simply <strong>exclude a-priori</strong> so-called non-measurable sets.  This is more-or-less what the $σ$-algebra \(\mathcal{F}\) is doing &ndash; it is a special collection of subsets of \(\Omega\) which ensures that any \(S \in \mathcal{F}\) is a measurable set to which we can assign a notion of volume that corresponds with our intuition.  In the language of probability, it means that only certain subsets of \(\Omega\) can be assigned a probability: the measurable subsets.  There are some pathological subsets of Nature&rsquo;s dartboard which we simply cannot measure.</p>
<p>Specifically, a collection of subsets \(\mathcal{F}\) of \(\Omega\) is a $σ$-algebra of subsets if the following conditions are satisfied:</p>
<ol>
<li>\(\Omega \in \mathcal{F}\) (The universe is one of the sets)</li>
<li>\(A, B \in \mathcal{F} \implies A \setminus B \in \mathcal{F}\) (Closed under set differences)</li>
<li>If \(A_1, A_2, \ldots \in \mathcal{F}\) and \(A_i \cap A_j = \emptyset\) if \(i \ne j\), then \(\bigcup_{i = 1}^\infty A_i \in \mathcal{F}\) (Closed under countable disjoint union)</li>
</ol>
<p>Notice that these are basically nothing but the conditions that make the axioms of probability &ldquo;work&rdquo;, particularly closure under countable unions.  That is, the notion of a $σ$-algebra is cooked up exactly for the purpose of making sure that measure corresponds to our intuition of volume.</p>
<p>To actually construct a volume measure for the dartboard, one begins by constructing a measure on the real line \(\mathbb{R}\) by defining \(\mathbb{P}([a, b]) = b - a\), and then generating a $σ$-algebra out of intervals (by taking complements and countable unions <em>etc</em>.).  This guarantees that the resulting \(\sigma\) algebra is generated by &ldquo;reasonable&rdquo; operations on the sets, excluding those which are not measurable.  Then, by putting together the rules of the $σ$-algebra above, and using the basic intuition of \(\mathbb{P}([a, b]) = b - a\), we can eventually construct what is called <a
    class="link"
    href="https://en.wikipedia.org/wiki/Lebesgue_measure"target="_blank" rel="noopener">Lebesgue Measure</a
>
, a rigorous way of assigning volume to (measurable) subsets of \(\mathbb{R}\).  This is enough to get us to a suitable notion of volume, and therefore to a construction of a probability measure \(\mathbb{P}\)!</p>
<p><kbd><strong>Remark</strong> (Axiom of Choice): The problem of measure is not entirely inescapable.  If we did not accept the</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Axiom_of_choice"target="_blank" rel="noopener">Axiom of Choice</a
>
, <kbd>which basically just says that it is possible to choose an arbitrary element from an infinite set, then non-measurable sets could not be constructed, and the problem would disappear.  Some mathematicians thus view the existence of non-measurable sets, which are incredibly pathological, as a reason to refuse to accept the axiom of choice.  However, we need to pick our poison; there are at the same time a vast number of perfectly intuitive and incredibly useful theorems which <em>depend</em> on the axiom of choice.  My favourite example being the</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem#Hahn.E2.80.93Banach_separation_theorem"target="_blank" rel="noopener">Hanh-Banach Theorem</a
>
, <kbd>the cornerstone of convex analysis (named after the same Banach as the Banach-Tarski Paradox).  Instead of doing without more-or-less all of functional analysis, most instead choose to accept non-measurable sets as a fact of (mathematical) life, and carry around all the baggage associated with $\sigma$-algebras and measure theory.</kbd></p>


<h2 class="group " id="random-variables"
    >Random Variables<a href="#random-variables"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>All of the formalism laid out in understanding the probability triple \((\Omega, \mathcal{F}, \mathbb{P})\) is rather complicated.  Fortunately, in most &ldquo;practical&rdquo; scenarios where we want to <em>use</em> probability for modelling, we just let this formalism sit back as an abstract foundation, more-or-less out of sight.  We instead work more directly with <em>random variables</em> which are defined as nothing but <em>functions from \(\Omega\) into \(\mathbb{R}\)</em> as in: \[X: \Omega \rightarrow \mathbb{R},\] sometimes written as \(X(\omega)\).</p>
<p>The &ldquo;randomness&rdquo; thus passes from Nature&rsquo;s Dartboard \(\Omega\), an abstract set that we don&rsquo;t want to think about too much, to \(\mathbb{R}\), a natural set that we like to work with.  The way this works is as follows &ndash; suppose we want to compute the probability that \(X(\omega)\) is less than some real number \(x\), that is \(F(x) = \mathbb{P}(X \le x)\).  This is an important function called the <em>cumulative distribution function</em> of \(X\) and it turns out to be enough to fully determine the random behaviour of \(X\).  The notation \(\mathbb{P}(X \le x)\) is really nothing but a shorthand for \[F(x) = \mathbb{P}(\{\omega: X(\omega) \le x\}).\]</p>
<p>That is, the common notation \(\mathbb{P}(X \le x)\), which is used to mean &ldquo;the probability that the random variable \(X\) is less than or equal to the real number \(x\)&rdquo; is really a shorthand for \(\mathbb{P}(\{\omega:\ X(\omega) \le x\})\) which is &ldquo;the volume of the set of all events \(\omega\) such that the function \(X\) evaluated \(X(\omega)\) is less than or equal to the real number \(x\)&rdquo;.  We can simplify this further:</p>
<p>\begin{equation}
\begin{aligned}
F(x)
&amp;= \mathbb{P}(\{\omega: X(\omega) \le x\})\\
&amp;= \mathbb{P}(\{\omega: X(\omega) \in (-\infty, x]\})\\
&amp;= \mathbb{P}(X^{-1} ((-\infty, x]))\\
&amp;= \mathbb{P} \circ X^{-1} ((-\infty, x]).
\end{aligned}\notag
\end{equation}</p>
<p>These elementary manipulations show us that that the cumulative distribution function \(F\) is nothing but the composition of the probability measure \(\mathbb{P}\) with the inverse mapping of \(X\) (the domain of which are all the half-open intervals like \((-\infty, x]\))!  What does this mean?  It allows us to define events on \(\mathbb{R}\), like the probability of the event \(X \le x\) by measuring the size of the set of all \(\omega\) which satisfy \(X(\omega) \le x\).  So, what we do &ldquo;in practice&rdquo; is that we start by positing a distribution function \(F\) (like a Gaussian distribution) which we want to work with on \(\mathbb{R}\), and then we just trust that there is some appropriate probability triple \((\Omega, \mathcal{F}, \mathbb{P})\) which is &ldquo;adequately expressive&rdquo; and some function \(X\) such that \(F = \mathbb{P} \circ X^{-1}\).  The function \(X\) is now a random variable having <em>c.d.f.</em> \(F\).  This gives us a huge variety of possibilities for modelling, and the randomness on \(\mathbb{R}\) is inherited from the simple and intuitive notion of randomness as volume on Nature&rsquo;s dartboard.</p>
<p><kbd><strong>Remark</strong> (Measurable Functions): Technically, the random variable $X$ must be a</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Measurable_function"target="_blank" rel="noopener">measurable function</a
>
.  <kbd>This is another important technical condition, but irrelevant for developing an intuition.</kbd></p>
<p><kbd><strong>Remark</strong> (Functions of Random Variables): Suppose we have some function $g: \mathbb{R} \rightarrow \mathbb{R}$.  It is worthwhile to point out that a function of a random variable $g(X)$ defines a <em>new</em> random variable $Y(\omega) = g \circ X(\omega)$, since it is nothing but a another function from $\Omega$ to $\mathbb{R}$.  The cumulative distribution function of this new random variable is given by $F_Y(y) = \mathbb{P} \circ X^{-1} \circ g^{-1}((-\infty, y])$.</kbd></p>


<h3 class="group " id="expectation"
    >Expectation<a href="#expectation"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>Another essential concept in probability theory is that of the <strong>expectation</strong>.  Intuitively, this is like a generalization of an <em>average</em>.  If the sample space \(\Omega\) is finite, then we <em>define</em> \[\mathbb{E}X = \sum_{i = 1}^N X(\omega) \mathbb{P}(\omega),\] and if \(\mathbb{P}(\omega) = \frac{1}{N}\) then it is exactly an average of the random variable over the sample space.  Generally, finite sample spaces aren&rsquo;t rich enough to be interesting, so we define the expectation by means of an integral \[\mathbb{E}X = \int_\Omega X(\omega) \mathsf{d}\mathbb{P}(\omega).\]  This is called a <a
    class="link"
    href="https://en.wikipedia.org/wiki/Lebesgue_integration"target="_blank" rel="noopener">Lebesgue Integral</a
>
, and our desire to define and use this integral is another part of the motivation for studying probability in the context of measure theory.  Regardless, it is usually possible to dispense with this integral &ldquo;in practice&rdquo; and to work directly with the distribution function \(F\).  This works as follows:</p>
<p>\begin{equation}
\begin{aligned}
\mathbb{E}X
&amp;= \int_\Omega X(\omega) \mathsf{d}\mathbb{P}(\omega)\\
&amp;\overset{(a)}{=} \int_{X(\Omega)} x\ \mathsf{d} \mathbb{P} \circ X^{-1} (x)\\
&amp;\overset{(b)}{=} \int_{\mathbb{R}} x\ \mathsf{d} F(x)\\
&amp;\overset{( c)}{=} \int_{-\infty}^\infty xf(x)\ \mathsf{d}x,
\end{aligned}\notag
\end{equation}</p>
<p>where \((a)\) comes from making the substitution \(x = X(\omega)\), \((b)\) just simplifies using \(F = \mathbb{P} \circ X^{-1}\), and \(( c)\) is a final simplification for <strong>continuous</strong> random variables many readers may be used to: if \(F\) is a differentiable function, then the random variable admits a <strong>density function</strong> \(f(x) = F^\prime(x)\) and \(\mathsf{d}F(x)\) turns into \(f(x)\mathsf{d}x\), resulting in an &ldquo;ordinary&rdquo; Riemann integral (as opposed to \(\int_\mathbb{R} x \mathsf{d} F(x)\) which is a <a
    class="link"
    href="https://en.wikipedia.org/wiki/Lebesgue%E2%80%93Stieltjes_integration"target="_blank" rel="noopener">Lebesgue-Stieltjes integral</a
>
).</p>
<p><kbd><strong>Remark</strong> (Expectations of Functions of Random Variables): Given a function $g: \mathbb{R} \rightarrow \mathbb{R}$, we may like to compute the expectation of the new random variable $g(X)$ as in $\mathbb{E}g(X) = \int_\Omega g(X(\omega)) \mathsf{d} \mathbb{P}(\omega).$ We could do so by determining the distribution function $G(x) = \mathbb{P}(g(X) \le x)$ of the new random variable $g(X)$ and then doing the calculation $\int_\mathbb{R} x \mathbb{d} G(x)$.  But alternatively, we can see by repeating the same substitution $x = X(\omega)$ in the integral that we made earlier that $\mathbb{E}g(X) = \int_\mathbb{R} g(x) \mathsf{d} F(x)$ which is still just an integral over the distribution of $X$!  This is an incredibly convenient fact called the</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician"target="_blank" rel="noopener">Law of the Unconcious Statistician</a
>
.</p>


<h2 class="group " id="data-and-sampling-the-passage-to-statistics"
    >Data and Sampling &ndash; the Passage to Statistics<a href="#data-and-sampling-the-passage-to-statistics"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>Many people are now drawn to the study of probability for the purposes of doing statistics, <em>i.e.</em>, analyzing data.  So, I wanted to add some more commentary about random variables and their connection to the modelling of data.  That is, how do statistics and probability connect?</p>
<p>Statistics is often concerned with <em>experiments</em>.  Suppose we plan an experiment where we are going to measure someone&rsquo;s height.  The outcome of this experiment is not known a-priori, so it makes sense to model it using probability theory &ndash; the outcome is random.  As well, a person&rsquo;s height is a real number, and we now know that we can model random real numbers using <em>random variables</em>.  Thus, we might like to construct a model of the measure of someone&rsquo;s height as a random variable \(H(\omega)\).</p>
<p>If we measure the heights of many people, we will wind up with a multitude of random variables \(H_1, H_2, \ldots, H_N\), where \(N\) is the number of measurements we make.  These random variables all together define some <em>joint distribution</em> \(F(h_1, \ldots, h_N) = \mathbb{P}(H_1 \le h_1, \ldots, H_N \le h_N)\), and statistics is often concerned with what can be learned about the distribution of these random variables through the actually observed outcomes of experiments.</p>


<h3 class="group " id="independent-and-identically-distributed-random-variables"
    >Independent and Identically Distributed Random Variables<a href="#independent-and-identically-distributed-random-variables"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h3>

<p>The multivariate distribution described above can be difficult to work with &ndash; some assumptions need to be made.  The most common assumption is that the random variables are <em>independent and identically distributed</em>, that is, <em>i.i.d.</em>  What this means is that we <em>assume</em>:</p>
<ol>
<li>\(\mathbb{P}(H_i \le h) = \mathbb{P}(H_j \le h)\) (identically distributed)</li>
<li>\(\mathbb{P}(H_1 \le h_1, \ldots, H_N \le h_N) = \mathbb{P}(H_1 \le h_1) \times \cdots \times \mathbb{P}(H_N \le h_N)\) (independent).</li>
</ol>
<p>The idea of the random variables having an identical distribution is that we are observing separate outcomes of the same experiment.  Independence, where the joint distribution is nothing but the product of the individual (<em>marginal</em>) distributions, encodes the idea that the outcome of one experiment does not influence (and is not influenced by) any other experiment.</p>
<p>Thus, one of the most common settings in statistics is that we are given a sequence \(X_1, X_2, \ldots, X_N\) of <em>i.i.d.</em> random variables, and our goal is to learn something about the <em>statistics</em> of their common distribution, call it \(F_X\).  The word <em>statistic</em> here just means &ldquo;functionals of the distribution&rdquo;, <em>i.e.</em>, functions which take the distribution function \(F_X\) as an input, and output some real number.  The mean value \(\mathbb{E}X = \int_{\mathbb{R}} x \mathsf{d} F_X(x)\) being an important example, <em>quantiles</em> being another: \(\mathbb{P}(X \le x) = \int_{\mathbb{R}} \mathbf{1}[x \le q]\mathsf{d} F_X(x)\).</p>
<p>Given data, and statistical theorems about how collections of <em>i.i.d.</em> random variables behave, statisticians are able to answer questions like: &ldquo;what is the probability that a randomly selected person will be over six feet tall?&rdquo;, &ldquo;in a room of \(N\) people, what is the probability that there will be someone more than six feet five inches?&rdquo;, <em>etc</em>.  Going further, the combination of powerful computers with sophisticated statistical models can be used to produce striking results &ndash; the dart throwing woman that serves as the cover photo of this blog post was itself generated from a statistical model.  That is, the image itself is essentially nothing but a particular realization of a random variable!</p>
<p><kbd><strong>Remark</strong> (The LLN): The most natural estimator of</kbd> \(\mathbb{E}X\) <kbd>given the sample is the <em>average</em></kbd> \(\overline{X}_N = \frac{1}{N}\sum_{i = 1}^N X_i\) <kbd>and indeed, many of the most famous theorems in statistics (the</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Law_of_large_numbers"target="_blank" rel="noopener">Law of Large Numbers</a
>
 <kbd>and the</kbd> <a
    class="link"
    href="https://en.wikipedia.org/wiki/Central_limit_theorem"target="_blank" rel="noopener">Central Limit Theorem</a
>
) <kbd>are concerned exactly with how $\overline{X}_N$ behaves under various different assumptions about the common distribution.  Indeed, it can be shown that in almost all sensible cases $\overline{X}_N \rightarrow \mathbb{E}X \text{ as } N \rightarrow \infty$ (given an appropriate sense of convergence for random variables): the average of the sample converges to the expectation as the size of the sample increases.  This is a highly intuitive outcome that can be derived entirely from the axioms of probability, and it is an incredibly satisfying result!  Some historical formulations of probability <em>began</em> with the postulate that this is true, so it is a testament to how effectively the axioms &ldquo;clean up&rdquo; the theory.</kbd></p>


<h2 class="group " id="conclusion"
    >Conclusion<a href="#conclusion"
        ><i class="eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100"></i></a
></h2>

<p>Axiomatic and measure theoretic probability is a topic more than worth studying.  It is very difficult, if not impossible, to have a full understanding of probability (particularly stochastic processes) without eventually going down this path.  I hope that I have here shown that the elementary ideas are not just esoteric abstractions, but that they are well supported by a very natural intuition.  It is the technical need to deal with annoying pathologies (like the Banach-Tarski paradox) that necessitates some technical machinery.  But, the underlying intuition comes down to nothing but the measure of volumes in Nature&rsquo;s Dartboard.  Indeed, most abstractions in mathematics have some completely natural and intelligible intuition behind them!</p>
<p><kbd><strong>Remark</strong> (Some book recommendations): The first book about probability I read was</kbd> <a
    class="link"
    href="https://amzn.to/3mVmQ0l"target="_blank" rel="noopener">An Intermediate Course in Probability</a
>
 <kbd>by Allan Gut.  However, this book is not measure theoretic.  Instead,</kbd> <a
    class="link"
    href="https://amzn.to/3mYdUY1"target="_blank" rel="noopener">this book by the same author</a
>
 <kbd>can serve as a reasonable introduction, or the</kbd> <a
    class="link"
    href="https://amzn.to/3LoD6jD"target="_blank" rel="noopener">well known book by Billingsley</a
>
 <kbd>which is more-or-less the canonical book to recommend.  Another great book which is somewhat less concerned with the details of measure theory, focusing more on probability, is</kbd> <a
    class="link"
    href="https://amzn.to/41Vq2YL"target="_blank" rel="noopener">Probability: Theory and Examples</a
>
 <kbd>which contains all of the classic results and, as the name suggestions, a lot of very nice examples.  However, I only read this book long after I became well acquainted with measure-theoretic probability.  Since its treatment of measure is rather more limited, it&rsquo;s difficult for me to know whether or not this is a good book to start with.</kbd></p>
<p><kbd><strong>Remark</strong> (Other topics):  There are a number of other rather famous books broadly in the area of machine learning and statistics &ndash; examples being the following:</kbd> <a
    class="link"
    href="https://amzn.to/3mYy0kY"target="_blank" rel="noopener">Theoretical Statistics</a
>
, <a
    class="link"
    href="https://amzn.to/3N79MQ0"target="_blank" rel="noopener">Pattern Recognition and Machine Learning</a
>
, <a
    class="link"
    href="https://amzn.to/3Aodk8Y"target="_blank" rel="noopener">Bayesian Data Analysis</a
>
, <a
    class="link"
    href="https://amzn.to/3oxC101"target="_blank" rel="noopener">Deep Learning</a
>
, and <a
    class="link"
    href="https://amzn.to/3V8mLD5"target="_blank" rel="noopener">Mathematics for Machine Learning</a
>
 <kbd>(this last one I have not myself read).  These books are perfectly good, but they are emphatically <em>not</em> about probability theory &ndash; they are applied books in machine learning and statistics.  To learn probability theory itself, stick with the above.</kbd></p></section><div class="px-6 pb-5 pt-4 text-center text-xl text-gray-500 md:px-10 md:pb-10 md:pt-14 flex items-center justify-center"><a
                class="mr-4 inline-flex h-5 w-5 items-center"
                title="Share to twitter"
                href="https://twitter.com/share?&text=The%20basic%20axioms%20of%20probability%20and%20the%20idea%20of%20a%20random%20variable%20as%20a%20function%20from%20the%20sample%20space%20to%20%5c%28%5cmathbb%7bR%7d%5c%29%20are%20quite%20abstract%20and%20rather%20confusing%20at%20first%20pass.%20However,%20the%20axioms%20can%20be%20well%20motivated%20from%20our%20intuition,%20and%20defining%20random%20variables%20simply%20as%20functions%20turns%20out%20to%20be%20a%20brilliant%20and%20intuitive%20abstraction.%20My%20goal%20in%20this%20post%20is%20to%20try%20to%20explain%20the%20ideas%20behind%20the%20axiomatization%20of%20probability%20theory,%20and%20hopefully%20make%20the%20study%20of%20measure-theoretic%20probability%20seem%20a%20bit%20less%20intimidating.&url=https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/"
                target="_blank"
                rel="noopener noreferrer"
                ><i class="eva eva-twitter hover:text-theme"></i
            ></a><a
                class="mr-4 inline-flex h-5 w-5 items-center"
                title="Share to weibo"
                href="https://service.weibo.com/share/share.php?url=https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/&title=Nature%27s%20Dartboard:%20The%20Axioms%20of%20Probability&sudaref=rjtk.github.io"
                target="_blank"
                rel="noopener noreferrer"
            ><svg class="inline-block h-5 w-5 fill-current hover:text-theme" viewBox="0 0 1193 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2681" width="200" height="200"> <path d="M456.991736 557.336482c-107.598583 0-194.644628 74.956316-194.644629 166.838252s87.046045 166.838253 194.644629 166.838253c107.598583 0 194.644628-74.956316 194.644628-166.838253s-87.046045-166.838253-194.644628-166.838252zM391.707202 822.101535c-36.269185 0-66.493506-27.806375-66.493507-62.866588s29.015348-62.866588 66.493507-62.866588c36.269185 0 66.493506 27.806375 66.493506 62.866588S427.976387 822.101535 391.707202 822.101535z m93.090909-91.881936c-14.507674 0-26.597403-12.089728-26.597403-26.597403s12.089728-26.597403 26.597403-26.597403 26.597403 12.089728 26.597403 26.597403-12.089728 26.597403-26.597403 26.597403zM239.376623 281.690673C97.9268 394.125148-18.134593 600.859504 30.224321 661.308146c32.642267 41.105077 59.239669-19.343566 124.524203-89.46399 30.224321-32.642267 77.374262-54.403778 122.106258-90.672964 20.552538-16.92562 197.062574-35.060213 230.913813-35.060212 97.9268-99.135773 114.85242-207.943329 74.956316-258.720189-48.358914-59.239669-201.898465-16.92562-343.348288 94.299882z" p-id="2682" ></path> <path d="M808.802834 560.9634C906.729634 461.827627 911.565525 377.199528 870.460449 326.422668c-47.149941-60.448642-211.570248-32.642267-351.811098 78.583235" p-id="2683" ></path> <path d="M605.695396 353.020071c-14.507674 8.46281-25.38843 12.089728-29.015349 7.253837-1.208973-2.417946-1.208973-6.044864 1.208973-9.671783-16.92562-1.208973-33.85124-1.208973-50.776859-1.208973C235.749705 349.393152 0 500.514758 0 686.696576s235.749705 337.303424 527.112161 337.303424 527.112161-151.121606 527.11216-337.303424c0-170.465171-194.644628-309.497048-448.528925-333.676505zM481.171192 959.924439C275.645809 959.924439 108.807556 847.489965 108.807556 708.458087s166.838253-251.466352 372.363636-251.466351 372.363636 112.434475 372.363637 251.466351-166.838253 251.466352-372.363637 251.466352z" p-id="2684" ></path> <path d="M1021.582054 423.140496c-3.626919 0-6.044864 0-9.671782-1.208973-18.134593-4.835891-29.015348-24.179457-22.970485-42.31405 2.417946-7.253837 3.626919-16.92562 3.626919-29.015348 0-65.284534-53.194805-119.688312-119.688312-119.688312-19.343566 0-33.85124-15.716647-33.851239-33.851239s15.716647-33.85124 33.851239-33.85124c103.971665 0 187.390791 84.628099 187.390791 187.390791 0 18.134593-2.417946 33.85124-6.044864 48.358914-4.835891 14.507674-18.134593 24.179457-32.642267 24.179457z" p-id="2685" ></path> <path d="M1146.106257 473.917355c-2.417946 0-6.044864 0-8.46281-1.208972-20.552538-4.835891-32.642267-25.38843-27.806375-45.940969 6.044864-24.179457 8.46281-47.149941 8.46281-71.329397C1118.299882 201.898465 991.357733 74.956316 836.609209 74.956316c-20.552538 0-37.478158-16.92562-37.478158-37.478158S816.056671 0 836.609209 0c197.062574 0 356.646989 159.584416 356.646989 356.646989 0 29.015348-3.626919 59.239669-10.880755 88.255018-3.626919 18.134593-19.343566 29.015348-36.269186 29.015348z" p-id="2686" ></path> </svg></a><i class="eva eva-copy mr-4 cursor-pointer hover:text-theme" title="Copy Link" data-clipboard-text="https://rjtk.github.io/posts/natures-dartboard-the-axioms-of-probability/"></i></div>
<div class="border-b dark:border-darkBorder"></div><div class="flex justify-between px-2 py-4 text-xl md:px-6 md:text-2xl">
    <div>
        <a
            href="/posts/group-theory-symmetry-and-a-brain-teaser/"
            title="Group Theory, Symmetry, and a Brain Teaser"
            class="invisible flex cursor-pointer items-center text-gray-500 transition duration-300 ease-[ease] hover:text-theme dark:text-darkTextPlaceholder dark:hover:text-darkText"style="visibility: visible;">
            <span class="flex items-center text-5xl opacity-70 dark:bg-opacity-100">
                <i class="eva eva-chevron-left-outline"></i>
            </span>
            <span>Prev</span>
        </a>
    </div><div class="hidden items-center text-xs xl:flex">
        Unless otherwise stated, this blog is licensed under 「
        <a href="https://creativecommons.org/licenses/by-nc-sa/4.0" target="_blank" rel="noopener noreferrer" class="text-theme">CC BY-NC-SA 4.0</a>
        」
    </div>
    <div class="flex items-center text-sm xl:hidden">
        <a href="https://creativecommons.org/licenses/by-nc-sa/4.0" target="_blank" rel="noopener noreferrer" class="text-theme">
            <img src="/Cc-by-nc-sa.svg" alt="CC BY-NC-SA 4.0" title="CC BY-NC-SA 4.0" class="w-24" />
        </a>
    </div>

    <a
        href=""
        title=""
        class="invisible flex cursor-pointer items-center text-gray-500 transition duration-300 ease-[ease] hover:text-theme dark:text-darkTextPlaceholder dark:hover:text-darkText">
        <span>Next</span>
        <span class="flex items-center text-5xl opacity-70 dark:bg-opacity-100">
            <i class="eva eva-chevron-right-outline"></i>
        </span>
    </a>
</div>

        </div>
    </div>
</div>
</div>
                </main><footer>
    <div
        class="com-footer flex flex-col items-center border-t py-4 px-4 text-sm leading-none text-gray-600 dark:border-darkBorder dark:text-darkTextPlaceholder md:flex-row md:justify-between"
    >
        <div class="mb-2 flex items-center justify-between text-center md:mb-0">
            <span class="">© 2022 - 2023</span>
            <span class="mx-1.5 opacity-50"> | </span>Powered by <a data-no-swup class="mx-1 font-bold hover:text-theme" href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> <span class="text-xs opacity-70">❤</span> <a data-no-swup class="mx-1 font-bold hover:text-theme" href="https://github.com/Ice-Hazymoon/hugo-theme-luna" target="_blank" rel="noopener noreferrer">Luna</a></div>

        <div class="flex items-center"><span class="noscript-hidden mx-1.5 hidden opacity-50 md:block"> | </span><a data-no-swup href="https://rjtk.github.io/index.xml" target="_blank" class="mr-1.5 hover:text-theme">
                    <span class=" md:hidden lg:inline"><svg t="1650887361919" class="mr-0.5 w-96 w-3 fill-current text-inherit inline-block align-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3091"><path d="M320.16155 831.918c0 70.738-57.344 128.082-128.082 128.082S63.99955 902.656 63.99955 831.918s57.344-128.082 128.082-128.082 128.08 57.346 128.08 128.082z m351.32 94.5c-16.708-309.2-264.37-557.174-573.9-573.9C79.31155 351.53 63.99955 366.21 63.99955 384.506v96.138c0 16.83 12.98 30.944 29.774 32.036 223.664 14.568 402.946 193.404 417.544 417.544 1.094 16.794 15.208 29.774 32.036 29.774h96.138c18.298 0.002 32.978-15.31 31.99-33.58z m288.498 0.576C943.19155 459.354 566.92955 80.89 97.00555 64.02 78.94555 63.372 63.99955 77.962 63.99955 96.032v96.136c0 17.25 13.67 31.29 30.906 31.998 382.358 15.678 689.254 322.632 704.93 704.93 0.706 17.236 14.746 30.906 31.998 30.906h96.136c18.068-0.002 32.658-14.948 32.01-33.008z" p-id="3092"></path></svg></span>
                    <span>RSS</span>
                </a><a data-no-swup href="https://rjtk.github.io/sitemap.xml" target="_blank" class="mr-1.5 hover:text-theme">
                    <span class=" md:hidden lg:inline"><svg t="1650887940556" class="mr-0.5 w-3 fill-current text-inherit inline-block align-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6291"><path d="M950.044 625.778h-68.266v-56.89c0-45.51-39.822-85.332-85.334-85.332h-256v-85.334h68.267c39.822 0 73.956-34.133 73.956-73.955V130.844c0-39.822-34.134-73.955-73.956-73.955H415.29c-39.822 0-73.956 34.133-73.956 73.955v193.423c0 39.822 34.134 73.955 73.956 73.955h68.267v85.334h-256c-45.512 0-85.334 39.822-85.334 85.333v56.889H73.956C34.133 625.778 0 659.91 0 699.733v193.423c0 39.822 34.133 73.955 73.956 73.955h193.422c39.822 0 73.955-34.133 73.955-73.955V699.733c0-39.822-34.133-73.955-73.955-73.955H199.11v-56.89c0-17.066 11.378-28.444 28.445-28.444h568.888c17.067 0 28.445 11.378 28.445 28.445v56.889h-68.267c-39.822 0-73.955 34.133-73.955 73.955v193.423c0 39.822 34.133 73.955 73.955 73.955h193.422c39.823 0 73.956-34.133 73.956-73.955V699.733c0-39.822-34.133-73.955-73.956-73.955z" p-id="6292"></path></svg></span>
                    <span>Sitemap</span>
                </a><div id="google_translate_element" class="overflow-hidden rounded border dark:border-darkBorder"></div></div><div id="run-time" class="mt-2 flex-grow text-right md:mt-0">
    <span>Run time: </span><b id="run-time-d">0</b>
    <span class="text-xs">days</span>
    <b id="run-time-h">0</b>
    <span class="text-xs">h</span>
    <b id="run-time-m">0</b>
    <span class="text-xs">m</span>
    <b id="run-time-s">0</b>
    <span class="text-xs">s</span>
</div>
</div>

    <script type="text/javascript">
window.__theme = {
    cdn: '',
    pjax: true ,
    isServer: false ,
    $version:"",
    lang: 'en-us',
    imageZoom: true ,
    lazyload: true ,
    bionicReading: {
        enabled: true ,
        skipLinks: false ,
        autoBionic: false ,
        excludeWords:[],
        excludeClasses:[],
        excludeNodeNames:[],
    },
    katex: true ,
    search: true ,
    backtop: true ,
    pangu: false ,
    autoDarkMode: false ,
    googleAnalytics:false,
    hugoEncrypt: {
        wrongPasswordText: 'Password is incorrect',
        userStorage:window['sessionStorage'],
    },
    console: {
        enabled: true ,
        leftColor: '#dd6065',
        rightColor: '#feb462',
        leftText: 'Hugo Theme Luna',
        rightText: 'Powered by Hugo ❤ Luna',
    },
    assets: {
        error_svg: "\/images\/error.svg",
        search_svg: "\/images\/search.svg",
    },
    i18n: {
        copy: {
            success: "Copy success",
            failed: "Copy failed",
            copyCode: "Copy code",
        },
        search: {
            untitled: "Untitled",
            loadFailure: "Initialization of the search engine failed",
            input: "Type something...",
        },
        darkMode: {
            dark: "Switch to dark mode",
            light: "Switch to light mode"
        }
    },creatTime: "2022\/12\/11"}
</script>
<script type="text/javascript" src="/ts/main.988b77b29f6061a05182a3737ac3fe3b34578e939adc83437110316c5e2023d5.js" defer integrity="sha256-mIt3sp9gYaBRgqNzesP&#43;OzRXjpOa3INDcRAxbF4gI9U="></script><script type="text/javascript" src="/translate-google.ecd37c3cc8f7e1b08910201dd0d9fc3e5f4b1395fb8ebead3141b931f036f777.js" defer integrity="sha256-7NN8PMj34bCJECAd0Nn8Pl9LE5X7jr6tMUG5MfA293c="></script><script type="text/javascript">
    window.translateelement_styles = "\/sass\/translateelement.min.c65796c6c3e4d48c75f72776948be83c5c448d1c5cc466b996b00657c558d28a.css";
    function googleTranslateElementInit(){
        new google.translate.TranslateElement({
            pageLanguage: 'en-us',
            includedLanguages: 'af,ga,sq,it,ar,ja,az,kn,eu,ko,bn,la,be,lv,bg,lt,ca,mk,zh-CN,ms,zh-TW,mt,hr,no,cs,fa,da,pl,nl,pt,en,ro,eo,ru,et,sr,tl,sk,fi,sl,fr,es,gl,sw,ka,sv,de,ta,el,te,gu,th,ht,tr,iw,uk,hi,ur,hu,vi,is,cy,id,yi',
            autoDisplay:false
        },'google_translate_element');
    }
</script>





<script>
    
    
</script>

<script data-swup-reload-script>
    
    
</script>
</footer>
</div>
        </div><a
        href="#nav"
        title="Back to top"
        id="back-top"
        class="fixed right-6 bottom-9 z-10 translate-y-3 scale-90 cursor-pointer rounded-full bg-white opacity-0 transition duration-300 ease-[ease] dark:bg-darkBgAccent sm:scale-100"
    >
        <div class="relative">
            <div class="absolute left-0 top-0 flex h-full w-full items-center justify-center text-xl">
                <i class="eva eva-arrow-upward-outline"></i>
            </div>
            <svg id="svg" width="54" height="54" viewBox="0 0 54 54" preserveAspectRatio="xMinYMin meet">
                <circle
                    transform="rotate(-90, 27 , 27)"
                    style="stroke-dasharray: 157, 157; stroke-dashoffset: 157;"
                    cx="27"
                    cy="27"
                    r="25"
                    fill="none"
                    stroke-width="4"
                    stroke-linecap="round"
                    stroke="var(--theme)"
                />
            </svg>
        </div>
    </a><noscript>
    <style>
        .dark-mode-switch,
        #run-time,
        #bionicReading,
        .noscript-hidden,
        [data-clipboard-text],
        [data-lazyload] {
            display: none;
        }
        #back-top {
            opacity: 1;
        }
        .noscript-show {
            display: initial;
        }
    </style>
</noscript>
</body>
</html>
